%!TEX root = main.tex

\section{When do response conditionals exist?}

\todo[inline]{Warmup paragraph}

\todo[inline]{Lemmas are intermediate steps}

The specific question we ask here is: given a conditional probability model $(\prob{P}_\square^{\RV{Y}|\RV{D}[M]},\{\prob{P}_{\tilde{\alpha}}^{\RV{D}}\}_A,f)$ where $\RV{D}=(\RV{D}_i)_{i\in M}$ are choice variables, $\RV{Y}=(\RV{Y}_i)_{i\in M}$ are outcome variables and $M$ is some index set, when does there exist a response conditional $\prob{P}_\square^{\RV{Y}_0|\RV{D}_0}$ that explains how each $\RV{Y}_i$ responds to each $\RV{D}_i$; we call these \emph{repeatable response conditionals}. The reason why we consider a sequential problem is that in practice, causal inference almost always deals with observational data that is assumed to be appropriately modeled with a sequence of independent and identically distributed random variables. Furthermore, we can characterise precisely the kinds of models for which such response conditionals exist. Thus the sequential setting is both a theoretically tractable and widely appliccable starting point.

We examine this question from two points of view: firstly,  we aks \emph{what kind of conditional probability models exhibit repeatable response condtionals?} Secondly, we ask \emph{what kinds of experiments are these conditional probability models appropriate for?} In the course of answering the second question, we show that apparently subtle differences in the description of an experimental procedure can determine whether a particular experiment should or should not be modeled with repeatable response conditionals.

We need to make strong assumptions about an experiment to establish the existence of repeatable response conditional in the first place, once we have repeatable response conditionals with respect to some pair of variables $(\RV{Y},\RV{D})$, response conditionals with respect to different pairs of variables may exist due to conditional independences in the original $\prob{P}_\square^{\RV{Y}_0|\RV{D}_0}$. These conditional independences can be tested for in the observed data.

\subsection{Repeatable experiments}

Our setup is a conditional probability model $(\prob{P}_\square^{\overline{\RV{Y}|\RV{D}}},A)$ where $\RV{Y}:=\RV{Y}_M=(\RV{Y}_i)_{i\in M}$ and $\RV{D}:=\RV{D}_M=(\RV{D}_i)_{i\in M}$ for some index set $M$; we say such a model is a model of a \emph{sequential experiment}. We say that $\RV{Y}_i$ is the consequence corresponding to the decision $\RV{D}_i$ for all $i\in M$ (i.e. variables with matching indices correspond). We say that $\prob{P}_\square^{\overline{\RV{Y}|\RV{D}}}$ features \emph{repeatable response conditionals} if there exists a hypothesis $\RV{H}$ such that $\prob{P}_\square^{\RV{Y}_i|\RV{H}\RV{D}_i}=\prob{P}_\square^{\RV{Y}_j|\RV{H}\RV{D}_j}$ for all $i,j\in M$, $\RV{H}\CI_{\prob{P}_\square} \RV{D}$ and $\RV{Y}_i\CI_{\prob{P}_{\square}} \RV{Y}_{M\setminus \{i\}} \RV{D}_{M\setminus \{i\}} |\RV{H}\RV{D}_i$. We remind the reader that in a conditional probability model, arbitrary conditional probabilities do not always exist, see Definition \ref{def:cprob_pset}.

There are two assumptions relevant to the existence of repeatable response conditionals. The first is a condition of interchangability: in particular, given a permutation of $M$, we get the same result from applying this permutation only to the set of actions we take, or from applying it only to the set of outcomes we observe. We call this \emph{exchange commutativity}.

The second is a condition of \emph{locality of consequences}; that is the assumption that $\RV{Y}_i$ is independent of $\RV{D}_j$ given $\RV{D}_i$ for any $j$. It is possible to have models in which commutativity to exchange holds but locality of consequences does not. Such a situation could arise in a model of stimulus payments to individuals in a nation; if exactly $n$ payments of \$10 000 are made, we might consider that it doesn't matter much exactly who receives the payments (this is a subtle question, though, we will return to it in more detail later). However, the amount of inflation induced depends on the number of payments; making 100 such payments will have a negligible effect on inflation, while making payments to everyone in the country is likely to have a substantial effect. \citet{dawid_causal_2000} discusses condition of \emph{post-treatment exchangeability} which is similarto exchange commutativity, and there he gives the example of herd immunity in vaccination campaigns as a situation where post-treatment exchangeability holds but locality of consequences does not.

As we have mentioned, exchange commutativity is similar to the condition of \emph{post-treatment exchangeability} found in \citet{dawid_decision-theoretic_2020}.  Exchange commutativity is also very similar to the exchangeability assumption of \citet{greenland_identifiability_1986}, and the assumption of exchangeability found in \citet{banerjee_chapter_2017}. However, in every case there is a subtle but important difference; exchange commutativity concerns exchanges of actions and outcomes, while these other exchangeability conditions concern exchange of ``people'' or ``experimental units''. Swapping people and experimental units are actions in the real world, and so these symmetries have to be described as part of the measurement procedure, and can't be purely characterised as symmetries of a probabilistic model. On the other hand, swapping the orders of variables \emph{can} be described purely as a symmetry of a probabilistic model, as these swaps involve only function composition. As we will discuss in detail, exchangeability of experimental units does not always imply exchange commutativity.

Locality of consequences is similar to the stable unit treatment distribution assumption (SUTDA) in \citet{dawid_decision-theoretic_2020}. It is also related to the ``no interference'' part of the stable unit treatment value assumption (SUTVA). The stable unit treatment value assumption (SUTVA) is given as \citep{rubin_causal_2005}:

\begin{blockquote}
(SUTVA) comprises two subassumptions. First, it assumes that \emph{there is no interference between units (Cox 1958)}; that is, neither $Y_i(1)$ nor $Y_i(0)$ is affected by what action any other unit received. Second, it assumes that \emph{there are no hidden versions of treatments}; no matter how unit $i$ received treatment $1$, the outcome that would be observed would be $Y_i(1)$ and similarly for treatment $0$.
\end{blockquote}

\todo[inline]{Not sure if or where I want to put this, I just think it helps to illustrate the difference}

Exchange commutativity is not equivalent to exchangeability in the sense of De Finetti's well-known theorem \citet{de_finetti_foresight_1992}. The latter can be understood as expressing an indifference between conducting the experiment as normal, or conducting the experiment and then swapping some labels. However, swapping \emph{choices} will (usually) lead to different ``pieces of the experiment'' receiving different treatment, which is something that can't be achieved by swapping labels after the experiment has concluded.

The difference is illustrated by the following pair of diagrams.

Exchangeability (swapping labels):

\begin{align}
    \tikzfig{exchangeability}
\end{align}

Exchange commutativity (swapping choices $\sim$ swapping labels):

\begin{align}
    \tikzfig{commutativity_of_exchange}
\end{align}

\todo[inline]{----end not sure where to put------}


% Another way to see where we are going is to consider graphical statements of our and De Finetti's result.

% Take $S=\{0,1\}$ and identify the space $\Delta(S)$ of probability measures on $S$ with the interval $[0,1]$. De Finetti showed that any infinite exchangeable probability measure $\prob{P}_\alpha$ on $\{0,1\}^\mathbb{N}$ can be represented by a prior $\prob{P}_\alpha^{\RV{H}}\in [0,1]$ for some $\RV{H}:\Omega\to H$ and a conditional probability $\prob{P}^{\RV{S}_0|\RV{H}}:[0,1]\kto \{0,1\}$ such that

% \begin{align}
%     \prob{P}_\alpha &= \tikzfig{de_finetti_rep0}\label{eq:definettirep}
% \end{align}

% Here $\prob{P}^{\RV{S}_0|\RV{H}}$ can be defined concretely by $\prob{P}^{\RV{S}_0|\RV{H}}(1|h)=h$. Equivalently, the probability gap model on $S^\mathbb{N}$ defined by the assumption of exchangeability is equivalent to the probability gap model defined by the conditional probability

% \begin{align}
%     \prob{P}^{\RV{S}|\RV{H}} = \tikzfig{de_finetti_conditional}
% \end{align}

% That is, there is some hypothesis $\RV{H}$ and conditional on $\RV{H}$ the measurements are independent and identically distributed. The proof of this is constructive -- $\RV{H}$ is a function of $\RV{S}$.



% \begin{align}
%     \prob{P}^{\RV{Y}|\RV{HD}} = \tikzfig{do_model_representation}
% \end{align}

% We will further argue that the class of see-do models considered in CBN and potential outcomes literature is equivalent to the family of causally contractible and exchangeable do-models where the decision rule for the first $n$ places is fixed to an unknown value, and may be freely chosen thereafter.

% \begin{theorem}[Existence of conditional in do models]
% Given a do model $(\prob{P}_{\square}^{\RV{Y}\|\RV{D}},R)$, for all $\alpha\in R$, $n\in\mathbb{N}$
% \begin{align}
%     \prob{P}_\alpha^{\RV{Y}_{[n]}\RV{D}_i} = \prob{P}_\alpha^{\RV{D}_{[n]}}\odot \prob{P}_\square^{\RV{Y}_{[n]}\|\RV{D}_{[n]}}
% \end{align}
% That is, $\prob{P}_\square^{\RV{Y}_{[n]}\|\RV{D}_{[n]}}\cong \prob{P}_\square^{\RV{Y}_{[n]}|\RV{D}_{[n]}}$
% \end{theorem}

% \begin{proof}
% For any $n>1\in \mathbb{N}$, $\alpha\in R$

% \begin{align}
%     \prob{P}_\alpha^{\RV{Y}_{[n]}\RV{D}_{[n]}} &= \tikzfig{do_model_1}\\
%     &= \tikzfig{do_model_2}\\
%     &= \tikzfig{do_model_3}\\
%     &= \tikzfig{do_model_4}\\
%     \implies \prob{P}_\alpha^{\RV{Y}_{[n]}|\RV{D}_{[n]}} &= \tikzfig{do_model_5}\\
%     &= \prob{P}_\alpha^{\RV{Y}_{[n-1]}|\RV{D}_{[n-1]}}\combprod \prob{P}_\square^{\RV{Y}_n|\RV{Y}_{[n-1]}\RV{D}_n}
% \end{align}

% Applying this recursively with $\prob{P}_\alpha^{\RV{Y}_{[1]}|\RV{D}_{[1]}}=\prob{P}_\square^{\RV{Y}_{[1]}|\RV{D}_{[1]}}$ yields

% \begin{align}
%     \prob{P}_\alpha^{\RV{Y}_{[n]}|\RV{D}_{[n]}} = \prob{P}_\square^{\RV{Y}_{[n]}\|\RV{D}_{[n]}}
% \end{align}
% as desired.
% \end{proof}
\subsection{Consequence contractibility}

We offer formal definitions of exchange commutativity and locality of consequences, as well as ``consequence contractibility'', which is the conjunction of both conditions.

A conditional probability model commutes with exchange if applying a permutation to the choice $\RV{D}_M$ ``before'' it is taken yields the same result as applying the corresponding permutation to $\RV{Y}_M$ ``after'' it is observed.

\begin{definition}[Swap map]
Given $M\subset \mathbb{N}$ a finite permutation $\rho:M\to M$ and a variable $\RV{X}:\Omega\to X^M$ such that $\RV{X}=(\RV{X}_i)_{i\in M}$, define the Markov kernel $\text{swap}_{\rho(\RV{X})}:X^M\kto X^M$ by $(d_i)_{i\in\mathbb{N}}\mapsto \delta_{(d_{\rho(i)})_{i\in\mathbb{N}}}$.
\end{definition}

\begin{definition}[Exchange commutativity]\label{def:caus_exch}
Suppose we have a sample space $(\Omega,\sigalg{F})$ and a conditional probability model $(\prob{P}_\square^{\RV{Y}|\RV{D}[M]},\{\prob{P}_{\tilde{\alpha}}^{\RV{D}}\}_A,f)$ with $\RV{Y}:=\RV{Y}_M:=(\RV{Y}_i)_{M}$, $\RV{D}:=\RV{D}_M:=(\RV{D}_i)_M$, $M\subseteq \mathbb{N}$. If, for any decision rule $\alpha \in A$,
\begin{align}
    \prob{P}_{\tilde{\alpha}}^{\RV{D}}\odot \text{swap}_{\rho(\RV{D})} \prob{P}_{\square}^{\RV{Y}|\RV{D}} &= \prob{P}_{\tilde{\alpha}}^{\RV{D}}\odot \prob{P}_{\square}^{\RV{Y}|\RV{D}}\text{swap}_{\rho(\RV{Y})}
\end{align}
Then $\prob{P}_\square$ \emph{commutes with exchange} with respect to $(\RV{D},\RV{Y})$.
\end{definition}

A conditional probability model exhibits locality of consequences if, given two different choices that agree on an subsequence of indices, the model yields identical outcomes if we restrict our attention to the subsequence on which the different choices match. For example, if we have $\RV{D}=(\RV{D}_1,\RV{D}_2,\RV{D}_3)$ and $\RV{Y}=(\RV{Y}_1,\RV{Y}_2,\RV{Y}_3)$ and $\prob{P}_{\tilde{\alpha}}^{\RV{D}_1\RV{D}_3}=\prob{P}_{\tilde{\beta}}^{\RV{D}_1\RV{D}_3}$ then $\prob{P}_{\alpha}^{\RV{Y}_1\RV{D}_1\RV{Y}_3\RV{D}_3}=\prob{P}_\beta^{\RV{Y}_1\RV{D}_1\RV{Y}_3\RV{D}_3}$.

\begin{definition}[Locality of consequences]\label{def:caus_cont}
Suppose we have a sample space $(\Omega,\sigalg{F})$ and a conditional probability model $(\prob{P}_\square^{\RV{Y}|\RV{D}[M]},\{\prob{P}_{\tilde{\alpha}}^{\RV{D}}\}_A,f)$ with $\RV{Y}:=\RV{Y}_M:=(\RV{Y}_i)_{M}$, $\RV{D}:=\RV{D}_M:=(\RV{D}_i)_M$, $M\subseteq \mathbb{N}$. For any ordered sequence $S=(s_i)_{i\in Q}$ where $Q\subset M$ and $i<j\implies s_i<s_j$, let $\RV{D}_S:=(\RV{D}_i)_{i\in S}$ and $\RV{D}_T:=(\RV{D}_i)_{i\in T}$. If for any $\alpha,\beta\in R$
\begin{align}
    \prob{P}_\alpha^{\RV{D}_{S}}&=\prob{P}_\beta^{\RV{D}_{S}}\\
    \implies \prob{P}_\alpha^{(\RV{D_i,Y_i})_{i\in S}}&=\prob{P}_\beta^{(\RV{D_i,Y_i})_{i\in S}}
\end{align}
then $\prob{P}_\square$ exhibits \emph{locality of consequences} with respect to $(\RV{D},\RV{Y})$.
\end{definition}

Neither condition implies the other. 
\begin{lemma}
Exchange commutativity does not imply locality of consequences or vise versa.
\end{lemma}

\begin{proof}
A conditional probability model that exhibits exchange commutativity but some choices have non-local consequences:

Suppose $D=Y=\{0,1\}$ and we have a conditional probability model $(\prob{P}_\square^{\RV{Y}|\RV{D}[M]},\{\prob{P}_{\tilde{\alpha}}^{\RV{D}}\}_A,f)$ where $\RV{D}=(\RV{D}_1,\RV{D}_2)$, $\RV{Y}=(\RV{Y}_1,\RV{Y}_2)$ and $A$ contains all deterministic probability measures in $\Delta(D^2)$. If

\begin{align}
    \prob{P}_\square^{\RV{Y}_1\RV{Y}_2|\RV{D}_1\RV{D}_2}(y_1,y_2|d_1,d_2) &= \llbracket (y_1,y_2)= (d_1+d_2,d_1+d_2) \rrbracket
\end{align}

Then $\prob{P}_{\delta_{00}}^{\RV{Y}_1\RV{D}_1}(y_1) = \llbracket y_1=0\rrbracket$ while $\prob{P}_{\delta_{01}}^{\RV{Y}_1} = \llbracket y_1=1 \rrbracket$. However, $\delta_00^{\RV{D}_1}=\delta_{01}^{\RV{D}_1}=\delta_0^{\RV{D}_1}$ so $\prob{P}_\square$ exhibits non-local consequences. However, taking $(d_i,d_j):=\delta_{d_i d_j}\in A$,

\begin{align}
    \prob{P}_{d_2,d_1}^{\RV{Y}_1\RV{D}_1\RV{Y}_2\RV{D}_2}(y_1,d_1,y_2,d_2) &= \llbracket (y_1,y_2)= (d_2+d_1,d_2+d_1) \rrbracket\\
    &= \llbracket (y_2,y_1)= (d_1+d_2,d_1+d_2) \rrbracket\\
    &= \prob{P}_{d_1,d_2}^{\RV{Y}_1\RV{D}_1\RV{Y}_2\RV{D}_2}(y_2,d_2,y_1,d_1)
\end{align}

so $\prob{P}_\square$ commutes with exchange.

A conditional probability model that exhibits locality of consequences but does not commute with exchange:

Alternatively, suppose the same setup, but define $\prob{P}_\square$ instead by
\begin{align}
    \prob{P}_\square{\RV{Y}_1\RV{Y}_2|\RV{D}_1\RV{D}_2}(y_1,y_2|d_1,d_2) &= \llbracket (y_1,y_2)= (0,1) \rrbracket
\end{align}
for all $\alpha\in A$.

Then $\prob{P}_\square$ exhibits locality of consequences. If $\prob{P}_\alpha^{\RV{D}_S}=\prob{P}_\beta^{\RV{D}_S}$ for $S\subset\{0,1\}$ then:
\begin{align}
    \prob{P}_{\alpha}^{\RV{Y}_S\RV{D}_S}(y_s,d_s) &= \sum_{y'_2\in \{0,1\}^{S^C}} \llbracket (y_1,y_2)= (0,1) \rrbracket\prob{P}_\alpha^{\RV{D}_S}(d_s) \\
                                                  &= \prob{P}_{\beta}^{\RV{Y}_S\RV{D}_S}(y_s,d_s)
\end{align}

However, $\prob{P}_\square$ does not commute with exchange. For all $\alpha,\beta \in A$:
\begin{align}
    \prob{P}_\alpha{\RV{Y}_1\RV{Y}_2}(y_1,y_2) &= \llbracket (y_1,y_2)= (0,1) \rrbracket\\
    &\neq \prob{P}_\beta{\RV{Y}_1\RV{Y}_2}(y_2,y_1)
\end{align}
\end{proof}

Although locality of consequences has a lot in common with an assumption non-interference, it still allows for some models in which exhibit certain kinds of interference between actions and outcomes of different indices. For example: I have an experiment where I first flip a coin and record the results of this flip as the outcome of the first step of the experiment, but I can choose either to record this same outcome as the provisional result of the second step (this is the choice $\RV{D}_1=0$), or choose to flip a second coin and record the result of that as the provisional result of the second step of the experiment (this is the choice $\RV{D}_1=1$). At the second step, I may further choose to copy the provisional results ($\RV{D}_2=0$) or invert them ($\RV{D}_2=1$). Then
\begin{itemize}
    \item The marginal distribution of both experiments in isolation is $\text{Bernoulli}(0.5)$ no matter what choices I make, so a model of this experiment would satisfies Definition \ref{def:caus_cont}
    \item Nevertheless, the choice for the first experiment affects the result of the second experiment
\end{itemize}

Note that this example would not satisfy exchange commutativity.

We call the conjunction of exchange commutativity and consequence locatlity \emph{consequence contractibility}.

\begin{definition}[Consequence contractibility]
A conditional probability model $(\prob{P}_{\square}^{\overline{\RV{Y}|\RV{D}}},A)$ is causally contractible if it is both commutative with exchange and commutative with marginalisation.
\end{definition}

% \begin{proposition}[Representation of do-models that commute with exchange]
% Suppose we have a fundamental probability set $\Omega$ and a do model $(\prob{P},\RV{D},\RV{Y},R)$ such that $\RV{D}:=(\RV{D}_i)_{i\in \mathbb{N}}$ and $\RV{Y}:=(\RV{Y}_i)_{i\in\mathbb{N}}$ where $\prob{P}$ commutes with exchange and there is some $\alpha^*\in R$ such that $\prob{P}^{\alpha^*}\gg\prob{P}_\beta$ for all $\beta in R$. Then there exists a symmetric function $\RV{H}:(Y\times D)^\mathbb{N}\to H$ such that  $\prob{P}^{\RV{Y}|\RV{DH}}$ exists and $\RV{Y}_i\CI_{\prob{P}}(\RV{D}_j,\RV{Y}_j)_{j\in \mathbb{N}}\setminus \{i\}|\RV{H}\RV{D}_i$, or equivalently 
% \begin{align}
%     \prob{P}^{\RV{Y}} &= \tikzfig{do_model_representation}
% \end{align}
% \end{proposition}

% % \begin{lemma}[Contraction and independence]
% % Let $\RV{J}$, $\RV{K}$ and $\RV{L}$ be variables on $\Omega$ and $\prob{Q}\in \Delta(\Omega)$ a base measure such that $\prob{Q}^{\RV{JK}}=\prob{Q}^{\RV{JL}}$ and $\sigma{K}\subset \sigma{L}$. Then $\RV{J}\CI\RV{L}|\RV{K}$. 
% % \end{lemma}

% % \begin{proof}
% % From Lemma 1.3 in \citet{kallenberg_basic_2005}.
% % \end{proof}

% \begin{proof}
% If $\prob{P}$ commutes with exchange, then for any $\alpha\in R$ such that $\prob{P}_\alpha^{\RV{D}}$ is exchangeable then $\prob{P}_\alpha$ is also exchangeable. Then there exists $\RV{H}$ a symmetric function of $(\RV{Y}_i,\RV{D}_i)_{i\in\mathbb{N}}$ such that $\RV{Y}_i\CI_{\prob{P}}(\RV{D}_j,\RV{Y}_j)_{j\in \mathbb{N}}\setminus \{i\}|\RV{H}\RV{D}_i$. This is De Finetti's representation theorem, and many proofs exists, see for example \citep{kallenberg_basic_2005}.

% In particular, let 

% \begin{align}
%     \RV{H}:=A\times B\mapsto \lim_{n\to\infty} \frac{1}{n}\sum_{i\in n} \mathds{1}_{A\times B}((\RV{Y}_i, \RV{D}_i))
% \end{align}

% Then for all $\alpha\in R$,
% \begin{align}
%     \prob{P}_\alpha^{(\RV{Y}_i,\RV{D}_i)_{i\in\mathbb{N}}|\RV{H}}(A\times B|h) \overset{a.s.}{=} h(A\times B)\label{eq:given_h}
% \end{align}

% The proof that the limit exists and the above equality holds can again be found int \citep{kallenberg_basic_2005}.
% \end{proof}

\subsection{Repeatable response conditionals exist iff a model is consequence contractible}

The main result in this section is Theorem \ref{th:iid_rep} which shows that a conditional probability model $\prob{P}_\square$ is causally contractible if and only if it can be represented as the product of a distribution over hypotheses $\prob{P}_\square^{\RV{H}}$ and a collection of identical conditional probabilities $\prob{P}_\square^{\RV{Y}_1|\RV{D}_1\RV{H}}$. Note the hypothesis $\RV{H}$ that appears in this conditional; it can be given the interpretation of a random variable that expresses the ``true but initially unknown'' $\RV{Y}_1|\RV{D}_1$ conditional probability.

\begin{lemma}[Exchangeable randomness pushback]\label{th:table_rep}
Given a conditional probability model $(\prob{P}_\square^{\RV{Y}|\RV{D}},A)$ such that $\RV{D}:=(\RV{D}_i)_{i\in \mathbb{N}}$ and $\RV{Y}:=(\RV{Y}_i)_{i\in \mathbb{N}}$, $\prob{P}_\square$ is consequence contractible if and only if there exists a column exchangeable probability distribution $\mu^{\RV{Y}^D}\in \Delta(Y^{|D|\times \mathbb{N}})$ such that
\begin{align}
    \prob{P}_\square^{\RV{Y}|\RV{D}} &= \tikzfig{lookup_representation}\label{eq:lup_rep}\\
    &\iff\\
    \prob{P}_\square^{\RV{Y}|\RV{D}}(y|(d_i)_{i\in \mathbb{N}}) &= \mu^{\RV{Y}^D} \Pi_{(d_i i)_{i\in\mathbb{N}}}(y)
\end{align}
Where $\Pi_{(d_i i)_{i\in\mathbb{N}}}:Y^{|D|\times \mathbb{N}}\to Y^{\mathbb{N}}$ is the function that projects the $(d_i,i)$ indices for all $i\in \mathbb{N}$ and $\prob{F}_{\text{ev}}$ is the Markov kernel associated with the evaluation map
\begin{align}
    \text{ev}:D^\mathbb{N}\times Y^{D\times \mathbb{N}}&\to Y\\
    ((d_i)_\mathbb{N},(y_{ij})_{i\in D,j\in \mathbb{N}})&\mapsto (y_{d_i i})_{i\in \mathbb{N}}
\end{align}
\end{lemma}

\begin{proof}
Only if:
Consider the weaker conditional probability model $(\prob{P}_\square^{\RV{Y}|\RV{D}},A')$ where $A'\supset A$ contains all $\alpha$ such that $\prob{P}_\alpha^{\RV{D}}$ is deterministic. If the result holds for $A'$ then it holds also for $A$ which is a subset.

For all $d\in D$, abuse notation to say taht $\prob{P}_d$ is a probability set in $A'$ such that $\prob{P}_d^{\RV{D}}=\delta_d$. For any $\alpha\in A'$, we have

\begin{align}
    \prob{P}_\alpha^{\RV{DY}}(B\times C) &= \int_B \prob{P}_\square^{\RV{Y}|\RV{D}}(C|d)\prob{P}_\alpha^{\RV{D}}(\mathrm{d}d)\\
                                &= \int_B \int_D \prob{P}_\square^{\RV{Y}|\RV{D}}(C|d')\prob{P}_d^{\RV{D}}(\mathrm{d}d')\prob{P}_\alpha^{\RV{D}}(\mathrm{d}d)\\
                                &= \int_B \prob{P}_d^{\RV{Y}}(C)\prob{P}_\alpha^{\RV{D}}(\mathrm{d}d)
\end{align}

Thus $d\mapsto \prob{P}_d^{\RV{Y}}$ is a version of $\prob{P}_\square^{\RV{Y}|\RV{C}}$.

Choose $e:=(e_i)_{i\in\mathbb{N}}$ such that $e_{|D|i+j}$ is the $i$th element of $D$ for all $i,j\in \mathbb{N}$. Abusing notation, write $e$ also for the decision function that chooses $e$ deterministically.

Define
\begin{align}
    \mu^{\RV{Y}^D}((y_{ij})_{D\times \mathbb{N}}):=\prob{P}_e^{\RV{Y}}((y_{|D|i+j})_{i\in D, j\in \mathbb{N}})
\end{align}

Now consider any $d:=(d_i)_{i\in \mathbb{N}}\in D^{\mathbb{N}}$. By definition of $e$, $e_{|D|d_i + i}=d_i$ for any $i,j\in \mathbb{N}$.

Define
\begin{align}
    \prob{Q}:D\kto Y\\
    \prob{Q}:= \tikzfig{lookup_representation}
\end{align}

and consider some ordered sequence $A\subset \mathbb{N}$ and $B:= ((|D|d_i+i))_{i\in A}$. Note that $e_B:=(e_{|D|d_i +i})_{i\in B}=d_A=(d_i)_{i\in A}$. Then 

\begin{align}
    \sum_{y\in \RV{Y}^{-1}(y_A)} \prob{Q}(y|d) &= \sum_{y\in \RV{Y}^{-1}(y_A)} \mu^{(\RV{Y}^{D}_{d_ii})_{A}}(y) \\
    &= \sum_{y\in \RV{Y}^{-1}(y_A)} \prob{P}_e^{(\RV{Y}_{|D|d_i+i})_{A}}(y)\\
    &= \prob{P}_e^{\RV{Y}_{B}}(y_A)\\
    &= \prob{P}_{d}^{\RV{Y}_A}(y_A)&\text{by consequence contractibility}
\end{align}

Because this holds for all $A\subset\mathbb{N}$, by the Kolmogorov extension theorem

\begin{align}
    \prob{Q}(y|d) &= \prob{P}_d^{\RV{Y}}(y)
\end{align}

And so $\prob{Q}$ is also a version of $\prob{P}_\square^{\RV{Y}|\RV{C}}$.

Next we will show $\mu^{\RV{Y}^D}$ is exchangeable. Consider any subsequences $\RV{Y}^D_S$ and $\RV{Y}^D_T$ of $\RV{Y}^D$ with $|S|=|T|$. Let $\rho(S)$ be the ``expansion'' of the indices $S$, i.e. $\rho(S)=(|D|i+j)_{i\in S,j\in D}$. Then by construction of $e$, $e_{\rho(S)}=e_{\rho(T)}$ and therefore

\begin{align}
    \mu^{\RV{Y}^D}\Pi_S&= \prob{P}_e^{\RV{Y}_{\rho(S)}})\\
    &= \prob{P}_e^{\RV{Y}_{\rho(T)}})&\text{by contractibility of }\prob{P}_\square\text{ and the equality } e_{\rho(S)}=e_{\rho(T)}\\
    &= \mu^{\RV{Y}^D}\Pi_T
\end{align}


If:
Suppose 
\begin{align}
    \prob{P}_\square^{\RV{Y}|\RV{D}} &= \tikzfig{lookup_representation}
\end{align}

and consider any two deterministic decision functions $d,d'\in D^{\mathbb{N}}$ such that some subsequences are equal $d_S=d'_T$.

Let $\RV{Y}^{d_S}=(\RV{Y}_{d_i i})_{i\in S}$.

By definition,

\begin{align}
    \prob{P}_\square^{\RV{Y}_S|\RV{D}}(y_S|d) &= \sum_{y^D_S\in Y^{|D|\times |S|}}\mu^{\RV{Y}^D} \Pi_S(y^D_S)\prob{F}_{\text{ev}}(y_S|d,y^D_S)\\
    &= \sum_{y^D_S\in Y^{|D|\times |T|}}\prob{P}_\square^{\RV{Y}^D_T}(y^D_S)\prob{F}_{\text{ev}}(y_S|d,y^D_S)&\text{ by contractibility of }\mu^{\RV{Y}^D}\Pi_T \\
    &= \prob{P}_\square^{\RV{Y}_T|\RV{D}}(y_S|d)
\end{align}
\end{proof}

We have specialised notation for talking about marginals, conditionals, conditional independence and so forth. It is useful to use this notation to discuss properties of the representation in Equation \ref{eq:lup_rep}. Thus we adopt the convention that, given a consequence contractible model, the sample space $(\Omega,\sigalg{F})$ contains a variable $\RV{Y}^D:\Omega\to Y^{|D|\times\mathbb{N}}$ such that $\prob{P}_\square^{\RV{Y}^D}$ exists and is equal to $\mu^{\RV{Y}^D}$ in Lemma \ref{th:table_rep}.

As we pointed out, there are similarities between $\RV{Y}^D$ in a standard consequence contractible model and the potential outcomes variables in potential outcomes models. However, the $\RV{Y}^D$ in our models usually can't be interpreted as potential outcomes. For example, consider a series of bets on fair coinflips. Model the consequence $\RV{Y}_i$ as uniform on $\{0,1\}$ for any decision $\RV{D}_i$, for all $i$. Specifically, $D=Y=\{0,1\}$ and $\prob{P}_\alpha^{\RV{Y}_n}(y)=\prod_{i\in [n]} 0.5$ for all $n$, $y\in Y^n$, $\alpha\in R$. Then the construction of $\prob{P}^{\RV{Y}^D}$ following the method in Lemma \ref{th:table_rep} yields $\prob{P}^{Y^D_i}(y^D_i)=\prod_{j\in D} 0.5$ for all $y^D_i\in Y^D$. In this model $\RV{Y}^0_i$ and $\RV{Y}^1_i$ are independent and uniformly distributed. However, if we wanted $\RV{Y}^0_i$ to be interpretable as ``what would happen if I bet on outcome 0 on turn $i$'' and $\RV{Y}^1$ to represent ``what would happen if I bet on outcome 1 on turn $i$'', then we ought to have $\RV{Y}^0_i = 1-\RV{Y}^1_i$. 

Lemma \ref{th:table_rep} also does not establish that causal contractibility is necessary for the existence of a potential outcomes. A counterexample is a potential outcomes model with potential outcomes $\RV{Z}^D$ where the distribution $\prob{P}_\square^{\RV{Z}^D}$ exists and is not column exchangeable. Such a model is not consequence contractible.

The tabular distribution $\prob{P}_\square^{\RV{Y}^D}$ along with the evaluation function $\kernel{F}_{\text{ev}}$ is a randomness pushback of the conditional probability $\prob{P}_\square^{\RV{Y}|\RV{D}}$. Because $\prob{P}_\square^{\RV{Y}^D}$ is a column exchangeable probability distribution we can apply De Finetti's theorem to show $\prob{P}_\square^{\RV{Y}^D}$ is representable as a product of identical parallel copies of $\prob{P}_\square^{\RV{Y}_1^D|\RV{H}}$ and a common prior $\prob{P}_\square^{\RV{H}}$. This in turn can be used to show that $\prob{P}_\square^{\RV{Y}|\RV{D}}$ can be represented as a product of identical parallel copies of $\prob{P}_\square^{\RV{Y}_1|\RV{D}_1\RV{H}}$ and the same common prior $\prob{P}_\square^{\RV{H}}$. This is the main result: the copies of $\prob{P}_\square^{\RV{Y}_1|\RV{D}_1\RV{H}}$ are the repeatable response conditionals.

\begin{theorem}\label{th:iid_rep}
Suppose we have a sample space $(\Omega,\sigalg{F})$ and a conditional probability model $(\prob{P}^{\RV{Y}|\RV{D}}_\square,A)$ such that $\RV{D}:=(\RV{D}_i)_{i\in \mathbb{N}}$ and $\RV{Y}:=(\RV{Y}_i)_{i\in \mathbb{N}}$. $\prob{P}_\square$ is causally contractible if and only if there exists some $\RV{H}:\Omega\to H$ such that $\prob{P}^{\RV{H}}_\square$ and $\prob{P}_\square^{\RV{Y}_i|\RV{H}\RV{D}_i}$ exist for all $i\in \mathbb{N}$ and
\begin{align}
    \prob{P}_\square^{\RV{Y}|\RV{D}} &= \tikzfig{do_model_representation}\\
    &\iff\\
    \RV{Y}_i&\CI_{\prob{P}_\square} \RV{Y}_{\mathbb{N}\setminus i},\RV{D}_{\mathbb{N}\setminus i}|\RV{H}\RV{D}_i&\forall i\in \mathbb{N}\\
    \land \RV{H} \CI_{\prob{P}_\square} \RV{D}\\
    \land \prob{P}_\square^{\RV{Y}_i|\RV{H}\RV{D}_i} &= \prob{P}^{\RV{Y}_0|\RV{H}\RV{D}_0} & \forall i\in \mathbb{N}
\end{align}
Where $\Pi_{D,i}:D^\mathbb{N}\kto D$ is the $i$th projection map.
\end{theorem}

\begin{proof}
We make use of Lemma \ref{th:table_rep} to show that we can represent the conditional probability $\prob{P}_\square^{\RV{Y}|\RV{D}}$ as
\begin{align}
        \prob{P}_\square^{\RV{Y}|\RV{D}} &= \tikzfig{lookup_representation}\\
\end{align}

As a preliminary, we will show
\begin{align}
    \kernel{F}_{\mathrm{ev}} &= \tikzfig{lookup_rep_intermediate}\label{eq:ev_alternate_rep}
\end{align}

Where $\Pi_{Y^D,i}:Y^{D\times\mathbb{N}}\kto Y^D$ is the $i$th column projection map on $\RV{Y}^{D\times \mathbb{N}}$ and $\mathrm{ev}_{Y^D\times D}:Y^D\times D\to Y$ is the evaluation function
\begin{align}
    ((y_i)_{i\in D},d)\mapsto y_d
\end{align}

Recall that $\mathrm{ev}$ is the function

\begin{align}
    ((d_i)_\mathbb{N},(y_{ij})_{i\in D,j\in \mathbb{N}})&\mapsto (y_{d_i i})_{i\in \mathbb{N}}
\end{align}

By definition, for any $\{A_i\in\sigalg{Y}|i\in \mathbb{N}\}$

\begin{align}
    \kernel{F}_{\mathrm{ev}}(\prod_{i\in \mathbb{N}}A_i|(d_i)_\mathbb{N},(y_{ij})_{i\in D,j\in \mathbb{N}}) &= \delta_{(y_{d_i i})_{i\in \mathbb{N}}}(\prod_{i\in \mathbb{N}}A_i)\\
        &= \prod_{i\in \mathbb{N}} \delta_{y_{d_i i}} (A_i)\\
        &= \mathrm{copy}^{\mathbb{N}}\prod_{i\in \mathbb{N}}( \Pi_{D,i}\otimes \Pi_{Y,i})\kernel{F}_{\mathrm{ev}_{Y^D\times D}}
\end{align}

Which is what we wanted to show.

Only if:
With $\prob{P}^{\RV{Y}^D}_\square$ column exchangeable. That is, noting that $\RV{Y}^D=(\RV{Y}^D_i)_{i\in \mathbb{N}}$, the $\RV{Y}^D_i$ are exchangeable with respect to $\prob{P}^{\RV{Y}^D}_\square$. From \citet{kallenberg_basic_2005} we have a directing random measure $\RV{H}$ such that
\begin{align}
    \prob{P}_\square^{\RV{Y}^D|\RV{H}} &= \tikzfig{de_finetti_representation}\\
    &\iff\\
    \prob{P}_\square^{\RV{Y}^D|\RV{H}}(\prod_{i\in \mathbb{N}} A_i|h) &= \prod_{i\in \mathbb{N}} \prob{P}_\square^{\RV{Y}_0^D|\RV{H}}(A_i|h)
\end{align}

Furthermore, because $\RV{Y}$ is a deterministic function of $\RV{D}$ and $\RV{Y}^D$, $\RV{Y}\CI_{\prob{P}_\square} \RV{H}|(\RV{D},\RV{Y}^D)$ and by definition of $\RV{Y}^D$, $\RV{Y}^D\CI_{\prob{P}_\square}\RV{D}$ and so

\begin{align}
    \prob{P}_\square^{\RV{Y}|\RV{HD}} &= \prob{P}_\square^{\RV{Y}^D|\RV{HD}}\odot \prob{P}_\square^{\RV{Y}|\RV{Y}^D\RV{HD}}\\
                                      &= \tikzfig{Y_pushback_factorisation}
                                      &= \tikzfig{do_model_representation}
\end{align}

If:
By assumption
\begin{align}
    \prob{P}_\square^{\RV{Y}|\RV{D}}(\prod_{i\in \mathbb{N}} A_i|h,(d_i)_{i\in \mathbb{N}}) &= \int_H \prod_{i\in \mathbb{N}}\prob{P}_\square^{\RV{Y}_1|\RV{HD}_1}(A_i|h,d_i)\prob{P}_\square^{\RV{H}}(\mathrm{d}h)
\end{align}

Consider $\alpha,\alpha'$ such that $\prob{P}^{\RV{D}_M}_\alpha = \prob{P}^{\RV{D}_L}_{\alpha'}$ for $L,M\subset \mathbb{N}$ with $|M|=|L|$, both finite. Then

\begin{align}
    \prob{P}_\alpha^{\RV{Y}_M}(A) &= \int_{D^{\mathbb{N}}} \prob{P}_\alpha^{\RV{Y}_M|\RV{D}}(A|d)\prob{P}_\alpha^{\RV{D}}(\mathrm{d}d)\\
                                  &= \int_H\int_{D^{\mathbb{N}}} \prod_{i\in M}\prob{P}_\square^{\RV{Y}_1|\RV{HD}_1}(A_i|h,d_i)\prob{P}_\alpha^{\RV{D}}(\mathrm{d}d)\prob{P}_\square^{\RV{H}}(\mathrm{d}h)\\
                                  &= \int_H\int_{D^{|M|}} \prod_{i\in M}\prob{P}_\square^{\RV{Y}_1|\RV{HD}_1}(A_i|h,d_i)\prob{P}_\alpha^{\RV{D}_M}(\mathrm{d}d_M)\prob{P}_\square^{\RV{H}}(\mathrm{d}h)\\
                                  &= \int_H\int_{D^{|M|}} \prod_{i\in M}\prob{P}_\square^{\RV{Y}_1|\RV{HD}_1}(A_i|h,d_i)\prob{P}_{\alpha'}^{\RV{D}_N}(\mathrm{d}d_N)\prob{P}_\square^{\RV{H}}(\mathrm{d}h)\\
                                  &= \int_H\int_{D^{\mathbb{N}}} \prod_{i\in M}\prob{P}_\square^{\RV{Y}_1|\RV{HD}_1}(A_i|h,d_i)\prob{P}_{\alpha'}^{\RV{D}}(\mathrm{d}d)\prob{P}_\square^{\RV{H}}(\mathrm{d}h)\\
                                  &= \prob{P}_{\alpha'}^{\RV{Y}_M}(A)
\end{align}

\end{proof}

\subsection{Modelling different decision procedures}

We have a formal condition -- consequence contractibility -- that is equivalent to the existence of repeatable response conditionals. A conditional probability model of an experiment for which there is some ``fixed, unknown probabilistic causal effect'' of $\RV{X}_i$ on $\RV{Y}_i$ for all $i$ must be consequence contractible. This means that any subsequence of $\RV{X}_i$s and corresponding $\RV{Y}_i$s for which the $\RV{X}_i$s are matched must share exactly the same model. 

To understand whether such an assumption is justified, we can't look only at the model -- we have to consider the measurement procedure. It's not a mathematical fact that a conditional probability model should exhibit the independence $\RV{X}_i\CI_{\prob{P}_\square} \RV{Y}_i$. On the other hand, if the description of the measurement procedure supports the judgement that what the model says regarding $\proc{Y}_i$ should be the same before and after one learns the value yielded by $\proc{X}_i$, then it is appropriate to adopt a model in which $\RV{X}_i\CI_{\prob{P}_\square} \RV{Y}_i$.

We will reason about decision procedures in the following way:

\begin{itemize}
    \item Propose a decision procedure $\proc{S}$ and assume a conditional probability model $\prob{P}_\square$ associated with it
    \item Consider an alternative decision procedure $\proc{S}'$ and argue that a conditional probability model $\prob{P}_\square'$, related somehow to $\prob{P}_\square$, is appropriate to model it
    \item If, in addition, we accept that \emph{the same model} is appropriate for both $\proc{S}$ and $\proc{S}'$, then we have $\prob{P}_\square=\prob{P}_\square'$
\end{itemize}

An example of this reasoning is found in discussions of exchangeability, outside the setting of decision problems. Suppose we have a measurement procedure $\proc{S}$ and an observed variable $(\RV{Y}\circ\proc{S},\RV{Y})$ where $\RV{Y}=(\RV{Y}_i)_{i\in \mathbb{N}}$ for $\RV{Y}_i:\Omega\to Y$ -- i.e. $\proc{S}$ yields a countably infinite sequence of values from the set $Y$. Consider an alternative observed variable $(\RV{Y}'\circ\proc{S},\RV{Y}')$ where $\RV{Y}'=\text{swap}\circ\RV{Y}$. $\proc{Y}'$ is the procedure ``do whatever is done to measure $\proc{Y}$, then shuffle the results according to $\text{swap}$''.

$\proc{S}$ is modeled by some ordinary probability model $(\mu,\Omega,\sigalg{F})$, with marginal distribution $\mu^{\RV{Y}}$ of $\RV{Y}$. It may be the case that we think that the additional shuffle operation involved in $\proc{Y}'$ does not in any way alter the appropriate probability model associated with the variable. Thus $\mu^{\RV{Y}}=\mu^{\RV{Y}'}$, and the model $\mu$ is exchangeable with respect to $\RV{Y}$.

In the following section, we will apply this approach to considering whether the assumption of exchange commutativity is justified for a given decision procedure. The approach outlined can be used to assess whether measurement procedures support exchange commutativity or consequence locality. However, we will focus on exchange commutativity because, out of the two assumptions, we have more to say about it.

% As an aside, consequence contractibility means that a model $\prob{P}_\square$ must say precisely the same thing about any subsequence of (choice, outcome) pairs, provided both subsequences have a matching distribution over choices. Suppose there is some experiment in which some pairs arise from passive observations and others from active interventions. If we are to suppose that the whole sample supports ``fixed but unknown probabilistic causal effects'' -- that is, it is consequence contractible -- then we are equivalently claiming that a subsequence of $m+1$ $(\RV{X}_i,\RV{Y}_i)$ pairs generated only by active interventions is modeled identically to a sequence of $m$ pairs generated by passive observation and $1$ pair generated by active intervention. This suggests -- loosely speaking -- that assuming ``fixed but unknown probabilistic causal effects'' are identified from observations is the same as assuming that observational evidence is precisely as good as experimental evidence for learning said causal effects. This is an idle observation and not a rigorous argument.

\subsection{Decision procedures with response conditionals}

Suppose we have a decision procedure $\proc{S}_A$. We consider an exchange commutative model appropriate if we think that the experiment given by
\begin{itemize}
    \item Suppose $\alpha$ has been decided on
    \item Obtain a sequence of choices according to $\proc{D}$ (which depends on $\alpha$) and enact them
    \item Measure the consequences according to $\proc{Y}$
\end{itemize}
is equivalent to $\proc{S}'$ (in the sense of requiring the same model):
\begin{itemize}
    \item Decide on $\alpha'$ such that $\prob{P}_\alpha^{\RV{D}}\text{swap}=\prob{P}_{\alpha'}^{\RV{D}}$
    \item Obtain a sequence of choices according to $\proc{D}$ (which depends on $\alpha'$) and enact them
    \item Measure preliminary consequences according to $\proc{Y}$
    \item Apply the inverse shuffle $\text{swap}^{-1}$ to the preliminary consequences to get the consequences of interest $\proc{Y}'$
\end{itemize}

Unlike in the case of ordinary exchangeability, we need to consider two measurement procedures in which different decisions are made. Conditional probability models of decision problems express judgements that hold whatever decision is made. Under this interpretation, a conditional probability model $\prob{P}_\square^{\RV{Y}|\RV{D}}$ can tell us about the comparison of $\proc{S}$ and $\proc{S}'$ (see Section \ref{sec:cp_model}). In particular, it expresses the following attitude:

\begin{itemize}
    \item If we decide on $\alpha$, then the consequences are described by $\prob{P}_\alpha^{\RV{D}} \prob{P}_\square^{\RV{Y}|\RV{D}}$
    \item If we decide on $\alpha'$ such that $\prob{P}_{\alpha'}^{\RV{D}}=\prob{P}_\alpha^{\RV{D}}\text{swap}$ then then the consequences are described by $\prob{P}_\alpha^{\RV{D}}\text{swap}\prob{P}_\square^{\RV{Y}|\RV{D}}$
\end{itemize}

Thus if we hold that $\proc{S}$ is equivalent to $\proc{S}'$ for all $\alpha\in A$, we conclude that $\prob{P}_\square$ commutes with exchange.


There is a related kind of symmetry that involves shuffling ``experimental units''. This is not an operation that can be described purely mathematically, but there may be a more intuitively appealing case available that this kind of shuffle yields an equivalent experiment. For example, if an ``experimental unit'' is a patient who receives a treatment and whose recovery is then followed up on as a consequences, we might be willing to regard an experiment equivalent if the order of patients in the experiment is shuffled. We think it is sometimes reasonable to deduce exchange commutativity from this kind of symmetry, but as we will see care is needed when doing so.

\subsection{Example: exchange commutativity in the context of treatment choices}

Consider the following two scenarios:

\begin{enumerate}
    \item Dr Alice is going to see two patients who are both complaining of lower back pain. Prior to seeing either, she decides deterministically on $(\proc{D}_1,\proc{D}_2)$ for treatment decisions $\proc{D}_1$ and $\proc{D}_2$
    \item As before, but $\proc{D}_1$ is chosen after examining patient 1, and $\proc{D}_2$ after examining patient 2
\end{enumerate}

Alice could model either situation with a conditional probability model $(\prob{P}_\square^{\RV{Y}_1\RV{Y}_2|\RV{D}_1\RV{D}_2},A)$. In either situation, might the model be exchange commutative?

For each scenario, we want to consider two measurement procedures: first, the ``original'' measurement procedure, and then the measurement procedure with a swapped choices and consequences of interest. The question we want to consider is whether both procedures should be considered equivalent.

We will make two assumptions about measurement procedure equivalence: first, a measurement procedure is equivalent to an identical procedure in which patients are interchanged. Second, a measurement procedure is equivalent to an identical procedure in which the order of treatment and measurement of outcomes is interchanged.

We will describe measurement procedures using pseudocode, because this offers the opportunity to be precise about operations like swaps. Note that descriptions of measurement procedures, in pseudocode or otherwise, are incomplete descriptions.

Suppose the first scenario corresponds to the following procedure $\proc{S}$.
\begin{algorithmic}
    \Procedure{$\proc{S}$}{}
    \State $(\proc{D}_1,\proc{D}_2) \gets \mathrm{choose\_treatments}$
    \State $\proc{Y}_1\gets \mathrm{apply}(\proc{D}_1,\mathrm{patient\;A})$
    \State $\proc{Y}_2\gets \mathrm{apply}(\proc{D}_2,\mathrm{patient\;B})$
    \State \Return $(\alpha,\proc{D}_1,\proc{D}_2,\proc{Y}_1,\proc{Y}_2)$
    \EndProcedure
\end{algorithmic}

Our assumption of patient interchangeability means that the following procedure is equivalent

\begin{algorithmic}
    \Procedure{$\proc{S}$}{}
    \State $(\proc{D}_1,\proc{D}_2) \gets \mathrm{choose\_treatments}$
    \State $\proc{Y}_1\gets \mathrm{apply}(\proc{D}_1,\mathrm{patient\;B})$
    \State $\proc{Y}_2\gets \mathrm{apply}(\proc{D}_2,\mathrm{patient\;A})$
    \State \Return $(\alpha,\proc{D}_1,\proc{D}_2,\proc{Y}_1,\proc{Y}_2)$
    \EndProcedure
\end{algorithmic}

Consider the swapped procedure

\begin{algorithmic}
    \Procedure{$\proc{S}$}{}
    \State $(\proc{D}_1,\proc{D}_2) \gets \mathrm{choose\_treatments}$
    \State $\proc{Y}_1\gets \mathrm{apply}(\proc{D}_1,\mathrm{patient\;A})$
    \State $\proc{Y}_2\gets \mathrm{apply}(\proc{D}_2,\mathrm{patient\;B})$
    \State \Return $(\alpha,\proc{D}_1,\proc{D}_2,\proc{Y}_1,\proc{Y}_2)$
    \EndProcedure
\end{algorithmic}


Make the assumption that, on the basis that the patients are indistinguishable to Alice at the time of model construction, the same model is appropriate for the original measurement procedure and a modified measurement procedure in which the patients are swapped (we say the measurement procedures are ``equivalent''). Assume also that swapping the order of treatment and swapping the order in which outcomes are recorded yields an equivalent measurment procedure (in \citet{walley_statistical_1991}'s language, the first assumption is based on ``symmetry of evidence'' and the second on ``evidence of symmetry''). Putting these two assumptions together, the following procedure $\proc{S}'$ is equivalent to the original:

\begin{algorithmic}
    \Procedure{$\proc{S}'$}{}
    \Assert{patient A knowledge=patient B knowledge}
    \State $\alpha \gets \mathrm{choose\_alpha}$
    \State $(\proc{D}_1,\proc{D}_2) \gets \mathrm{decisions}(\alpha)$
    \State $\proc{Y}_2\gets \mathrm{apply}(\proc{D}_2,\mathrm{patient\;A})$
    \State $\proc{Y}_1\gets \mathrm{apply}(\proc{D}_1,\mathrm{patient\;B})$
    \State \Return $(\alpha,\proc{D}_1,\proc{D}_2,\proc{Y}_1,\proc{Y}_2)$
    \EndProcedure
\end{algorithmic}

Consider another measurement procedure $\proc{S}''$, which is a modified version of $\proc{S}$ where steps are added to swap decisions after they are chosen, then outcomes are swapped back once they have been observed:

\begin{algorithmic}
    \Procedure{$\proc{S}''$}{}
    \Assert{patient A knowledge=patient B knowledge}
    \State $\alpha \gets \mathrm{choose\_alpha}$
    \State $(\proc{D}_1,\proc{D}_2) \gets \mathrm{decisions}(\alpha)$
    \State $(\proc{D}_1^{\mathrm{swap}},\proc{D}_2^{\mathrm{swap}}) \gets (\proc{D}_2,\proc{D}_1)$
    \State $\proc{Y}^{\mathrm{swap}}_1\gets \mathrm{apply}(\proc{D}_1^{\mathrm{swap}},\mathrm{patient A})$
    \State $\proc{Y}_2^{\mathrm{swap}}\gets \mathrm{apply}(\proc{D}_2^{\text{swap}},\text{patient B})$
    \State $(\proc{Y}_1,\proc{Y}_2)\gets (\proc{Y}^{\mathrm{swap}}_2,\proc{Y}^{\mathrm{swap}}_1)$
    \State \Return $(\alpha,\proc{D}_1,\proc{D}_2,\proc{Y}_1,\proc{Y}_2)$
    \EndProcedure
\end{algorithmic}

Instead of explicitly performing the swaps, we can substitute $\proc{D}_2$ for $\proc{D}_1^{\text{swap}}$, $\proc{Y}_2$ for $\proc{Y}_1^{\text{swap}}$ and so on. The result is a procedure identical to $\proc{S}'$

\begin{algorithmic}
    \Procedure{$\proc{S}''$}{}
    \Assert{patient A knowledge=patient B knowledge}
    \State $\alpha \gets \mathrm{choose\_alpha}$
    \State $(\proc{D}_1,\proc{D}_2) \gets \mathrm{decisions}(\alpha)$
    \State $\proc{Y}_2\gets \mathrm{apply}(\proc{D}_2,\mathrm{patient\;A})$
    \State $\proc{Y}_1\gets \mathrm{apply}(\proc{D}_1,\mathrm{patient\;B})$
    \State \Return $(\alpha,\proc{D}_1,\proc{D}_2,\proc{Y}_1,\proc{Y}_2)$
    \EndProcedure
\end{algorithmic}

Thus $\proc{S}''$ is exactly the same as $\proc{S}'$, which by assumption is equivalent to the original $\proc{S}$, and so the assumptions of interchangeable patients and reversible order of treatment application imply the model should commute with exchange. Thus, if we could extend this example to an infinite sequence of patients, there would exist a Markov kernel $\prob{P}_\square^{\RV{Y}|\RV{DH}}:D\times H\kto Y$ representing a ``definite but unknown causal consequence'' shared by all experimental units.

This argument does \emph{not} hold for scenario 2. In the absence of a deterministic function $\text{decisions}(\alpha)$ which defines the procedure for obtaining $\proc{D}_1$ and $\proc{D}_2$, there is some flexibility for how exactly these variables are measured (or chosen). In particular, we can posit measurement procedures such that permuting patients is not equivalent to permuting decisions and then appying the reverse permutation to outcomes.

For example, procedure $\proc{T}$ is compatible with scenario 2 (note that there are many procedures compatible with the given description)

\begin{algorithmic}
    \Procedure{$\proc{T}$}{}
    \Assert{patient A knowledge=patient B knowledge}
    \State $\alpha \gets \mathrm{choose\_alpha}$
    \State patient A knowledge$\gets \mathrm{inspect}$(patient A)
    \State patient B knowledge$\gets \mathrm{inspect}$(patient B)
    \State $(\proc{D}_1,\proc{D}_2) \gets \mathrm{vagueDecisions}(\alpha$, patient A knowledge, patient B knowledge)
    \State $\proc{Y}_1\gets \mathrm{apply}(\proc{D}_1,\mathrm{patient\;A})$
    \State $\proc{Y}_2\gets \mathrm{apply}(\proc{D}_2,\mathrm{patient\;B})$
    \State \Return $(\alpha,\proc{D}_1,\proc{D}_2,\proc{Y}_1,\proc{Y}_2)$
    \EndProcedure
\end{algorithmic}

Permutation of patients and treatment order now yields

\begin{algorithmic}
    \Procedure{$\proc{T}'$}{}
    \Assert{patient A knowledge=patient B knowledge}
    \State $\alpha \gets \mathrm{choose\_alpha}$
    \State patient B knowledge$\gets \mathrm{inspect}$(patient B)
    \State patient A knowledge$\gets \mathrm{inspect}$(patient A)
    \State $(\proc{D}_1,\proc{D}_2) \gets \mathrm{vagueDecisions}(\alpha$, patient B knowledge, patient A knowledge)
    \State $\proc{Y}_2\gets \mathrm{apply}(\proc{D}_2,\mathrm{patient\;A})$
    \State $\proc{Y}_1\gets \mathrm{apply}(\proc{D}_1,\mathrm{patient\;B})$
    \State \Return $(\alpha,\proc{D}_1,\proc{D}_2,\proc{Y}_1,\proc{Y}_2)$
    \EndProcedure
\end{algorithmic}

While paired permuation of decisions and outcomes yields

\begin{algorithmic}
    \Procedure{$\proc{T}''$}{}
    \Assert{patient A knowledge=patient B knowledge}
    \State $\alpha \gets \mathrm{choose\_alpha}$
    \State patient A knowledge$\gets \mathrm{inspect}$(patient A)
    \State patient B knowledge$\gets \mathrm{inspect}$(patient B)
    \State $(\proc{D}_1,\proc{D}_2) \gets \mathrm{vagueDecisions}(\alpha$, patient A knowledge, patient B knowledge)
    \State $\proc{Y}_2\gets \mathrm{apply}(\proc{D}_2,\mathrm{patient\;A})$
    \State $\proc{Y}_1\gets \mathrm{apply}(\proc{D}_1,\mathrm{patient\;B})$
    \State \Return $(\alpha,\proc{D}_1,\proc{D}_2,\proc{Y}_1,\proc{Y}_2)$
    \EndProcedure
\end{algorithmic}

$\proc{T}'$ is not the same as $\proc{T}''$. In scenario 1, because decisions were deterministic on $\alpha$, there was no room to pick anything different once $\alpha$ was chosen, so it doesn't matter if we add patient inspection steps or not. In scenario 2, decisions are not deterministic and there is vagueness in the procedure, so it is possible to describe compatible procedures where decisions depend on patient characteristics, and this dependence is not ``undone'' by swapping decisions.

\todo[inline]{I've started but not finished revising the previous}

\subsection{Causal consequences of non-deterministic variables}

In the previous section we gave an example of how commutativity of exchange can hold when we have a sequence of decisions such that we accept the follwing:

\begin{itemize}
    \item Reordering the time at which decisions are made is held to be of no consequence
    \item The available information relevant to each decision is symmetric at the time the decision function is adopted
    \item The decision function deterministically prescribes which decisions are taken
\end{itemize}

We also discussed how the absence of determinism undermines the argument for exchange commutativity.

The determinism assumption rules out choosing decisions randomly. However, if we have response conditionals with a particular conditioning variable, response conditionals for other conditioning variables may exist if a certain conditional independence that we refer to as \emph{proxy control} holds. That is, if we have a response conditional for $(\RV{X},\RV{Y})$ given $\RV{D}$, $\RV{D}$ is deterministic for all choices and $\RV{Y}$ is independent of $\RV{D}$ given $\RV{X}$, then we also have a response conditional for $\RV{Y}$ given $\RV{X}$ and $\RV{X}$ may not be deterministic.

We also show that proxy control is necessary for the existence of additional response conditionals if $\RV{D}$ is deterministically controllable; that is, if it can be forced to take on any deterministic probability distribution. If the judgements underpinning the existence of response conditionals ultimately rest on decision variables that are deterministic for each choice that can be made, and we claim that a response conditional for $\RV{Y}$ given $\RV{X}$ exists where $\RV{X}$ is just some not-necessarily-deterministic variable, then $\RV{X}$ must be a proxy for controlling $\RV{Y}$ given $\RV{D}$.

\begin{definition}[Deterministically controllable]
Given a probabilty gap model $(\prob{P}_\square,\{\prob{P}_{\tilde{\alpha}}^{\RV{D}}\}_A,f)$ on $(\Omega,\sigalg{F})$ and a variable $\RV{X}:\Omega\to X$, if for any $x\in X$ there exists $\alpha\in A$ such that $\prob{P}_\alpha^{\RV{X}}=\delta_x$ then $\RV{X}$ is determinstically controllable.
\end{definition}

\begin{theorem}\label{lem:proxy_control}
Given $(\prob{P}_\square^{\RV{XY}|\RV{D}[M]},\{\prob{P}_{\tilde{\alpha}}^{\RV{D}}\}_A,f)$ with decisions $\RV{D}_M$ and consequences $\RV{Y}_M$, $\RV{X}_M$, if $\prob{P}_\square^{\RV{Y}_M\RV{X}_M|\RV{D}_M}$ is causally contractible with response conditional $\prob{P}_\square^{\RV{Y}_0\RV{X}_0|\RV{D}_0\RV{H}}$ such that $\RV{Y}_i\CI_{\prob{P}_\square} \RV{D}_i|\RV{HX}_i$ for all $i\in M$, then a causally contractible conditional probability $\prob{P}_\square^{\RV{Y}_M|\RV{X}_M}$ exists. If $\RV{D}_M$ is deterministically controllable and $D$ countable, then $\RV{Y}_i\CI_{\prob{P}_\square} \RV{D}_i|\RV{HX}_i$ is also necessary for the existence of $\prob{P}_\square^{\RV{Y}_M|\RV{X}_M}$.
\end{theorem}

\begin{proof}
\textbf{Sufficiency:}
We want to show that $\RV{Y}_i\CI_{\prob{P}_\square} \RV{Y}_{\{i\}^C}\RV{X}_{\{i\}^C} |\RV{H}\RV{X}_i$ for all $i\in M$, that $\prob{P}_\square^{\RV{Y}_i|\RV{H}\RV{X}_i}$ exists for all $i\in M$ and that $\prob{P}_\square^{\RV{Y}_i|\RV{H}\RV{X}_i}=\prob{P}_\square^{\RV{Y}_j|\RV{H}\RV{X}_j}$.

From causal contractibility we have
\begin{align}
(\RV{X}_i,\RV{Y}_i)\CI_{\prob{P}_\square} (\RV{X}_{\{i\}^C},\RV{Y}_{\{i\}^C},\RV{D}_{\{i\}^C}) |\RV{H}\RV{D}_i\label{eq:cc}\\
\RV{Y}_i\CI_{\prob{P}_\square} (\RV{Y}_{\{i\}^C},\RV{X}_{\{i\}^C}) |\RV{H}\RV{D}_i\RV{X}_i\label{eq:wu}
\end{align}

Where Eq. \ref{eq:wu} follows from \ref{eq:cc} by weak union.

Thus by contraction, $\RV{Y}_i\CI_{\prob{P}_\square} \RV{Y}_{\{i\}^C}\RV{D}_{M} |\RV{H}\RV{X}_i$.

By Corollary \ref{cor:ci_cp_exist} and the existence of $\prob{P}^{\RV{Y}_i\RV{X}_i|\RV{H}\RV{D}_i}$ for all $i\in M$, $\prob{P}_\square^{\RV{Y}_i|\RV{H}\RV{X}_i}$ exists for all $i$. Furthermore, because $\prob{P}^{\RV{Y}_i\RV{X}_i|\RV{H}\RV{D}_i}=\prob{P}^{\RV{Y}_j\RV{X}_j|\RV{H}\RV{D}_j}$ for all $i,j\in M$, $\prob{P}_\square^{\RV{Y}_i|\RV{H}\RV{X}_i}=\prob{P}_\square^{\RV{Y}_j|\RV{H}\RV{X}_j}$ for all $i,j\in M$.
\textbf{Necessity:}
We will show for all $\alpha\in A$, $B\in \sigalg{Y}$, $(x,d,h)\in X\times D\times H$ that 
\begin{align}
    \prob{P}_\alpha^{\RV{Y}_0|\RV{X}_0\RV{D}_0\RV{H}}(B|x,d,h)=\prob{P}_\alpha^{\RV{Y}_0|\RV{X}_0\RV{H}}(B|x,h)
\end{align}

By assumption, we have the conditionals $\prob{P}_\square^{\RV{Y}_i\RV{X}_i|\RV{D}_i\RV{H}}$ and $\prob{P}_\square^{\RV{X}_i|\RV{H}\RV{D}_i}$ for all $i\in M$. We can conclude that $\prob{P}_\square^{\RV{Y}_i|\RV{X}_i\RV{D}_0i\RV{H}}$ also exists, as it is a higher order conditional with respect to $\prob{P}_\square^{\RV{Y}_i\RV{X}_i|\RV{D}_i\RV{H}}$.

For arbitrary $d\in D$, let $\alpha_d\in A$ be such that $\prob{P}_{\alpha_d}^{\RV{D}_i}=\delta_{d}$. For every version of $\prob{P}_{\alpha_d}^{\RV{Y}_i|\RV{X}_i\RV{D}_i\RV{H}}$ and $\prob{P}_{\alpha_d}^{\RV{Y}_i|\RV{X}_i\RV{D}_i\RV{H}}$
\begin{align}
    \prob{P}_{\alpha_d}^{\RV{Y}_i|\RV{X}_i\RV{H}}(B|x,h) &= \int_{D}\prob{P}_{\alpha_d}^{\RV{Y}_i|\RV{X}_i\RV{D}_i\RV{H}}(B|x,d',h)\delta_d(\mathrm{d}d')\\
                                                         &= \prob{P}_{\alpha_d}^{\RV{Y}_i|\RV{X}_i\RV{D}_i\RV{H}}(B|x,d,h)
\end{align}
For all $x\in X$, $h\in H$ $B\subset\sigalg{Y}$ except on a set of points $C\subset X\times H$ of uniform $\prob{P}_{\alpha_d}$ measure 0.
\todo[inline]{Need to add independence of hypothesis to representation theorem}
However, note that for any $\alpha$
\begin{align}
    \prob{P}_\alpha^{\RV{X}_i\RV{H}\RV{D}_i}(E\times F\times G) &= \sum_{d\in G} \prob{P}_\alpha^{\RV{D}_i}(d)\prob{P}_\square^{\RV{X}_i\RV{H}|\RV{D}_i}(E\times F|d)\\
                                                                  &= \sum_{d\in G} \prob{P}_\alpha^{\RV{D}_i}(d)\sum_{d'\in D}\prob{P}_\square^{\RV{X}_i\RV{H}|\RV{D}_i}(E\times F|d')\prob{P}_{\alpha_d}^{\RV{D}_i}(\{d'\}\\
                                                                  &= \sum_{d\in G} \prob{P}_\alpha^{\RV{D}_i}(d)\prob{P}_{\alpha_d}^{\RV{X}_i\RV{H}\RV{D}_i}(E\times F\times\{d\})
\end{align}

Thus for each $d\in D$ the set $\{d\}\times C\subset D\times X\times H$ is of uniform $\prob{P}_\alpha$ measure 0 for any $\alpha\in A$. Because $\prob{P}_\square=\cup_{\alpha\in A}\prob{P}_\alpha$, it is also of uniform $\prob{P}_\square$ measure 0. Thus 

\begin{align}
    \prob{P}_{\square}^{\RV{Y}_0|\RV{X}_0\RV{H}}(B|x,h) &= \prob{P}_{\square}^{\RV{Y}_0|\RV{X}_0\RV{D}_0\RV{H}}(B|x,d,h)
\end{align}
as desired.
\end{proof}

As an example of this, suppose $\RV{X}:\Omega\to X$ is a source of random numbers, the set of decisions $D$ is a set of functions $X\to T$ for treatments $\RV{T}:\Omega\to T$ and $\RV{W}:\Omega\to W$ are the ultimate patient outcomes, with $\RV{Y}_i=(\RV{W}_i,\RV{T}_i)$. Then it may be reasonable to assume that $\RV{W}_i\CI(\RV{D}_i,\RV{X}_i)|\RV{T}_i\RV{H}$ (where conditioning on $\RV{H}$ can be thought of as saying that this independence holds under infinite sample size). In this case, $\RV{T}_i$ is a proxy for controlling $\RV{Y}_i$, and there exists a causal consequence $\prob{P}_\square^{\RV{Y}_0|\RV{T}_0\RV{H}}$.

A ``causal consequence of body mass index'' is unlikely to exist on the basis of symmetric information and deterministic decisions because there are no actions available to set body mass index deterministically. However, given an underlying problem where we have symmetric information over a collection of patients and some kind of decision that can be made deterministically, causal consequences of body mass index may exist if body mass index is a proxy for controlling the outcomes of interest.

\subsection{Body mass index revisited}

We return briefly to consider the question: given some collection of people indexed by $M$, with body mass index $\RV{B}_i$ and health outcomes of interest $\RV{Y}_i$ and some choices $\RV{D}_i$ a decision maker is contemplating relevant to these characteristics, suppose we have a conditional probability model $(\prob{P}_\square^{\RV{BY}|\RV{D}[M]},\{\prob{P}_{\tilde{\alpha}}^{\RV{D}}\}_A,f)$ causally contractible with respect to $(\RV{D},\RV{Y})$ (for example, perhaps decision maker is contemplating a treatment plan to apply to every individual).

Do response conditionals $\prob{P}_\square^{\RV{Y}_i|\RV{B}_i}$ exist? We have by Lemma \ref{lem:proxy_control} that this exists if and only if $\RV{Y}_i\CI_{\prob{P}_\square} \RV{D}_i|\RV{B}_i$. Thus we have reduced the question of the existence of response conditionals for BMI (or ``causal effects'' of BMI) to an empirical question. We might guess this is unlikely to hold; not only are there multiple ways we could imagine affecting a person's BMI with possibly different health implications, but it seems unlikely that the ultimate health outcome someone experiences can be predicted from BMI alone.

However, there might be something to be said for a ``causal effect of BMI''. In particular, while it seems unlikely that BMI is a precise proxy for controlling health outcomes, it seems to at least be a reasonable empirical question to ask if BMI is an \emph{approximate} proxy for health outcomes.

\todo[inline]{Do I prove a theorem about approximate proxy control?}