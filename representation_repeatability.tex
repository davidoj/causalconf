%!TEX root = main.tex



\section{Syntax and semantics of causal consequences}

Causal Bayesian networks and potential outcomes employ different naming conventions to distinguish ``causal effects'' from ``simple correlations''. Causal Bayesian networks write $P(\RV{Y}|do(\RV{X}))$ and $P(\RV{Y}|\RV{X})$, while potential outcomes distinguishes $P(\RV{Y}|\RV{X})$ from $x\mapsto P(\RV{Y}^x)$. If we are not going to worry too much about details of interpretation, we can interpret the expression $P(\RV{Y}|\RV{X})$ as expressing something like this: there is an objective probability $P(\RV{Y},\RV{X})$ that describes a sequence of independent and identically distributed observations, and $P(\RV{Y}|\RV{X})$ is a disintegration of this probability. The existence of an objective probability $P(\RV{Y},\RV{X})$ can be justified by an assumption that the sequence of observations should be modeled exchangeably.

We pursue a similar line of thinking with respect to understanding causal consequences like $P(\RV{Y}|do(\RV{X}))$ or $x\mapsto P(\RV{Y}^x)$. We assume that ``causal consequences'' are conditional probabilities of the form $\prob{P}_\square^{\RV{Y}|\RV{D}\RV{H}}$ where $\RV{Y}$ is an outcome, $\RV{D}$ is some decision, $\RV{H}$ is a hypothesis and $\prob{P}_\square$ is a probability gap model. Our interest is in understanding what causal consequences are \emph{from the point of view of someone choosing a decision function}. We do not address the question of how they may be inferred from observed data.

 We show that conditional probability models that are \emph{causally contractible} with respect to a sequence of decisions and a corresponding sequence of outcomes are representible by mixtures of ``objective but unknown'' conditional probabilities. This is analogous to De Finetti's theorem that shows exchangeable probability distributions are representable by mixtures of ``objective but unknown'' independent and identically distributed probability distributions. A similar argument to ours is found in \citet{dawid_decision-theoretic_2020}.

We also consider the question of when causal contractibility could be supposed to hold. This is a subtle question, as the answer appears to differ for situations that are quite similar. For example, consider:
\begin{enumerate}
    \item Dr Alice is going to see two patients who are both complaining of lower back pain and are otherwise unknown to Alice. Prior to seeing them, she considers the available research and formulates a general sense of whether or not she'll treat each one, which she quantifies with $\prob{P}_\alpha^{\RV{D}_1\RV{D}_2}$
    \item As before, but prior to seeing the patients she considers the available research and decides to treat on the basis of applying a function to a random number generator with known characteristics. The choice of function and random number generator allow her to quantity probability of treatment with $\prob{P}_\alpha^{\RV{D}_1\RV{D}_2}$
\end{enumerate}

\todo[inline]{I removed the discussion of probability combs for simplicity, so I have not considered policies for treatment that depend on earlier experiments in the examples above}

We will argue that Alice could reasonably assume causal contractibility in the second case but not the first. While we are unable to offer a general theory of when causal contractibility holds, we note that an apparently key difference between the two situations is that in the first case the ``decision'' $\RV{D}_1$ is indeterministic for some $\alpha$, though $\RV{D}_2$ is deterministic, while in the second case both $\RV{D}_1$ and $\RV{D}_2$ are determinstic functions.

\subsection{Repeatable experiments}

A conditional probability model $(\prob{P}_\square^{\overline{\RV{Y}|\RV{D}}},A)$ is a model of a sequential experiment if $\RV{Y}:=\RV{Y}_M=(\RV{Y}_i)_{i\in M}$ and $\RV{D}:=\RV{D}_M=(\RV{D}_i)_{i\in M}$ for some index set $M$. We say that $\RV{Y}_i$ is the consequence corresponding to the decision $\RV{D}_i$ for all $i\in M$. We identify a ``causal consequence'' with a conditional probability of the form $\prob{P}_\square^{\RV{Y}_i|\RV{H}\RV{D}_i}$, where $\RV{H}$ is a hypothesis that is deterministically identical for every $i$. Causal consequences do not generally exist, see Definition \ref{def:cprob_pset}.

If $(\prob{P}_\square^{\overline{\RV{Y}|\RV{D}}},A)$ represents a sequential experiment, we might guess that causal consequences exist if the experiment is in some sense ``repeatable''. We consider two precise notions of repeatability. The first condition is \emph{commutativity of exchange}, which is the assumption that swapping the choices that we apply at each step and then applying the corresponding inverse swap to consequences leaves the model unchanged. The second condition is \emph{commutativity of marginalisation} -- if we perform the whole experiment multiple times, making the same choice $\RV{D}_i$ at any point $i$ gets the same results, regardless of what other choices are made.

Commutativity of exchange is similar to the condition of \emph{post-treatment exchangeability} found in \citet{dawid_decision-theoretic_2020}, and commutativity of marginalisation is similar to the stable unit treatment distribution assumption (SUTDA) in the same, as well as the ``no interference'' part of the stable unit treatment value assumption (SUTVA) with which it shares a name. Commutativity of exchange is also very similar to the exchangeability assumption of \citet{greenland_identifiability_1986} for further discussions of exchangeability in the context of causal modelling, and note that both authors consider exchanging to be an operation that alters which person receives which treatment. The assumption of exchangeability found in \citet{banerjee_chapter_2017} can also be regarded as similar to commutativity of exchange.

\todo[inline]{Not sure if or where I want to put this, I just think it helps to illustrate the difference}

Commutativity of exchange is not equivalent to exchangeability in the sense of De Finetti's well-known theorem \citet{de_finetti_foresight_1992}. The latter can be understood as expressing an indifference between conducting the experiment as normal, or conducting the experiment and then swapping some labels. However, swapping \emph{choices} will (usually) lead to different ``pieces of the experiment'' receiving different treatment, which is something that can't be achieved by swapping labels after the experiment has concluded.

The difference is illustrated by the following pair of diagrams.

Exchangeability (swapping labels):

\begin{align}
    \tikzfig{exchangeability}
\end{align}

Commutativity of exchange (swapping choices $\sim$ swapping labels):

\begin{align}
    \tikzfig{commutativity of exchange}
\end{align}

Commutativity of exchange is a property of probability gap models, not a property of fixed probability model for which there is no analogue of ``attaching a different choice'' in that case.

\todo[inline]{----end not sure where to put------}


% Another way to see where we are going is to consider graphical statements of our and De Finetti's result.

% Take $S=\{0,1\}$ and identify the space $\Delta(S)$ of probability measures on $S$ with the interval $[0,1]$. De Finetti showed that any infinite exchangeable probability measure $\prob{P}_\alpha$ on $\{0,1\}^\mathbb{N}$ can be represented by a prior $\prob{P}_\alpha^{\RV{H}}\in [0,1]$ for some $\RV{H}:\Omega\to H$ and a conditional probability $\prob{P}^{\RV{S}_0|\RV{H}}:[0,1]\kto \{0,1\}$ such that

% \begin{align}
%     \prob{P}_\alpha &= \tikzfig{de_finetti_rep0}\label{eq:definettirep}
% \end{align}

% Here $\prob{P}^{\RV{S}_0|\RV{H}}$ can be defined concretely by $\prob{P}^{\RV{S}_0|\RV{H}}(1|h)=h$. Equivalently, the probability gap model on $S^\mathbb{N}$ defined by the assumption of exchangeability is equivalent to the probability gap model defined by the conditional probability

% \begin{align}
%     \prob{P}^{\RV{S}|\RV{H}} = \tikzfig{de_finetti_conditional}
% \end{align}

% That is, there is some hypothesis $\RV{H}$ and conditional on $\RV{H}$ the measurements are independent and identically distributed. The proof of this is constructive -- $\RV{H}$ is a function of $\RV{S}$.



% \begin{align}
%     \prob{P}^{\RV{Y}|\RV{HD}} = \tikzfig{do_model_representation}
% \end{align}

% We will further argue that the class of see-do models considered in CBN and potential outcomes literature is equivalent to the family of causally contractible and exchangeable do-models where the decision rule for the first $n$ places is fixed to an unknown value, and may be freely chosen thereafter.

% \begin{theorem}[Existence of conditional in do models]
% Given a do model $(\prob{P}_{\square}^{\RV{Y}\|\RV{D}},R)$, for all $\alpha\in R$, $n\in\mathbb{N}$
% \begin{align}
%     \prob{P}_\alpha^{\RV{Y}_{[n]}\RV{D}_i} = \prob{P}_\alpha^{\RV{D}_{[n]}}\odot \prob{P}_\square^{\RV{Y}_{[n]}\|\RV{D}_{[n]}}
% \end{align}
% That is, $\prob{P}_\square^{\RV{Y}_{[n]}\|\RV{D}_{[n]}}\cong \prob{P}_\square^{\RV{Y}_{[n]}|\RV{D}_{[n]}}$
% \end{theorem}

% \begin{proof}
% For any $n>1\in \mathbb{N}$, $\alpha\in R$

% \begin{align}
%     \prob{P}_\alpha^{\RV{Y}_{[n]}\RV{D}_{[n]}} &= \tikzfig{do_model_1}\\
%     &= \tikzfig{do_model_2}\\
%     &= \tikzfig{do_model_3}\\
%     &= \tikzfig{do_model_4}\\
%     \implies \prob{P}_\alpha^{\RV{Y}_{[n]}|\RV{D}_{[n]}} &= \tikzfig{do_model_5}\\
%     &= \prob{P}_\alpha^{\RV{Y}_{[n-1]}|\RV{D}_{[n-1]}}\combprod \prob{P}_\square^{\RV{Y}_n|\RV{Y}_{[n-1]}\RV{D}_n}
% \end{align}

% Applying this recursively with $\prob{P}_\alpha^{\RV{Y}_{[1]}|\RV{D}_{[1]}}=\prob{P}_\square^{\RV{Y}_{[1]}|\RV{D}_{[1]}}$ yields

% \begin{align}
%     \prob{P}_\alpha^{\RV{Y}_{[n]}|\RV{D}_{[n]}} = \prob{P}_\square^{\RV{Y}_{[n]}\|\RV{D}_{[n]}}
% \end{align}
% as desired.
% \end{proof}
More precisely, a conditional probability model ``commutes with exchange'' if applying any finite permutation to blind choices or separately applying the corresponding permuation to consequences each yields the same result. We can apply the exchange ``before'' multiplying by the conditional $\prob{P}_{\square}^{\RV{Y}|\RV{D}}$ or after it and we get the same result.

\begin{definition}[Swap map]
Given $M\subset \mathbb{N}$ a finite permutation $\rho:M\to M$ and a variable $\RV{X}:\Omega\to X^M$ such that $\RV{X}=(\RV{X}_i)_{i\in M}$, define the Markov kernel $\text{swap}_{\rho(\RV{X})}:X^M\kto X^M$ by $(d_i)_{i\in\mathbb{N}}\mapsto \delta_{(d_{\rho(i)})_{i\in\mathbb{N}}}$.
\end{definition}

\begin{definition}[Commutativity of exchange]\label{def:caus_exch}
Suppose we have a sample space $(\Omega,\sigalg{F})$ and a conditional probability model $(\prob{P}_{\square}^{\overline{\RV{Y}|\RV{D}}},A)$ with $\RV{Y}=\RV{Y}_M$, $\RV{D}=\RV{D}_M$, $M\subseteq \mathbb{N}$. If, for any two decision rules $\alpha^{\overline{\RV{D}}},\beta^{\overline{\RV{D}}} \in A$,
\begin{align}
    \alpha^{\RV{D}}\odot \text{swap}_{\rho(\RV{D})} \prob{P}_{\square}^{\RV{Y}|\RV{D}} &= \alpha^{\RV{D}}\odot \prob{P}_{\square}^{\RV{Y}|\RV{D}}\text{swap}_{\rho(\RV{Y})}
\end{align}
Then $\prob{P}_\square$ \emph{commutes with exchanges}.
\end{definition}

A do model is non interfering if it gives identical results for identical subsequences of different choices when we limit our attention to the corresponding subsequences of consequences. For example, if we have $\RV{D}=(\RV{D}_1,\RV{D}_2,\RV{D}_3)$ and $\RV{Y}=(\RV{Y}_1,\RV{Y}_2,\RV{Y}_3)$ and $\alpha^{\RV{D}_1\RV{D}_3}=\prob{P}_\beta^{\RV{D}_1\RV{D}_3}$ then $\prob{P}_{\alpha}^{\RV{Y}_1\RV{D}_1\RV{Y}_3\RV{D}_3}=\prob{P}_\beta^{\RV{Y}_1\RV{D}_1\RV{Y}_3\RV{D}_3}$.

\begin{definition}[Commutativity of marginalisation]\label{def:caus_cont}
Suppose we have a sample space $(\Omega,\sigalg{F})$ and a conditional probability model $(\prob{P}_{\square}^{\overline{\RV{Y}|\RV{D}}},A)$ with $\RV{Y}=\RV{Y}_M$, $\RV{D}=\RV{D}_M$, $M\subseteq \mathbb{N}$. For any $S=(s_i)_{i\in Q}$, $Q\subset M$, and $i<j\implies p_i<p_j \And q_i<q_j$, let $\RV{D}_S:=(\RV{D}_i)_{i\in S}$ and $\RV{D}_T:=(\RV{D}_i)_{i\in T}$. If for any $\alpha,\beta\in R$
\begin{align}
    \prob{P}_\alpha^{\RV{D}_{S}}&=\prob{P}_\beta^{\RV{D}_{S}}\\
    \implies \prob{P}_\alpha^{(\RV{D_i,Y_i})_{i\in S}}&=\prob{P}_\beta^{(\RV{D_i,Y_i})_{i\in S}}
\end{align}
then $\prob{P}_\square$ \emph{commutes with marginalisation}.
\end{definition}

Neither condition implies the other. 
\begin{lemma}
Commutativity of exchange does not imply commutativity or vise versa.
\end{lemma}

\begin{proof}
Suppose $D=Y=\{0,1\}$ and we have a conditional probability model $(\prob{P}_\square^{\overline{\RV{Y}|\RV{D}}},A)$ where $\RV{D}=(\RV{D}_1,\RV{D}_2)$, $\RV{Y}=(\RV{Y}_1,\RV{Y}_2)$ and A contains all deterministic probability measures in $\Delta(D^2)$. If

\begin{align}
    \prob{P}_\square^{\RV{Y}_1\RV{Y}_2|\RV{D}_1\RV{D}_2}(y_1,y_2|d_1,d_2) &= \llbracket (y_1,y_2)= (d_1+d_2,d_1+d_2) \rrbracket
\end{align}

Then $\prob{P}_{\delta_{00}}^{\RV{Y}_1\RV{D}_1}(y_1) = \llbracket y_1=0\rrbracket$ while $\prob{P}_{\delta_{01}}^{\RV{Y}_1} = \llbracket y_1=1 \rrbracket$. However, $\delta_00^{\RV{D}_1}=\delta_{01}^{\RV{D}_1}=\delta_0^{\RV{D}_1}$ so $\prob{P}_\square$ does not commute with marginalisation. However, taking $(d_i,d_j):=\delta_{d_i d_j}\in A$,

\begin{align}
    \prob{P}_{d_2,d_1}^{\RV{Y}_1\RV{D}_1\RV{Y}_2\RV{D}_2}(y_1,d_1,y_2,d_2) &= \llbracket (y_1,y_2)= (d_2+d_1,d_2+d_1) \rrbracket\\
    &= \llbracket (y_2,y_1)= (d_1+d_2,d_1+d_2) \rrbracket\\
    &= \prob{P}_{d_1,d_2}^{\RV{Y}_1\RV{D}_1\RV{Y}_2\RV{D}_2}(y_2,d_2,y_1,d_1)
\end{align}

so $\prob{P}_\square$ commutes with exchange.

Alternatively, suppose the same setup, but define $\prob{P}_\square$ instead by, for all $\alpha\in A$

\begin{align}
    \prob{P}_\square{\RV{Y}_1\RV{Y}_2|\RV{D}_1\RV{D}_2}(y_1,y_2|d_1,d_2) &= \llbracket (y_1,y_2)= (0,1) \rrbracket
\end{align}

Then $\prob{P}_\square$ commutes with marginalisation. If $\prob{P}_\alpha^{\RV{D}_S}=\prob{P}_\beta^{\RV{D}_S}$ for $S\subset\{0,1\}$ then

\begin{align}
    \prob{P}_{\alpha}^{\RV{Y}_S\RV{D}_S}(y_s,d_s) &= \sum_{y'_2\in \{0,1\}^{S^C}} \llbracket (y_1,y_2)= (0,1) \rrbracket\prob{P}_\alpha^{\RV{D}_S}(d_s) \\
                                                  &= \prob{P}_{\beta}^{\RV{Y}_S\RV{D}_S}(y_s,d_s)
\end{align}
but not exchange. For all $\alpha,\beta \in A$:

\begin{align}
    \prob{P}_\alpha{\RV{Y}_1\RV{Y}_2}(y_1,y_2) &= \llbracket (y_1,y_2)= (0,1) \rrbracket\\
    &\neq \prob{P}_\beta{\RV{Y}_1\RV{Y}_2}(y_2,y_1)
\end{align}
\end{proof}

Although commutativity of marginalisation seems to be a bit like non-interference -- the marginal distribution I get for $\RV{Y}_i$ depends only on the decision $\RV{D}_i$ -- it still allows for some models in which we seem to have interference of a kind. For example: in the first experiment I flip a coin and decide either to pass the results to the second experiment ($\RV{D}_1=0$) or flip another coin and pass those results second experiment ($\RV{D}_1=1$). In the second I either copy the results I have been given ($\RV{D}_2=0$) or invert them ($\RV{D}_2=1$). Then
\begin{itemize}
    \item The marginal distribution of both experiments is $\text{Bernoulli}(0.5)$ no matter what choices I make, so it satisfies Definition \ref{def:caus_cont}
    \item Nevertheless, the choice for the first experiment seems to ``affect'' the result of the second experiment (affect in quotes because it is an intuitive judgement, not a formal property)
\end{itemize}

Here we are most interested in the conjunction of these assumptions, a condition we call \emph{causal contractibility}

\begin{definition}[Causal contractibility]
A conditional probability model $(\prob{P}_{\square}^{\overline{\RV{Y}|\RV{D}}},A)$ is causally contractible if it is both commutative with exchange and commutative with marginalisation.
\end{definition}

% \begin{proposition}[Representation of do-models that commute with exchange]
% Suppose we have a fundamental probability set $\Omega$ and a do model $(\prob{P},\RV{D},\RV{Y},R)$ such that $\RV{D}:=(\RV{D}_i)_{i\in \mathbb{N}}$ and $\RV{Y}:=(\RV{Y}_i)_{i\in\mathbb{N}}$ where $\prob{P}$ commutes with exchange and there is some $\alpha^*\in R$ such that $\prob{P}^{\alpha^*}\gg\prob{P}_\beta$ for all $\beta in R$. Then there exists a symmetric function $\RV{H}:(Y\times D)^\mathbb{N}\to H$ such that  $\prob{P}^{\RV{Y}|\RV{DH}}$ exists and $\RV{Y}_i\CI_{\prob{P}}(\RV{D}_j,\RV{Y}_j)_{j\in \mathbb{N}}\setminus \{i\}|\RV{H}\RV{D}_i$, or equivalently 
% \begin{align}
%     \prob{P}^{\RV{Y}} &= \tikzfig{do_model_representation}
% \end{align}
% \end{proposition}

% % \begin{lemma}[Contraction and independence]
% % Let $\RV{J}$, $\RV{K}$ and $\RV{L}$ be variables on $\Omega$ and $\prob{Q}\in \Delta(\Omega)$ a base measure such that $\prob{Q}^{\RV{JK}}=\prob{Q}^{\RV{JL}}$ and $\sigma{K}\subset \sigma{L}$. Then $\RV{J}\CI\RV{L}|\RV{K}$. 
% % \end{lemma}

% % \begin{proof}
% % From Lemma 1.3 in \citet{kallenberg_basic_2005}.
% % \end{proof}

% \begin{proof}
% If $\prob{P}$ commutes with exchange, then for any $\alpha\in R$ such that $\prob{P}_\alpha^{\RV{D}}$ is exchangeable then $\prob{P}_\alpha$ is also exchangeable. Then there exists $\RV{H}$ a symmetric function of $(\RV{Y}_i,\RV{D}_i)_{i\in\mathbb{N}}$ such that $\RV{Y}_i\CI_{\prob{P}}(\RV{D}_j,\RV{Y}_j)_{j\in \mathbb{N}}\setminus \{i\}|\RV{H}\RV{D}_i$. This is De Finetti's representation theorem, and many proofs exists, see for example \citep{kallenberg_basic_2005}.

% In particular, let 

% \begin{align}
%     \RV{H}:=A\times B\mapsto \lim_{n\to\infty} \frac{1}{n}\sum_{i\in n} \mathds{1}_{A\times B}((\RV{Y}_i, \RV{D}_i))
% \end{align}

% Then for all $\alpha\in R$,
% \begin{align}
%     \prob{P}_\alpha^{(\RV{Y}_i,\RV{D}_i)_{i\in\mathbb{N}}|\RV{H}}(A\times B|h) \overset{a.s.}{=} h(A\times B)\label{eq:given_h}
% \end{align}

% The proof that the limit exists and the above equality holds can again be found int \citep{kallenberg_basic_2005}.
% \end{proof}

\subsection{Causal consequences exist if the model is causally contractible}

The main result in this section is Theorem \ref{th:iid_rep} which shows that a conditional probability model $\prob{P}_\square$ is causally contractible if and only if it can be represented as the product of a distribution over hypotheses $\prob{P}_\square^{\RV{H}}$ and a collection of identical conditional probabilities $\prob{P}_\square^{\RV{Y}_1|\RV{D}_1\RV{H}}$. This can be interpreted as expressing the idea that all $(\RV{Y}_i,\RV{D}_i)$ pairs share a canonical but unknown ``consequence function'' $D\kto Y$. As discussed in Section \ref{sec:curry}, the existence of such a consequence function implies the existence of a common unknown curried consequence function. Curried consequence functions look very similar to potential outcomes models, but they don't necessarily support any counterfactual interpretation.

\begin{lemma}[Exchangeable curried representation]\label{th:table_rep}
A conditional probability model $(\prob{P}^{\RV{Y}|\RV{D}}_\square,A)$ such that $\RV{D}:=(\RV{D}_i)_{i\in \mathbb{N}}$ and $\RV{Y}:=(\RV{Y}_i)_{i\in \mathbb{N}}$. $\prob{P}_\square$ is causally contractible if and only if
\begin{align}
    \prob{P}_\square^{\RV{Y}|\RV{D}} &= \tikzfig{lookup_representation}\\
    &\iff\\
    \prob{P}_\square^{\RV{Y}|\RV{D}}(y|d) &= \prob{P}^{(\RV{Y}^D_{d_i i})_{\mathbb{N}}}(y)
\end{align}
Where $\prob{P}^{\RV{Y}^D}$ is an exchangeable probability measure on $Y^{D\times\mathbb{N}}$, for convenience we extend the sample space with the random variable $\RV{Y}^D:=(\RV{Y}_{ij}^D)_{i\in D,j\in \mathbb{N}}$ and $\prob{L}^{\RV{D},\RV{Y}^D}$ is the Markov kernel associated with the lookup function
\begin{align}
    l:D^\mathbb{N}\times Y^{D\times \mathbb{N}}&\to Y\\
    ((d_i)_\mathbb{N},(y_{ij})_{i\in D,j\in \mathbb{N}})&\mapsto y_{d_i i}
\end{align}
\end{lemma}

\begin{proof}
Only if:
Choose $e:=(e_i)_{i\in\mathbb{N}}$ such that $e_{|D|i+j}$ is the $i$th element of $D$ for all $i,j\in \mathbb{N}$. Abusing notation, write $e$ also for the decision function that chooses $e$ deterministically.

Define
\begin{align}
    \prob{P}^{\RV{Y}^D}((y_{ij})_{D\times \mathbb{N}}):=\prob{P}_e^{\RV{Y}}((y_{|D|i+j})_{i\in D, j\in \mathbb{N}})
\end{align}

Now consider any $d:=(d_i)_{i\in \mathbb{N}}\in D^{\mathbb{N}}$. By definition of $e$, $e_{|D|d_i + i}=d_i$ for any $i,j\in \mathbb{N}$.

\begin{align}
    \prob{Q}:D\kto Y\\
    \prob{Q}:= \tikzfig{lookup_representation}
\end{align}

and consider some ordered sequence $A\subset \mathbb{N}$ and $B:= ((|D|d_i+i))_{i\in A}$. Note that $e_B:=(e_{|D|d_i +i})_{i\in B}=d_A=(d_i)_{i\in A}$. Then 

\begin{align}
    \sum_{y\in \RV{Y}^{-1}(y_A)} \prob{Q}(y|d) &= \sum_{y\in \RV{Y}^{-1}(y_A)} \prob{P}^{(\RV{Y}^{D}_{d_ii})_{A}}(y) \\
    &= \sum_{y\in \RV{Y}^{-1}(y_A)} \prob{P}_e^{(\RV{Y}_{|D|d_i+i})_{A}}(y)\\
    &= \prob{P}_e^{\RV{Y}_{B}}(y_A)\\
    &= \prob{P}_{d}^{\RV{Y}_A}(y_A)&\text{by causal contractibility}
\end{align}

Because this holds for all $A\subset\mathbb{N}$, by the Kolmogorov extension theorem

\begin{align}
    \prob{Q}(y|d) &= \prob{P}_d^{\RV{Y}}(y)
\end{align}

Because $d$ is the decision function that deterministically chooses $d$, for all $d\in D$

\begin{align}
    \prob{Q}(y|d) &= \prob{P}_d^{\RV{Y}|\RV{D}}(y|d)
\end{align}

And because $\prob{P}_d^{\RV{Y}|\RV{D}}(y|d)$ is unique for all $d\in D^{\mathbb{N}}$ and $\prob{P}^{\RV{Y}|\RV{D}}$ exists by assumption

\begin{align}
    \prob{P}^{\RV{Y}|\RV{D}}=\prob{Q}
\end{align}

Next we will show $\prob{P}^{\RV{Y}^D}$ is contractible. Consider any subsequences $\RV{Y}^D_S$ and $\RV{Y}^D_T$ of $\RV{Y}^D$ with $|S|=|T|$. Let $\rho(S)$ be the ``expansion'' of the indices $S$, i.e. $\rho(S)=(|D|i+j)_{i\in S,j\in D}$. Then by construction of $e$, $e_{\rho(S)}=e_{\rho(T)}$ and therefore

\begin{align}
    \prob{P}^{\RV{Y}^D_S}&= \prob{P}_e^{\RV{Y}_{\rho(S)}})\\
    &= \prob{P}_e^{\RV{Y}_{\rho(T)}})&\text{by contractibility of }\prob{P}\text{ and the equality } e_{\rho(S)}=e_{\rho(T)}\\
    &= \prob{P}^{\RV{Y}^D_T}
\end{align}


If:
Suppose 
\begin{align}
    \prob{P}^{\RV{Y}|\RV{D}} &= \tikzfig{lookup_representation}
\end{align}

and consider any two deterministic decision functions $d,d'\in D^{\mathbb{N}}$ such that some subsequences are equal $d_S=d'_T$.

Let $\RV{Y}^{d_S}=(\RV{Y}_{d_i i})_{i\in S}$.

By definition,

\begin{align}
    \prob{P}^{\RV{Y}_S|\RV{D}}(y_S|d) &= \sum_{y^D_S\in Y^{|D|\times |S|}}\prob{P}^{\RV{Y}^D_S}(y^D_S)\prob{L}^{\RV{D}_S,\RV{Y}^S}(y_S|d,y^D_S)\\
    &= \sum_{y^D_S\in Y^{|D|\times |T|}}\prob{P}^{\RV{Y}^D_T}(y^D_S)\prob{L}^{\RV{D}_S,\RV{Y}^S}(y_S|d,y^D_S)&\text{ by contractibility of }\prob{P}^{\RV{Y}^D_T}\\
    &= \prob{P}^{\RV{Y}_T|\RV{D}}(y_S|d)
\end{align}
\end{proof}

The curried representation of Lemma \ref{th:table_rep} does not need to support an interpretation as a distribution of potential outcomes. For example, consider a series of bets on fair coinflips -- in this case, the consequence $\RV{Y}_i$ is uniform on $\{0,1\}$ for any decision $\RV{D}_i$. Tha $D=Y=\{0,1\}$ and $\prob{P}_\alpha^{\RV{Y}_n}(y)=\prod_{i\in [n]} 0.5$ for all $n$, $y\in Y^n$, $\alpha\in R$. Then the construction in Lemma \ref{th:table_rep} yields $\prob{P}^{Y^D_i}(y^D_i)=\prod_{j\in D} 0.5$ for all $y^D_i\in Y^D$. That is, $\RV{Y}^0_i$ and $\RV{Y}^1_i$ are independent and uniformly distributed. However, if we wanted $\RV{Y}^0_i$ to represent ``what would happen if I bet on outcome 0 on turn $i$'' and $\RV{Y}^1$ to represent ``what would happen if I bet on outcome 1 on turn $i$'', then it seems that we ought to have $\RV{Y}^0_i = 1-\RV{Y}^1_i$. 

We could suppose that Lemma \ref{th:table_rep} provides necessary but not sufficient conditions for the existence of a potential outcomes representation of a conditional probability model. However, it doesn't seem to succeed at that either. We note, for example, that \citet{rubin_causal_2005} does not assume that the distribution of potential outcomes is exchangeable. A non-exchangeable $\prob{P}^{\RV{Y}^D}$ does not induce a causally contractible conditional probability model, and at the same time commutativity with marginalisation is not sufficient for a conditional probability model to support a curried representation in the sense of Lemma \ref{th:table_rep}. What seems to be missing is an additional assumption that consequences are mutually independent of one another given the associated decision. 

We can also represent contractible conditional probability models repeated copies of an unknown ``consequence function'', a Markov kernel that maps from decisions to probability distributions over consequences, coupled by a common hypothesis $\RV{H}$. 

\begin{theorem}\label{th:iid_rep}
Suppose we have a fundamental probability set $\Omega$ and a do model $(\prob{P},\RV{D},\RV{Y},R)$ such that $\RV{D}:=(\RV{D}_i)_{i\in \mathbb{N}}$ and $\RV{Y}:=(\RV{Y}_i)_{i\in\mathbb{N}}$. $\prob{P}$ is causally contractible if and only if there exists some $\RV{H}:\Omega\to H$ such that $\prob{P}^{\RV{Y}_i|\RV{H}\RV{D}_i}$ exists for all $i\in \mathbb{N}$ and
\begin{align}
    \prob{P}^{\RV{Y}|\RV{H}\RV{D}} &= \tikzfig{do_model_representation}\\
    &\iff\\
    \RV{Y}_i&\CI_{\prob{P}} \RV{Y}_{\mathbb{N}\setminus i},\RV{D}_{\mathbb{N}\setminus i}|\RV{H}\RV{D}_i&\forall i\in \mathbb{N}\\
    \land \prob{P}^{\RV{Y}_i|\RV{H}\RV{D}_i} &= \prob{P}^{\RV{Y}_0|\RV{H}\RV{D}_0} & \forall i\in \mathbb{N}
\end{align}
\end{theorem}

\begin{proof}
We make use of Lemma \ref{th:table_rep} to show that we can represent the conditional probability as an exchangeable tabular probability distribution. We then use the property of exchangeability of the columns of that distribution in conjunction with De Finetti's theorem to derive the result.
\end{proof}

\subsection{Modelling different measurement procedures}

An important question is: when is it reasonable to assume causal contractibility? We're going to focus just on the assumption of commutativity of exchange because we have more interesting things to say about it. There is a tempting but false line of argument one could adopt: $(\prob{P}_\square^{\RV{Y}_M|\RV{D}_M},A)$ is a model of $|M|$ indistinguishable ``experimental units'', because they are indistinguishable they can be interchanged without altering the appropriate model, and so commutativity of exchange holds.

The problem with this line of reasoning is that interchangeability of ``experimental units'' doesn't imply commutativity of exchange. The problem is, roughly speaking, we may have indistinguishable experimental units when a decision function is chosen, but the decision function might leave some uncertainty over the actual decisions, which means the experimental units may be distinguishable when the actual decisions are made. If the decision function is deterministic, this possibility is ruled out. We'll explain this in more detail with an example, and in the next section we'll discuss randomisation.

\subsection{Example: commutativity of exchange in the context of treatment choices}

To justify an assumption of commutativity of exchange, we will argue as follows:
\begin{itemize}
    \item Two measurement procedures should be considered equivalent in the sense that the same model is appropriate for both
    \item The models associated with the two procedures are related to one another by composition with the relevant swap maps
    \item Therefore the model associated with the first experiment is equivalent to the same model composed with the relevant swap maps
\end{itemize}

First, we want to spell out in detail how composing a model of one measurement procedure with a swap map can result in a model appliccable to a different measurement procedure. Recall that we assume that a single master measurement procedure $\proc{S}$ taking values in $\Psi$, and observables are all functions of $\proc{S}$. Given a model $(\prob{P}_\square,A)$ associated with $\proc{S}$, the model does not in general apply to an alternative measurement procedure $\proc{S}'$.

However, it is also a principle of measurement procedures that a measurement procedure followed by the application of a function is itself a measurement procedure. Thus a model $(\prob{P}_\square,A)$ associated with $\proc{S}$ may also be informative about a procedure $f\circ \proc{S}$ for any $f:\Psi\to X$.

In particular, consider measurement procedures related by \emph{swaps}. For example, suppose we have $(\proc{D}_1,\proc{D}_2)$ and $(\proc{D}^{\text{swap}}_1,\proc{D}^{\text{swap}}_2):=(\proc{D}_2,\proc{D}_1)$. Then, given any probability model $\prob{P}_\alpha^{\RV{D}_1\RV{D}_2}$ we have $\prob{P}_\alpha^{\RV{D}_1^{\text{swap}}\RV{D}_2^{\text{swap}}} = \prob{P}_\alpha^{\RV{D}_1\RV{D}_2}$. In this way, $\prob{P}_\alpha^{\RV{D}_1\RV{D}_2}$ is a model of $(\proc{D}_1,\proc{D}_2)$ and induces a unique model of $(\proc{D}^{\text{swap}}_1,\proc{D}^{\text{swap}}_2)$ via composition with a swap map.

Technically, this requires an assumption: if $\RV{X}$ is associated with $\proc{X}$ then $f\circ \RV{X}$ is associated with $f\circ \proc{X}$ (roughly: the abstract mathematical idea of composing a function with something and the actual process of applying a function to something and obtaining a result are treated as the same thing)

Concretely, commutativity of exchange can be justified if we suppose that the same model $(\prob{P}_\square^{\RV{Y}_M|\RV{D})_M},A)$ should describe
\begin{itemize}
    \item A measurement procedure $\proc{S}$ that yields $|M|$ outcomes $\proc{Y}_M$ and and $|M|$ decisions $\proc{D}_M$
    \item Any other $|M|$ outcomes $\proc{Y}^{\text{swap}}_M$ and $|M|$ decisions $\proc{D}^{\text{swap}}_M$, related to the originals by a swap.
\end{itemize}

Consider the following two scenarios:

\begin{enumerate}
    \item Dr Alice is going to see two patients who are both complaining of lower back pain and are otherwise unknown to Alice. Prior to seeing them, she settles on a decision function $\alpha$ which deterministically sets her treatment choices according to a function $\text{decisions}(\alpha)$
    \item As before, but $\alpha$ is a ``decision inclination'' and $\prob{P}_\alpha^{\RV{D}_1\RV{D}_2}$ nondeterministic
\end{enumerate}

Alice could model both situations with a sequential conditional probability model $(\prob{P}_\square^{\RV{Y}_1\RV{Y}_2|\RV{D}_1\RV{D}_2},A)$ with the elements of $A$ identified with probability models of the form $\prob{P}_\alpha^{\RV{D}_1\RV{D}_2}$. Might she, in one or both situations, consider this condiitonal probability model to be causally contractible?

We will assume that both satisfy commutativity of marginalisation -- that is, the first patient's outcomes are expected to be the same no matter what is planned for the second patient and vise versa. We want to know if they satisfy commutativity of exchange.

The argument we want to make (if it can be supported) is:
\begin{itemize}
    \item We can describe two measurement procedures that should share the same model
    \item The first is a measurement procedure for $(\RV{D}_1,\RV{D}_2,\RV{Y}_1,\RV{Y}_2)$
    \item The second is a measurement procedure for $(\RV{D}^{\text{swap}}_1,\RV{D}^{\text{swap}}_2,\RV{Y}^{\text{swap}^{-1}}_1,\RV{Y}^{\text{swap}^{-1}}_2)$
\end{itemize}

At the outset, Alice does not know any features that might distinguish the two patients, so it is reasonable to think that she should adopt the same model for a) the original experiment and b) the same experiment, except with the patients interchanged. Note that interchanging \emph{patients} does not correspond directly to any operation on the model $(\prob{P}_\square^{\RV{Y}_1\RV{Y}_2|\RV{D}_1\RV{D}_2},A)$ which describes decisions and, not patients.

We will define measurement procedures using pseudocode, because we find it a lot easier to keep track of operations like swaps in this format. This presentation has the unintended effect of suggesting that measurement procedures are like computer programs. We're not sure if this is a helpful way to think about things -- one of the key points of this example is that precise and imprecise measurement procedures may need quite different models, but thinking of measurement procedures as computer programs suggests that all measurement procedures are precise, which is not the case. Some steps may be precise, and we can express these steps with pseudocode, while other steps may be less precise. 

Suppose the first scenario corresponds to the following procedure $\proc{S}$ which yields values in $A\times D^2\times Y^2$. $\RV{D}_i$ is the projection $(\alpha,d_1,d_2,y_1,y_2)\mapsto d_i$ composed with $\proc{S}$ and $\RV{Y}_i$ is the projection $(\alpha,d_1,d_2,y_1,y_2)\mapsto y_i$ composed with $\proc{S}$.
\begin{algorithmic}
    \Procedure{$\proc{S}$}{}
    \Assert{patient A knowledge=patient B knowledge}
    \State $\alpha \gets \mathrm{choose\_alpha}$
    \State $(\proc{D}_1,\proc{D}_2) \gets \mathrm{decisions}(\alpha)$
    \State $\proc{Y}_1\gets \mathrm{apply}(\proc{D}_1,\mathrm{patient\;A})$
    \State $\proc{Y}_2\gets \mathrm{apply}(\proc{D}_2,\mathrm{patient\;B})$
    \State \Return $(\alpha,\proc{D}_1,\proc{D}_2,\proc{Y}_1,\proc{Y}_2)$
    \EndProcedure
\end{algorithmic}


Make the assumption that, on the basis that the patients are indistinguishable to Alice at the time of model construction, the same model is appropriate for the original measurement procedure and a modified measurement procedure in which the patients are swapped (we say the measurement procedures are ``equivalent''). Assume also that swapping the order of treatment and swapping the order in which outcomes are recorded yields an equivalent measurment procedure (in \citet{walley_statistical_1991}'s language, the first assumption is based on ``symmetry of evidence'' and the second on ``evidence of symmetry''). Putting these two assumptions together, the following procedure $\proc{S}'$ is equivalent to the original:

\begin{algorithmic}
    \Procedure{$\proc{S}'$}{}
    \Assert{patient A knowledge=patient B knowledge}
    \State $\alpha \gets \mathrm{choose\_alpha}$
    \State $(\proc{D}_1,\proc{D}_2) \gets \mathrm{decisions}(\alpha)$
    \State $\proc{Y}_2\gets \mathrm{apply}(\proc{D}_2,\mathrm{patient\;A})$
    \State $\proc{Y}_1\gets \mathrm{apply}(\proc{D}_1,\mathrm{patient\;B})$
    \State \Return $(\alpha,\proc{D}_1,\proc{D}_2,\proc{Y}_1,\proc{Y}_2)$
    \EndProcedure
\end{algorithmic}

Consider another measurement procedure $\proc{S}''$, which is a modified version of $\proc{S}$ where steps are added to swap decisions after they are chosen, then outcomes are swapped back once they have been observed:

\begin{algorithmic}
    \Procedure{$\proc{S}''$}{}
    \Assert{patient A knowledge=patient B knowledge}
    \State $\alpha \gets \mathrm{choose\_alpha}$
    \State $(\proc{D}_1,\proc{D}_2) \gets \mathrm{decisions}(\alpha)$
    \State $(\proc{D}_1^{\mathrm{swap}},\proc{D}_2^{\mathrm{swap}}) \gets (\proc{D}_2,\proc{D}_1)$
    \State $\proc{Y}^{\mathrm{swap}}_1\gets \mathrm{apply}(\proc{D}_1^{\mathrm{swap}},\mathrm{patient A})$
    \State $\proc{Y}_2^{\mathrm{swap}}\gets \mathrm{apply}(\proc{D}_2^{\text{swap}},\text{patient B})$
    \State $(\proc{Y}_1,\proc{Y}_2)\gets (\proc{Y}^{\mathrm{swap}}_2,\proc{Y}^{\mathrm{swap}}_1)$
    \State \Return $(\alpha,\proc{D}_1,\proc{D}_2,\proc{Y}_1,\proc{Y}_2)$
    \EndProcedure
\end{algorithmic}

Instead of explicitly performing the swaps, we can substitute $\proc{D}_2$ for $\proc{D}_1^{\text{swap}}$, $\proc{Y}_2$ for $\proc{Y}_1^{\text{swap}}$ and so on. The result is a procedure identical to $\proc{S}'$

\begin{algorithmic}
    \Procedure{$\proc{S}''$}{}
    \Assert{patient A knowledge=patient B knowledge}
    \State $\alpha \gets \mathrm{choose\_alpha}$
    \State $(\proc{D}_1,\proc{D}_2) \gets \mathrm{decisions}(\alpha)$
    \State $\proc{Y}_2\gets \mathrm{apply}(\proc{D}_2,\mathrm{patient\;A})$
    \State $\proc{Y}_1\gets \mathrm{apply}(\proc{D}_1,\mathrm{patient\;B})$
    \State \Return $(\alpha,\proc{D}_1,\proc{D}_2,\proc{Y}_1,\proc{Y}_2)$
    \EndProcedure
\end{algorithmic}

Thus $\proc{S}''$ is exactly the same as $\proc{S}'$, which by assumption is equivalent to the original $\proc{S}$, and so the assumptions of interchangeable patients and reversible order of treatment application imply the model should commute with exchange. Thus, if we could extend this example to an infinite sequence of patients, there would exist a Markov kernel $\prob{P}_\square^{\RV{Y}|\RV{DH}}:D\times H\kto Y$ representing a ``definite but unknown causal consequence'' shared by all experimental units.

This argument does \emph{not} hold for scenario 2. In the absence of a deterministic function $\text{decisions}(\alpha)$ which defines the procedure for obtaining $\proc{D}_1$ and $\proc{D}_2$, there is some flexibility for how exactly these variables are measured (or chosen). In particular, we can posit measurement procedures such that permuting patients is not equivalent to permuting decisions and then appying the reverse permutation to outcomes.

For example, procedure $\proc{T}$ is compatible with scenario 2 (note that there are many procedures compatible with the given description)

\begin{algorithmic}
    \Procedure{$\proc{T}$}{}
    \Assert{patient A knowledge=patient B knowledge}
    \State $\alpha \gets \mathrm{choose\_alpha}$
    \State patient A knowledge$\gets \mathrm{inspect}$(patient A)
    \State patient B knowledge$\gets \mathrm{inspect}$(patient B)
    \State $(\proc{D}_1,\proc{D}_2) \gets \mathrm{vagueDecisions}(\alpha$, patient A knowledge, patient B knowledge)
    \State $\proc{Y}_1\gets \mathrm{apply}(\proc{D}_1,\mathrm{patient\;A})$
    \State $\proc{Y}_2\gets \mathrm{apply}(\proc{D}_2,\mathrm{patient\;B})$
    \State \Return $(\alpha,\proc{D}_1,\proc{D}_2,\proc{Y}_1,\proc{Y}_2)$
    \EndProcedure
\end{algorithmic}

Permutation of patients and treatment order now yields

\begin{algorithmic}
    \Procedure{$\proc{T}'$}{}
    \Assert{patient A knowledge=patient B knowledge}
    \State $\alpha \gets \mathrm{choose\_alpha}$
    \State patient B knowledge$\gets \mathrm{inspect}$(patient B)
    \State patient A knowledge$\gets \mathrm{inspect}$(patient A)
    \State $(\proc{D}_1,\proc{D}_2) \gets \mathrm{vagueDecisions}(\alpha$, patient B knowledge, patient A knowledge)
    \State $\proc{Y}_2\gets \mathrm{apply}(\proc{D}_2,\mathrm{patient\;A})$
    \State $\proc{Y}_1\gets \mathrm{apply}(\proc{D}_1,\mathrm{patient\;B})$
    \State \Return $(\alpha,\proc{D}_1,\proc{D}_2,\proc{Y}_1,\proc{Y}_2)$
    \EndProcedure
\end{algorithmic}

While paired permuation of decisions and outcomes yields

\begin{algorithmic}
    \Procedure{$\proc{T}''$}{}
    \Assert{patient A knowledge=patient B knowledge}
    \State $\alpha \gets \mathrm{choose\_alpha}$
    \State patient A knowledge$\gets \mathrm{inspect}$(patient A)
    \State patient B knowledge$\gets \mathrm{inspect}$(patient B)
    \State $(\proc{D}_1,\proc{D}_2) \gets \mathrm{vagueDecisions}(\alpha$, patient A knowledge, patient B knowledge)
    \State $\proc{Y}_2\gets \mathrm{apply}(\proc{D}_2,\mathrm{patient\;A})$
    \State $\proc{Y}_1\gets \mathrm{apply}(\proc{D}_1,\mathrm{patient\;B})$
    \State \Return $(\alpha,\proc{D}_1,\proc{D}_2,\proc{Y}_1,\proc{Y}_2)$
    \EndProcedure
\end{algorithmic}

$\proc{T}'$ is not the same as $\proc{T}''$. In scenario 1, because decisions were deterministic on $\alpha$, there was no room to pick anything different once $\alpha$ was chosen, so it doesn't matter if we add patient inspection steps or not. In scenario 2, decisions are not deterministic and there is vagueness in the procedure, so it is possible to describe compatible procedures where decisions depend on patient characteristics, and this dependence is not ``undone'' by swapping decisions.


\subsection{Causal consequences of non-deterministic variables}

In the previous section we gave an example of how commutativity of exchange can hold when we have a sequence of decisions such that we accept the follwing:

\begin{itemize}
    \item Reordering the time at which decisions are made yields an equivalent problem
    \item The available information relevant to each decision is symmetric at the time the decision function is adopted
    \item The decision function deterministically prescribes which decisions are taken
\end{itemize}

We also discussed how the absence of the determinism assumption undermines the argument.

The determinism assumption rules out choosing decisions randomly. However, if we have causal consequences for deterministic decision variables, it is sometimes possible to extend them to indeterministic variables. 

\begin{lemma}
Given $(\prob{P}_\square,A)$ with decisions $\RV{D}_M$ and consequences $\RV{Y}_M$, if $\prob{P}_\square^{\RV{Y}_M|\RV{D}_M}$ is causally contractible with consequence map $\prob{P}_\square^{\RV{Y}_0|\RV{D}_0\RV{H}}$ and there exists $\RV{X}_i=f\circ \RV{Y}_i$ for some $f:Y\to X$ such that $\RV{Y}_i\CI_{\prob{P}_\square} \RV{D}_i|\RV{HX}_i$ for all $i\in M$, then a causally contractible conditional probability $\prob{P}_\square^{\RV{Y}_M|\RV{X}_M}$ exists.
\end{lemma}

\begin{proof}
We want to show $\RV{Y}_i\CI_{\prob{P}_\square} \RV{Y}_{\{i\}^C}\RV{X}_{\{i\}^C} |\RV{H}\RV{X}_i$ for all $i\in M$, $\prob{P}_\square^{\RV{Y}_i|\RV{H}\RV{X}_i}$ exists for all $i\in M$ and $\prob{P}_\square^{\RV{Y}_i|\RV{H}\RV{X}_i}=\prob{P}_\square^{\RV{Y}_j|\RV{H}\RV{X}_j}$.

Because $\RV{X}_i$ is a function of $\RV{Y}_i$, and $\RV{Y}_i\CI_{\prob{P}_\square} \RV{Y}_{\{i\}^C}\RV{D}_{\{i\}^C} |\RV{H}\RV{D}_i$, we also have $\RV{YX}_i\CI_{\prob{P}_\square} \RV{Y}_{\{i\}^C}\RV{X}_{\{i\}^C} |\RV{H}\RV{D}_i$, and by weak union $\RV{Y}_i\CI_{\prob{P}_\square} \RV{Y}_{\{i\}^C}\RV{X}_{\{i\}^C} |\RV{H}\RV{D}_i\RV{X}_i$

Thus by contraction, $\RV{Y}_i\CI_{\prob{P}_\square} \RV{Y}_{\{i\}^C}\RV{D}_{M} |\RV{H}\RV{X}_i$.

By Corollary \ref{cor:ci_cp_exist} and the existence of $\prob{P}^{\RV{Y}_i\RV{X}_i|\RV{H}\RV{D}_i}$ for all $i\in M$, $\prob{P}_\square^{\RV{Y}_i|\RV{H}\RV{X}_i}$ exists for all $i$. Furthermore, because $\prob{P}^{\RV{Y}_i\RV{X}_i|\RV{H}\RV{D}_i}=\prob{P}^{\RV{Y}_j\RV{X}_j|\RV{H}\RV{D}_j}$ for all $i,j\in M$, $\prob{P}_\square^{\RV{Y}_i|\RV{H}\RV{X}_i}=\prob{P}_\square^{\RV{Y}_j|\RV{H}\RV{X}_j}$ for all $i,j\in M$.
\end{proof}

If the condition $\RV{Y}_i\CI_{\prob{P}_\square} \RV{D}_i|\RV{HX}_i$ for all $i\in M$, we can say $\RV{X}_i$ is a proxy for controlling $\RV{Y}_i$.

As an example of this, suppose $\RV{X}:\Omega\to X$ is a source of random numbers, the set of decisions $D$ is a set of functions $X\to T$ for treatments $\RV{T}:\Omega\to T$ and $\RV{W}:\Omega\to W$ are the ultimate patient outcomes, with $\RV{Y}_i=(\RV{W}_i,\RV{T}_i)$. Then it may be reasonable to assume that $\RV{W}_i\CI(\RV{D}_i,\RV{X}_i)|\RV{T}_i\RV{H}$ (where conditioning on $\RV{H}$ can be thought of as saying that this independence holds under infinite sample size). In this case, $\RV{T}_i$ is a proxy for controlling $\RV{Y}_i$, and there exists a causal consequence $\prob{P}_\square^{\RV{Y}_0|\RV{T}_0\RV{H}}$.

A ``causal consequence of body mass index'' is unlikely to exist on the basis of symmetric information and deterministic decisions because there are no actions available to set body mass index deterministically. However, given an underlying problem where we have symmetric information over a collection of patients and some kind of decision that can be made deterministically, causal consequences of body mass index may exist if body mass index is a proxy for controlling the outcomes of interest.

\subsection{Intersubjective causal consequences}

While the assumption of causal contractibility itself does not depend on any notion of subjectivity, our discussion of the appliccability of this assumption assumed that a conditional probability model was being used to model Dr Alice's subjective uncertain knowledge. Crucially, the justification hinged on an assumption of the symmetry of Alice's information regarding different patients.

Causal inference is often performed in an intersubjective setting, where Ben might perform the experimeng, Carmel might do the analysis and Dr Alice make the ultimate decisions. This complicates the question of when the assumption of causal contractibility is appliccable. We leave the appropriate way to generalise this theory to such a setting open.