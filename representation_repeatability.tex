%!TEX root = main.tex

\section{When do response conditionals exist?}

\todo[inline]{Lemmas are intermediate steps}

Our approach is to model decision problems with probability sets $\prob{P}_C$ for some set of choices $C$. If we have a pair of variables $\RV{X}$ and $\RV{Y}$ such that $\prob{P}_C^{\RV{Y}|\RV{X}}$ exists, then the model says that the joint outcome $\prob{P}_\alpha^{\RV{XY}}$ of any choice $\alpha\in C$ can be computed from the marginal distribution $\prob{P}_\alpha^{\RV{X}}$ alone. We are going to ask the question: in which kind of probability sets do uniform conditionals of the form $\prob{P}_C^{\RV{Y}|\RV{X}\RV{H}}$ exist? Here $\RV{H}$ is a ``fixed but unknown'' hypothesis that becomes better known as more data is observed. Roughly speaking, $\prob{P}_C^{\RV{Y}|\RV{X}\RV{H}}$ represents the response of $\RV{Y}$ to $\RV{X}$ regardless of which choice is made.

A decision makers may be interested in a functions like $\prob{P}_C^{\RV{Y}|\RV{X}\RV{H}}$. Suppose they have substantial prior knowledge about how to control $\RV{X}$, less knowledge about controlling $\RV{Y}$ and access to a sequence of data points. If the data points can identify $\RV{H}$, then  If a decision maker has prior knowledge of how to control $\RV{X}$ and data that is informative about the value of $\RV{H}$, these pieces of knowledge together can help them to control $\RV{Y}$. We call a uniform conditional of the form $\prob{P}_C^{\RV{Y}|\RV{X}\RV{H}}$ a \emph{response conditional}.

If a model $\prob{P}_C$ supports $\prob{P}_C^{\RV{Y}|\RV{XH}}$, it also provides an answer to the concerns of HernÃ¡n and Taubman. Paraphrasing their argument without the use of potential outcomes: they were concerned that there may be different ways to achieve particular values or distributions over $\RV{X}$, and these may also lead to different distributions over $\RV{Y}$. In that case, they argued that there was no well-defined causal effect of $\RV{X}$ on $\RV{Y}$. However, if $\prob{P}_C^{\RV{Y}|\RV{XH}}$ then the model describes a situation where -- once we have enough data to pin down $\RV{H}$ -- it doesn't matter any more which particular choice leads to a given distribution over $\RV{X}$.

We consider first the question of what kind of probability sets $\prob{P}_C$ support uniform conditional distributions of the form $\prob{P}_C^{\RV{Y}|\RV{XH}}$. Secondly, we consider what kind of decision procedures can be modeled by probability sets of this type.

\subsection{Sequential decision models}

In order to pose this question, we need a setting in which we expect to observe sequential data. That is, our model is a probability set $\prob{P}_C$ on sample space $(\Omega,\sigalg{F})$ such that we have variables $\RV{Y}:=(\RV{Y}_i)_{i\in M}$ (the outcome sequence) and $\RV{D}:=(\RV{D}_i)_{i\in M}$ (the action sequence) for some index set $M\subset\mathbb{N}$. We say $\RV{Y}_i$ corresponds to $\RV{D}_i$. We are specifically looking for uniform conditionals of the form $\prob{P}_C^{\RV{Y}_i|\RV{D}_i\RV{H}}$ for all $i\in M$. $\RV{H}$ here is a hypothesis, and it must be fixed with respect to different choices -- i.e. $\RV{H}\CI^e_{\prob{P}_C} C$. 

We assume a starting point that $\prob{P}_C^{\RV{Y}|\RV{D}}$ exists. This could be guaranteed, for example, if each choice $\alpha$ corresponds to a unique deterministic distribution $\prob{P}_\alpha^{\RV{D}}$ (see Example \ref{ex:choice_var}).

There are two further assumptions relevant to the existence of response conditionals. The first is \emph{exchange commutativity}. This is the condition that we get the same result from applying a swap transformation to the input of $\prob{P}_C^{\RV{Y}|\RV{D}}$ as we get from applying the same swap transformation to its output.

The second is a condition of \emph{consequence locality}. This is the assumption that, for any $A\subset M$, $\prob{P}_C^{\RV{Y}_A|\RV{D}_A}$ exists and

\begin{align}
    \prob{P}_C^{\RV{Y}_A|\RV{D}_M} &= \tikzfig{consequence_locality}
\end{align}

In the language of extended conditional independence, it is the assumption $\RV{Y}_A\CI_{\prob{P}_C}^e C\RV{D}_{M\setminus A} |\RV{D}_A$.

Exchange commutativity is similar, but not identical, to a number of assumptions discussed in the literature. \emph{Post-treatment exchangeability} found in \citet{dawid_decision-theoretic_2020} is implied by exchange commutativity, but not the reverse. There are also notions of ``causal exchangeability'' found in \citet{greenland_identifiability_1986} and \citet{banerjee_chapter_2017}; a subtle difference between these notions and exchange commutativity is that these latter notions are symmetries of \emph{procedures} -- they involve actually swapping actions or individuals in an experiment -- while exchange commutativity is a symmetry of a probability set.

Consequence locality is similar to the stable unit treatment distribution assumption (SUTDA) in \citet{dawid_decision-theoretic_2020}. It is also related to the ``no interference'' part of the stable unit treatment value assumption (SUTVA). The stable unit treatment value assumption (SUTVA) is given as \citep{rubin_causal_2005}:

\begin{blockquote}
(SUTVA) comprises two subassumptions. First, it assumes that \emph{there is no interference between units (Cox 1958)}; that is, neither $Y_i(1)$ nor $Y_i(0)$ is affected by what action any other unit received. Second, it assumes that \emph{there are no hidden versions of treatments}; no matter how unit $i$ received treatment $1$, the outcome that would be observed would be $Y_i(1)$ and similarly for treatment $0$.
\end{blockquote}

Both SUTDA and SUTVA talk about how an outcome $\RV{Y}_i$ does not depend on, or is not affected by, any of the actions that do not correspond to it. Such statements would need to be made more precisely if we want to evaluate what precise relation they have to consequence locality.

\todo[inline]{Put the following in the discussion of decision procedures}

It is possible to have models in which commutativity to exchange holds but locality of consequences does not. Such a situation could arise in a model of stimulus payments to individuals in a nation; if exactly $n$ payments of \$10 000 are made, we might consider that it doesn't matter much exactly who receives the payments (this is a subtle question, though, we will return to it in more detail later). However, the amount of inflation induced depends on the number of payments; making 100 such payments will have a negligible effect on inflation, while making payments to everyone in the country is likely to have a substantial effect. \citet{dawid_causal_2000} discusses condition of \emph{post-treatment exchangeability} which is similar to exchange commutativity, and there he gives the example of herd immunity in vaccination campaigns as a situation where post-treatment exchangeability holds but locality of consequences does not.

\todo[inline]{Put the preceding in the discussion of decision procedures}

\todo[inline]{Not sure if or where I want to put this, I just think it helps to illustrate the difference}

The difference between exchangeability \citep{de_finetti_foresight_1992} and exchange commutativity is illustrated by the following pair of diagrams. Exchangeability is a symmetry of probability distributions -- a distribution is exchangeable if it is unchanged by swapping outputs. Exchange commutativity is a symmetry of Markov kernels -- a Markov kernel is exchange commutative if swapping inputs and swapping outputs gives the same result.

Exchangeability (swapping labels):

\begin{align}
    \tikzfig{exchangeability}
\end{align}

Exchange commutativity (swapping choices $\sim$ swapping labels):

\begin{align}
    \tikzfig{commutativity_of_exchange}
\end{align}

\todo[inline]{----end not sure where to put------}


% Another way to see where we are going is to consider graphical statements of our and De Finetti's result.

% Take $S=\{0,1\}$ and identify the space $\Delta(S)$ of probability measures on $S$ with the interval $[0,1]$. De Finetti showed that any infinite exchangeable probability measure $\prob{P}_\alpha$ on $\{0,1\}^\mathbb{N}$ can be represented by a prior $\prob{P}_\alpha^{\RV{H}}\in [0,1]$ for some $\RV{H}:\Omega\to H$ and a conditional probability $\prob{P}^{\RV{S}_0|\RV{H}}:[0,1]\kto \{0,1\}$ such that

% \begin{align}
%     \prob{P}_\alpha &= \tikzfig{de_finetti_rep0}\label{eq:definettirep}
% \end{align}

% Here $\prob{P}^{\RV{S}_0|\RV{H}}$ can be defined concretely by $\prob{P}^{\RV{S}_0|\RV{H}}(1|h)=h$. Equivalently, the probability gap model on $S^\mathbb{N}$ defined by the assumption of exchangeability is equivalent to the probability gap model defined by the conditional probability

% \begin{align}
%     \prob{P}^{\RV{S}|\RV{H}} = \tikzfig{de_finetti_conditional}
% \end{align}

% That is, there is some hypothesis $\RV{H}$ and conditional on $\RV{H}$ the measurements are independent and identically distributed. The proof of this is constructive -- $\RV{H}$ is a function of $\RV{S}$.



% \begin{align}
%     \prob{P}^{\RV{Y}|\RV{HD}} = \tikzfig{do_model_representation}
% \end{align}

% We will further argue that the class of see-do models considered in CBN and potential outcomes literature is equivalent to the family of causally contractible and exchangeable do-models where the decision rule for the first $n$ places is fixed to an unknown value, and may be freely chosen thereafter.

% \begin{theorem}[Existence of conditional in do models]
% Given a do model $(\prob{P}_{\square}^{\RV{Y}\|\RV{D}},R)$, for all $\alpha\in R$, $n\in\mathbb{N}$
% \begin{align}
%     \prob{P}_\alpha^{\RV{Y}_{[n]}\RV{D}_i} = \prob{P}_\alpha^{\RV{D}_{[n]}}\odot \prob{P}_\square^{\RV{Y}_{[n]}\|\RV{D}_{[n]}}
% \end{align}
% That is, $\prob{P}_\square^{\RV{Y}_{[n]}\|\RV{D}_{[n]}}\cong \prob{P}_\square^{\RV{Y}_{[n]}|\RV{D}_{[n]}}$
% \end{theorem}

% \begin{proof}
% For any $n>1\in \mathbb{N}$, $\alpha\in R$

% \begin{align}
%     \prob{P}_\alpha^{\RV{Y}_{[n]}\RV{D}_{[n]}} &= \tikzfig{do_model_1}\\
%     &= \tikzfig{do_model_2}\\
%     &= \tikzfig{do_model_3}\\
%     &= \tikzfig{do_model_4}\\
%     \implies \prob{P}_\alpha^{\RV{Y}_{[n]}|\RV{D}_{[n]}} &= \tikzfig{do_model_5}\\
%     &= \prob{P}_\alpha^{\RV{Y}_{[n-1]}|\RV{D}_{[n-1]}}\combprod \prob{P}_\square^{\RV{Y}_n|\RV{Y}_{[n-1]}\RV{D}_n}
% \end{align}

% Applying this recursively with $\prob{P}_\alpha^{\RV{Y}_{[1]}|\RV{D}_{[1]}}=\prob{P}_\square^{\RV{Y}_{[1]}|\RV{D}_{[1]}}$ yields

% \begin{align}
%     \prob{P}_\alpha^{\RV{Y}_{[n]}|\RV{D}_{[n]}} = \prob{P}_\square^{\RV{Y}_{[n]}\|\RV{D}_{[n]}}
% \end{align}
% as desired.
% \end{proof}
\subsection{Consequence contractibility}

Here we set out formal definitions of exchange commutativity and locality of consequences, as well as ``consequence contractibility'', which is the conjunction of both conditions.

\begin{definition}[Swap map]
Given $M\subset \mathbb{N}$ a finite permutation $\rho:M\to M$ and a variable $\RV{X}:\Omega\to X^M$ such that $\RV{X}=(\RV{X}_i)_{i\in M}$, define the Markov kernel $\text{swap}_{\rho(\RV{X})}:X^M\kto X^M$ by $(d_i)_{i\in\mathbb{N}}\mapsto \delta_{(d_{\rho(i)})_{i\in\mathbb{N}}}$.
\end{definition}

\begin{definition}[Exchange commutativity]\label{def:caus_exch}
Suppose we have a sample space $(\Omega,\sigalg{F})$ and a probability set $\prob{P}_C$ with uniform conditional probability $\prob{P}_C^{\RV{Y}|\RV{D}}$ where $\RV{Y}:=\RV{Y}:=(\RV{Y}_i)_{M}$, $\RV{D}:=\RV{D}_M:=(\RV{D}_i)_M$, $M\subseteq \mathbb{N}$. If for any finite permutation $\rho:M\to M$
\begin{align}
    \text{swap}_{\rho(\RV{D})} \prob{P}_{C}^{\RV{Y}|\RV{D}} &\overset{\prob{P}_C}{\cong} \prob{P}_{C}^{\RV{Y}|\RV{D}}\text{swap}_{\rho(\RV{Y})}
\end{align}
Then $\prob{P}_C^{\RV{Y}|\RV{D}}$ \emph{commutes with exchange}.
\end{definition}

If $\prob{P}_C^{\RV{Y}|\RV{D}}$ commutes with exchange and we have $\alpha,\alpha'\in C$ such that $\prob{P}_\alpha^{\RV{C}} = \prob{P}_{\alpha'}^{\RV{C}}\text{swap}_{\rho(\RV{D})}$, then $\prob{P}_\alpha^{\RV{Y}} = \prob{P}_{\alpha'}^{\RV{Y}}\text{swap}_{\rho(\RV{Y})}$. However, $\prob{P}_C^{\RV{Y}|\RV{D}}$ may commute with exchange even if there are no such $\alpha$ and $\alpha'\in C$.

\begin{definition}[Locality of consequences]\label{def:caus_cont}
Suppose we have a sample space $(\Omega,\sigalg{F})$ nd a probability set $\prob{P}_C$ with uniform conditional probability $\prob{P}_C^{\RV{Y}|\RV{D}}$ where $\RV{Y}:=\RV{Y}:=(\RV{Y}_i)_{M}$, $\RV{D}:=\RV{D}_M:=(\RV{D}_i)_M$, $M\subseteq \mathbb{N}$. If for any $A\subset M$
\begin{align}
    \prob{P}_S^{\RV{Y}_A|\RV{D}_M} &\overset{\prob{P}_C}{\cong} \tikzfig{consequence_locality}
\end{align}
then $\prob{P}_C^{\RV{Y}|\RV{D}}$ exhibits \emph{consequence locality}.
\end{definition}

If $\prob{P}_C^{\RV{Y}|\RV{D}}$ exhibits consequence locality then,given two different choices $\alpha$ and $\alpha'$ such that $\prob{P}_\alpha^{\RV{D}_A}=\prob{P}_{\alpha'}^{\RV{D}_A}$ then $\prob{P}_\alpha^{\RV{Y}_A}=\prob{P}_{\alpha'}^{\RV{Y}_A}$. However, once again, $\prob{P}_C^{\RV{Y}|\RV{D}}$ may exhibit consequence locality even if no such pair of choices exists.

Neither condition implies the other. 

\begin{theorem}
Exchange commutativity does not imply locality of consequences or vise versa.
\end{theorem}

\begin{proof}
A conditional probability model that exhibits exchange commutativity but some choices have non-local consequences:

Suppose $D=Y=\{0,1\}$ and we have a probability set $\prob{P}_C$ with uniform conditional $\prob{P}_C^{\RV{Y}|\RV{D}}$, where $\RV{D}=(\RV{D}_1,\RV{D}_2)$, $\RV{Y}=(\RV{Y}_1,\RV{Y}_2)$.

Suppose the unique version of $\prob{P}_C^{\RV{Y}|\RV{D}}$ is
\begin{align}
    \prob{P}_C^{\RV{Y}|\RV{D}}(y_1,y_2|d_1,d_2) &= \llbracket (y_1,y_2)= (d_1+d_2,d_1+d_2) \rrbracket
\end{align}
then 
\begin{align}
    \prob{P}_C^{\RV{Y}_1|\RV{D}}(y_1|d_1,d_2) &= \llbracket y_1 = d_1+d_2 \rrbracket
\end{align}
and there is no function depending on $y_1$ and $d_1$ only that is equal to this. Thus $\prob{P}_C$ exhibits non-local consequences. 

However, taking $\rho$ to be the unique nontrivial swap $\{0,1\}\to \{0,1\}$
\begin{align}
    \text{swap}_{\rho(\RV{D})}\prob{P}_{C}^{\RV{Y}|\RV{D}}(y_1,y_2|d_1,d_2) &= \prob{P}_{C}^{\RV{Y}|\RV{D}}(y_1,y_2|d_2,d_1)\\
    &= \llbracket (y_1,y_2)= (d_2+d_1,d_2+d_1) \rrbracket\\
    &= \llbracket (y_1,y_2)= (d_1+d_2,d_1+d_2) \rrbracket\\
    &= \llbracket (y_2,y_1)= (d_1+d_2,d_1+d_2) \rrbracket\\
    &= \prob{P}_{C}^{\RV{Y}|\RV{D}}\text{swap}_{\rho(\RV{Y})}(y_1,y_2|d_1,d_2)
\end{align}
so $\prob{P}_\square$ commutes with exchange.

A conditional probability model that exhibits locality of consequences but does not commute with exchange follows. Suppose again $D=Y=\{0,1\}$ and we have a probability set $\prob{P}_C$ with uniform conditional $\prob{P}_C^{\RV{Y}|\RV{D}}$, where $\RV{D}=(\RV{D}_1,\RV{D}_2)$, $\RV{Y}=(\RV{Y}_1,\RV{Y}_2)$. This time, suppose the unique version of $\prob{P}_C^{\RV{Y}|\RV{D}}$ is
\begin{align}
    \prob{P}_C{\RV{Y}|\RV{D}}(y_1,y_2|d_1,d_2) &= \llbracket (y_1,y_2)= (0,1) \rrbracket
\end{align}

Then  If $\prob{P}_\alpha^{\RV{D}_S}=\prob{P}_\beta^{\RV{D}_S}$ for $S\subset\{0,1\}$ then:
\begin{align}
    \prob{P}_C^{\RV{Y}_1|\RV{D}}(y_1|d_1,d_2) &= \llbracket y_1= 0 \rrbracket\\
    &= \prob{P}_C^{\RV{Y}_1|\RV{D}_1}(y_1|d_1)\\
    \prob{P}_C^{\RV{Y}_2|\RV{D}}(y_2|d_1,d_2)&= \llbracket y_2= 1 \rrbracket\\
    &= \prob{P}_C^{\RV{Y}_2|\RV{D}_2}(y_2|d_2)
\end{align}
so $\prob{P}_C^{\RV{Y}|\RV{D}}$ exhibits consequence locality.

However, $\prob{P}_C$ does not commute with exchange.
\begin{align}
    \text{swap}_{\rho(\RV{D})} \prob{P}_C^{\RV{Y}|\RV{D}}(y_1,y_2|d_1,d_2) &= \prob{P}_C^{\RV{Y}|\RV{D}}(y_1,y_2|d_2,d_1)\\
    &=\llbracket (y_1,y_2)= (0,1) \rrbracket\\
    &\neq \llbracket (y_2,y_1)= (0,1) \rrbracket\\
    &= \prob{P}_C^{\RV{Y}|\RV{D}}\text{swap}_{\rho(\RV{D})}(y_1,y_2|d_1,d_2)
\end{align}
\end{proof}

Although locality of consequences has a lot in common with an assumption non-interference, it still allows for some models in which exhibit certain kinds of interference between actions and outcomes of different indices. For example: I have an experiment where I first flip a coin and record the results of this flip as the outcome of the first step of the experiment, but I can choose either to record this same outcome as the provisional result of the second step (this is the choice $\RV{D}_1=0$), or choose to flip a second coin and record the result of that as the provisional result of the second step of the experiment (this is the choice $\RV{D}_1=1$). At the second step, I may further choose to copy the provisional results ($\RV{D}_2=0$) or invert them ($\RV{D}_2=1$). Then

\begin{align}
    \prob{P}_S^{\RV{Y}_1|\RV{D}}(y_1|d_1,d_2) &= 0.5\\
    \prob{P}_S^{\RV{Y}_2|\RV{D}}(y_2|d_1,d_2) &= 0.5
\end{align}
\begin{itemize}
    \item The marginal distribution of both experiments in isolation is $\text{Bernoulli}(0.5)$ no matter what choices I make, so a model of this experiment would satisfies Definition \ref{def:caus_cont}
    \item Nevertheless, the choice for the first experiment affects the result of the second experiment
\end{itemize}

Note that this example would not satisfy exchange commutativity.

We call the conjunction of exchange commutativity and consequence locality \emph{causal contractibility}.

\begin{definition}[Causal contractibility]
A probability set $\prob{P}_S$ with uniform conditional $\prob{P}_S^{\RV{Y}|\RV{D}}$ is causally contractible if it is both exchange commutative and exhibits consequence locality.
\end{definition}

\begin{theorem}[Causal contractibility]
A probability set $\prob{P}_S$ with uniform conditional $\prob{P}_S^{\RV{Y}|\RV{D}}$ is causally contractible if and only if for any $A,B\subset M$ with $|A|=|B|$
\begin{align}
    \prob{P}_S^{\RV{Y}_A|\RV{D}_A} &\overset{\prob{P}_C}{\cong} \prob{P}_S^{\RV{Y}_B|\RV{D}_B}
\end{align}
\end{theorem}

\begin{proof}

\end{proof}

% \begin{proposition}[Representation of do-models that commute with exchange]
% Suppose we have a fundamental probability set $\Omega$ and a do model $(\prob{P},\RV{D},\RV{Y},R)$ such that $\RV{D}:=(\RV{D}_i)_{i\in \mathbb{N}}$ and $\RV{Y}:=(\RV{Y}_i)_{i\in\mathbb{N}}$ where $\prob{P}$ commutes with exchange and there is some $\alpha^*\in R$ such that $\prob{P}^{\alpha^*}\gg\prob{P}_\beta$ for all $\beta in R$. Then there exists a symmetric function $\RV{H}:(Y\times D)^\mathbb{N}\to H$ such that  $\prob{P}^{\RV{Y}|\RV{DH}}$ exists and $\RV{Y}_i\CI_{\prob{P}}(\RV{D}_j,\RV{Y}_j)_{j\in \mathbb{N}}\setminus \{i\}|\RV{H}\RV{D}_i$, or equivalently 
% \begin{align}
%     \prob{P}^{\RV{Y}} &= \tikzfig{do_model_representation}
% \end{align}
% \end{proposition}

% % \begin{lemma}[Contraction and independence]
% % Let $\RV{J}$, $\RV{K}$ and $\RV{L}$ be variables on $\Omega$ and $\prob{Q}\in \Delta(\Omega)$ a base measure such that $\prob{Q}^{\RV{JK}}=\prob{Q}^{\RV{JL}}$ and $\sigma{K}\subset \sigma{L}$. Then $\RV{J}\CI\RV{L}|\RV{K}$. 
% % \end{lemma}

% % \begin{proof}
% % From Lemma 1.3 in \citet{kallenberg_basic_2005}.
% % \end{proof}

% \begin{proof}
% If $\prob{P}$ commutes with exchange, then for any $\alpha\in R$ such that $\prob{P}_\alpha^{\RV{D}}$ is exchangeable then $\prob{P}_\alpha$ is also exchangeable. Then there exists $\RV{H}$ a symmetric function of $(\RV{Y}_i,\RV{D}_i)_{i\in\mathbb{N}}$ such that $\RV{Y}_i\CI_{\prob{P}}(\RV{D}_j,\RV{Y}_j)_{j\in \mathbb{N}}\setminus \{i\}|\RV{H}\RV{D}_i$. This is De Finetti's representation theorem, and many proofs exists, see for example \citep{kallenberg_basic_2005}.

% In particular, let 

% \begin{align}
%     \RV{H}:=A\times B\mapsto \lim_{n\to\infty} \frac{1}{n}\sum_{i\in n} \mathds{1}_{A\times B}((\RV{Y}_i, \RV{D}_i))
% \end{align}

% Then for all $\alpha\in R$,
% \begin{align}
%     \prob{P}_\alpha^{(\RV{Y}_i,\RV{D}_i)_{i\in\mathbb{N}}|\RV{H}}(A\times B|h) \overset{a.s.}{=} h(A\times B)\label{eq:given_h}
% \end{align}

% The proof that the limit exists and the above equality holds can again be found int \citep{kallenberg_basic_2005}.
% \end{proof}

\subsection{Repeatable response conditionals exist iff a model is consequence contractible}

The main result in this section is Theorem \ref{th:iid_rep} which shows that a conditional probability model $\prob{P}_\square$ is causally contractible if and only if it can be represented as the product of a distribution over hypotheses $\prob{P}_\square^{\RV{H}}$ and a collection of identical conditional probabilities $\prob{P}_\square^{\RV{Y}_1|\RV{D}_1\RV{H}}$. Note the hypothesis $\RV{H}$ that appears in this conditional; it can be given the interpretation of a random variable that expresses the ``true but initially unknown'' $\RV{Y}_1|\RV{D}_1$ conditional probability.

\begin{lemma}[Exchangeable randomness pushback]\label{th:table_rep}
Given a conditional probability model $(\prob{P}_\square^{\RV{Y}|\RV{D}},A)$ such that $\RV{D}:=(\RV{D}_i)_{i\in \mathbb{N}}$ and $\RV{Y}:=(\RV{Y}_i)_{i\in \mathbb{N}}$, $\prob{P}_\square$ is consequence contractible if and only if there exists a column exchangeable probability distribution $\mu^{\RV{Y}^D}\in \Delta(Y^{|D|\times \mathbb{N}})$ such that
\begin{align}
    \prob{P}_\square^{\RV{Y}|\RV{D}} &= \tikzfig{lookup_representation}\label{eq:lup_rep}\\
    &\iff\\
    \prob{P}_\square^{\RV{Y}|\RV{D}}(y|(d_i)_{i\in \mathbb{N}}) &= \mu^{\RV{Y}^D} \Pi_{(d_i i)_{i\in\mathbb{N}}}(y)
\end{align}
Where $\Pi_{(d_i i)_{i\in\mathbb{N}}}:Y^{|D|\times \mathbb{N}}\to Y^{\mathbb{N}}$ is the function that projects the $(d_i,i)$ indices for all $i\in \mathbb{N}$ and $\prob{F}_{\text{ev}}$ is the Markov kernel associated with the evaluation map
\begin{align}
    \text{ev}:D^\mathbb{N}\times Y^{D\times \mathbb{N}}&\to Y\\
    ((d_i)_\mathbb{N},(y_{ij})_{i\in D,j\in \mathbb{N}})&\mapsto (y_{d_i i})_{i\in \mathbb{N}}
\end{align}
\end{lemma}

\begin{proof}
Only if:
Consider the weaker conditional probability model $(\prob{P}_\square^{\RV{Y}|\RV{D}},A')$ where $A'\supset A$ contains all $\alpha$ such that $\prob{P}_\alpha^{\RV{D}}$ is deterministic. If the result holds for $A'$ then it holds also for $A$ which is a subset.

For all $d\in D$, abuse notation to say taht $\prob{P}_d$ is a probability set in $A'$ such that $\prob{P}_d^{\RV{D}}=\delta_d$. For any $\alpha\in A'$, we have

\begin{align}
    \prob{P}_\alpha^{\RV{DY}}(B\times C) &= \int_B \prob{P}_\square^{\RV{Y}|\RV{D}}(C|d)\prob{P}_\alpha^{\RV{D}}(\mathrm{d}d)\\
                                &= \int_B \int_D \prob{P}_\square^{\RV{Y}|\RV{D}}(C|d')\prob{P}_d^{\RV{D}}(\mathrm{d}d')\prob{P}_\alpha^{\RV{D}}(\mathrm{d}d)\\
                                &= \int_B \prob{P}_d^{\RV{Y}}(C)\prob{P}_\alpha^{\RV{D}}(\mathrm{d}d)
\end{align}

Thus $d\mapsto \prob{P}_d^{\RV{Y}}$ is a version of $\prob{P}_\square^{\RV{Y}|\RV{C}}$.

Choose $e:=(e_i)_{i\in\mathbb{N}}$ such that $e_{|D|i+j}$ is the $i$th element of $D$ for all $i,j\in \mathbb{N}$. Abusing notation, write $e$ also for the decision function that chooses $e$ deterministically.

Define
\begin{align}
    \mu^{\RV{Y}^D}((y_{ij})_{D\times \mathbb{N}}):=\prob{P}_e^{\RV{Y}}((y_{|D|i+j})_{i\in D, j\in \mathbb{N}})
\end{align}

Now consider any $d:=(d_i)_{i\in \mathbb{N}}\in D^{\mathbb{N}}$. By definition of $e$, $e_{|D|d_i + i}=d_i$ for any $i,j\in \mathbb{N}$.

Define
\begin{align}
    \prob{Q}:D\kto Y\\
    \prob{Q}:= \tikzfig{lookup_representation}
\end{align}

and consider some ordered sequence $A\subset \mathbb{N}$ and $B:= ((|D|d_i+i))_{i\in A}$. Note that $e_B:=(e_{|D|d_i +i})_{i\in B}=d_A=(d_i)_{i\in A}$. Then 

\begin{align}
    \sum_{y\in \RV{Y}^{-1}(y_A)} \prob{Q}(y|d) &= \sum_{y\in \RV{Y}^{-1}(y_A)} \mu^{(\RV{Y}^{D}_{d_ii})_{A}}(y) \\
    &= \sum_{y\in \RV{Y}^{-1}(y_A)} \prob{P}_e^{(\RV{Y}_{|D|d_i+i})_{A}}(y)\\
    &= \prob{P}_e^{\RV{Y}_{B}}(y_A)\\
    &= \prob{P}_{d}^{\RV{Y}_A}(y_A)&\text{by consequence contractibility}
\end{align}

Because this holds for all $A\subset\mathbb{N}$, by the Kolmogorov extension theorem

\begin{align}
    \prob{Q}(y|d) &= \prob{P}_d^{\RV{Y}}(y)
\end{align}

And so $\prob{Q}$ is also a version of $\prob{P}_\square^{\RV{Y}|\RV{C}}$.

Next we will show $\mu^{\RV{Y}^D}$ is exchangeable. Consider any subsequences $\RV{Y}^D_S$ and $\RV{Y}^D_T$ of $\RV{Y}^D$ with $|S|=|T|$. Let $\rho(S)$ be the ``expansion'' of the indices $S$, i.e. $\rho(S)=(|D|i+j)_{i\in S,j\in D}$. Then by construction of $e$, $e_{\rho(S)}=e_{\rho(T)}$ and therefore

\begin{align}
    \mu^{\RV{Y}^D}\Pi_S&= \prob{P}_e^{\RV{Y}_{\rho(S)}})\\
    &= \prob{P}_e^{\RV{Y}_{\rho(T)}})&\text{by contractibility of }\prob{P}_\square\text{ and the equality } e_{\rho(S)}=e_{\rho(T)}\\
    &= \mu^{\RV{Y}^D}\Pi_T
\end{align}


If:
Suppose 
\begin{align}
    \prob{P}_\square^{\RV{Y}|\RV{D}} &= \tikzfig{lookup_representation}
\end{align}

and consider any two deterministic decision functions $d,d'\in D^{\mathbb{N}}$ such that some subsequences are equal $d_S=d'_T$.

Let $\RV{Y}^{d_S}=(\RV{Y}_{d_i i})_{i\in S}$.

By definition,

\begin{align}
    \prob{P}_\square^{\RV{Y}_S|\RV{D}}(y_S|d) &= \sum_{y^D_S\in Y^{|D|\times |S|}}\mu^{\RV{Y}^D} \Pi_S(y^D_S)\prob{F}_{\text{ev}}(y_S|d,y^D_S)\\
    &= \sum_{y^D_S\in Y^{|D|\times |T|}}\prob{P}_\square^{\RV{Y}^D_T}(y^D_S)\prob{F}_{\text{ev}}(y_S|d,y^D_S)&\text{ by contractibility of }\mu^{\RV{Y}^D}\Pi_T \\
    &= \prob{P}_\square^{\RV{Y}_T|\RV{D}}(y_S|d)
\end{align}
\end{proof}

We have specialised notation for talking about marginals, conditionals, conditional independence and so forth. It is useful to use this notation to discuss properties of the representation in Equation \ref{eq:lup_rep}. Thus we adopt the convention that, given a consequence contractible model, the sample space $(\Omega,\sigalg{F})$ contains a variable $\RV{Y}^D:\Omega\to Y^{|D|\times\mathbb{N}}$ such that $\prob{P}_\square^{\RV{Y}^D}$ exists and is equal to $\mu^{\RV{Y}^D}$ in Lemma \ref{th:table_rep}.

As we pointed out, there are similarities between $\RV{Y}^D$ in a standard consequence contractible model and the potential outcomes variables in potential outcomes models. However, the $\RV{Y}^D$ in our models usually can't be interpreted as potential outcomes. For example, consider a series of bets on fair coinflips. Model the consequence $\RV{Y}_i$ as uniform on $\{0,1\}$ for any decision $\RV{D}_i$, for all $i$. Specifically, $D=Y=\{0,1\}$ and $\prob{P}_\alpha^{\RV{Y}_n}(y)=\prod_{i\in [n]} 0.5$ for all $n$, $y\in Y^n$, $\alpha\in R$. Then the construction of $\prob{P}^{\RV{Y}^D}$ following the method in Lemma \ref{th:table_rep} yields $\prob{P}^{Y^D_i}(y^D_i)=\prod_{j\in D} 0.5$ for all $y^D_i\in Y^D$. In this model $\RV{Y}^0_i$ and $\RV{Y}^1_i$ are independent and uniformly distributed. However, if we wanted $\RV{Y}^0_i$ to be interpretable as ``what would happen if I bet on outcome 0 on turn $i$'' and $\RV{Y}^1$ to represent ``what would happen if I bet on outcome 1 on turn $i$'', then we ought to have $\RV{Y}^0_i = 1-\RV{Y}^1_i$. 

Lemma \ref{th:table_rep} also does not establish that causal contractibility is necessary for the existence of a potential outcomes. A counterexample is a potential outcomes model with potential outcomes $\RV{Z}^D$ where the distribution $\prob{P}_\square^{\RV{Z}^D}$ exists and is not column exchangeable. Such a model is not consequence contractible.

The tabular distribution $\prob{P}_\square^{\RV{Y}^D}$ along with the evaluation function $\kernel{F}_{\text{ev}}$ is a randomness pushback of the conditional probability $\prob{P}_\square^{\RV{Y}|\RV{D}}$. Because $\prob{P}_\square^{\RV{Y}^D}$ is a column exchangeable probability distribution we can apply De Finetti's theorem to show $\prob{P}_\square^{\RV{Y}^D}$ is representable as a product of identical parallel copies of $\prob{P}_\square^{\RV{Y}_1^D|\RV{H}}$ and a common prior $\prob{P}_\square^{\RV{H}}$. This in turn can be used to show that $\prob{P}_\square^{\RV{Y}|\RV{D}}$ can be represented as a product of identical parallel copies of $\prob{P}_\square^{\RV{Y}_1|\RV{D}_1\RV{H}}$ and the same common prior $\prob{P}_\square^{\RV{H}}$. This is the main result: the copies of $\prob{P}_\square^{\RV{Y}_1|\RV{D}_1\RV{H}}$ are the repeatable response conditionals.

\begin{theorem}\label{th:iid_rep}
Suppose we have a sample space $(\Omega,\sigalg{F})$ and a conditional probability model $(\prob{P}^{\RV{Y}|\RV{D}}_\square,A)$ such that $\RV{D}:=(\RV{D}_i)_{i\in \mathbb{N}}$ and $\RV{Y}:=(\RV{Y}_i)_{i\in \mathbb{N}}$. $\prob{P}_\square$ is causally contractible if and only if there exists some $\RV{H}:\Omega\to H$ such that $\prob{P}^{\RV{H}}_\square$ and $\prob{P}_\square^{\RV{Y}_i|\RV{H}\RV{D}_i}$ exist for all $i\in \mathbb{N}$ and
\begin{align}
    \prob{P}_\square^{\RV{Y}|\RV{D}} &= \tikzfig{do_model_representation}\\
    &\iff\\
    \RV{Y}_i&\CI_{\prob{P}_\square} \RV{Y}_{\mathbb{N}\setminus i},\RV{D}_{\mathbb{N}\setminus i}|\RV{H}\RV{D}_i&\forall i\in \mathbb{N}\\
    \land \RV{H} \CI_{\prob{P}_\square} \RV{D}\\
    \land \prob{P}_\square^{\RV{Y}_i|\RV{H}\RV{D}_i} &= \prob{P}^{\RV{Y}_0|\RV{H}\RV{D}_0} & \forall i\in \mathbb{N}
\end{align}
Where $\Pi_{D,i}:D^\mathbb{N}\kto D$ is the $i$th projection map.
\end{theorem}

\begin{proof}
We make use of Lemma \ref{th:table_rep} to show that we can represent the conditional probability $\prob{P}_\square^{\RV{Y}|\RV{D}}$ as
\begin{align}
        \prob{P}_\square^{\RV{Y}|\RV{D}} &= \tikzfig{lookup_representation}\\
\end{align}

As a preliminary, we will show
\begin{align}
    \kernel{F}_{\mathrm{ev}} &= \tikzfig{lookup_rep_intermediate}\label{eq:ev_alternate_rep}
\end{align}

Where $\Pi_{Y^D,i}:Y^{D\times\mathbb{N}}\kto Y^D$ is the $i$th column projection map on $\RV{Y}^{D\times \mathbb{N}}$ and $\mathrm{ev}_{Y^D\times D}:Y^D\times D\to Y$ is the evaluation function
\begin{align}
    ((y_i)_{i\in D},d)\mapsto y_d
\end{align}

Recall that $\mathrm{ev}$ is the function

\begin{align}
    ((d_i)_\mathbb{N},(y_{ij})_{i\in D,j\in \mathbb{N}})&\mapsto (y_{d_i i})_{i\in \mathbb{N}}
\end{align}

By definition, for any $\{A_i\in\sigalg{Y}|i\in \mathbb{N}\}$

\begin{align}
    \kernel{F}_{\mathrm{ev}}(\prod_{i\in \mathbb{N}}A_i|(d_i)_\mathbb{N},(y_{ij})_{i\in D,j\in \mathbb{N}}) &= \delta_{(y_{d_i i})_{i\in \mathbb{N}}}(\prod_{i\in \mathbb{N}}A_i)\\
        &= \prod_{i\in \mathbb{N}} \delta_{y_{d_i i}} (A_i)\\
        &= \mathrm{copy}^{\mathbb{N}}\prod_{i\in \mathbb{N}}( \Pi_{D,i}\otimes \Pi_{Y,i})\kernel{F}_{\mathrm{ev}_{Y^D\times D}}
\end{align}

Which is what we wanted to show.

Only if:
With $\prob{P}^{\RV{Y}^D}_\square$ column exchangeable. That is, noting that $\RV{Y}^D=(\RV{Y}^D_i)_{i\in \mathbb{N}}$, the $\RV{Y}^D_i$ are exchangeable with respect to $\prob{P}^{\RV{Y}^D}_\square$. From \citet{kallenberg_basic_2005} we have a directing random measure $\RV{H}$ such that
\begin{align}
    \prob{P}_\square^{\RV{Y}^D|\RV{H}} &= \tikzfig{de_finetti_representation}\\
    &\iff\\
    \prob{P}_\square^{\RV{Y}^D|\RV{H}}(\prod_{i\in \mathbb{N}} A_i|h) &= \prod_{i\in \mathbb{N}} \prob{P}_\square^{\RV{Y}_0^D|\RV{H}}(A_i|h)
\end{align}

Furthermore, because $\RV{Y}$ is a deterministic function of $\RV{D}$ and $\RV{Y}^D$, $\RV{Y}\CI_{\prob{P}_\square} \RV{H}|(\RV{D},\RV{Y}^D)$ and by definition of $\RV{Y}^D$, $\RV{Y}^D\CI_{\prob{P}_\square}\RV{D}$ and so

\begin{align}
    \prob{P}_\square^{\RV{Y}|\RV{HD}} &= \prob{P}_\square^{\RV{Y}^D|\RV{HD}}\odot \prob{P}_\square^{\RV{Y}|\RV{Y}^D\RV{HD}}\\
                                      &= \tikzfig{Y_pushback_factorisation}
                                      &= \tikzfig{do_model_representation}
\end{align}

If:
By assumption
\begin{align}
    \prob{P}_\square^{\RV{Y}|\RV{D}}(\prod_{i\in \mathbb{N}} A_i|h,(d_i)_{i\in \mathbb{N}}) &= \int_H \prod_{i\in \mathbb{N}}\prob{P}_\square^{\RV{Y}_1|\RV{HD}_1}(A_i|h,d_i)\prob{P}_\square^{\RV{H}}(\mathrm{d}h)
\end{align}

Consider $\alpha,\alpha'$ such that $\prob{P}^{\RV{D}_M}_\alpha = \prob{P}^{\RV{D}_L}_{\alpha'}$ for $L,M\subset \mathbb{N}$ with $|M|=|L|$, both finite. Then

\begin{align}
    \prob{P}_\alpha^{\RV{Y}_M}(A) &= \int_{D^{\mathbb{N}}} \prob{P}_\alpha^{\RV{Y}_M|\RV{D}}(A|d)\prob{P}_\alpha^{\RV{D}}(\mathrm{d}d)\\
                                  &= \int_H\int_{D^{\mathbb{N}}} \prod_{i\in M}\prob{P}_\square^{\RV{Y}_1|\RV{HD}_1}(A_i|h,d_i)\prob{P}_\alpha^{\RV{D}}(\mathrm{d}d)\prob{P}_\square^{\RV{H}}(\mathrm{d}h)\\
                                  &= \int_H\int_{D^{|M|}} \prod_{i\in M}\prob{P}_\square^{\RV{Y}_1|\RV{HD}_1}(A_i|h,d_i)\prob{P}_\alpha^{\RV{D}_M}(\mathrm{d}d_M)\prob{P}_\square^{\RV{H}}(\mathrm{d}h)\\
                                  &= \int_H\int_{D^{|M|}} \prod_{i\in M}\prob{P}_\square^{\RV{Y}_1|\RV{HD}_1}(A_i|h,d_i)\prob{P}_{\alpha'}^{\RV{D}_N}(\mathrm{d}d_N)\prob{P}_\square^{\RV{H}}(\mathrm{d}h)\\
                                  &= \int_H\int_{D^{\mathbb{N}}} \prod_{i\in M}\prob{P}_\square^{\RV{Y}_1|\RV{HD}_1}(A_i|h,d_i)\prob{P}_{\alpha'}^{\RV{D}}(\mathrm{d}d)\prob{P}_\square^{\RV{H}}(\mathrm{d}h)\\
                                  &= \prob{P}_{\alpha'}^{\RV{Y}_M}(A)
\end{align}

\end{proof}

\subsection{Modelling different decision procedures}

We have a formal condition -- consequence contractibility -- that is equivalent to the existence of repeatable response conditionals. A conditional probability model of an experiment for which there is some ``fixed, unknown probabilistic causal effect'' of $\RV{X}_i$ on $\RV{Y}_i$ for all $i$ must be consequence contractible. This means that any subsequence of $\RV{X}_i$s and corresponding $\RV{Y}_i$s for which the $\RV{X}_i$s are matched must share exactly the same model. 

To understand whether such an assumption is justified, we can't look only at the model -- we have to consider the measurement procedure. It's not a mathematical fact that a conditional probability model should exhibit the independence $\RV{X}_i\CI_{\prob{P}_\square} \RV{Y}_i$. On the other hand, if the description of the measurement procedure supports the judgement that what the model says regarding $\proc{Y}_i$ should be the same before and after one learns the value yielded by $\proc{X}_i$, then it is appropriate to adopt a model in which $\RV{X}_i\CI_{\prob{P}_\square} \RV{Y}_i$.

We will reason about decision procedures in the following way:

\begin{itemize}
    \item Propose a decision procedure $\proc{S}$ and assume a conditional probability model $\prob{P}_\square$ associated with it
    \item Consider an alternative decision procedure $\proc{S}'$ and argue that a conditional probability model $\prob{P}_\square'$, related somehow to $\prob{P}_\square$, is appropriate to model it
    \item If, in addition, we accept that \emph{the same model} is appropriate for both $\proc{S}$ and $\proc{S}'$, then we have $\prob{P}_\square=\prob{P}_\square'$
\end{itemize}

An example of this reasoning is found in discussions of exchangeability, outside the setting of decision problems. Suppose we have a measurement procedure $\proc{S}$ and an observed variable $(\RV{Y}\circ\proc{S},\RV{Y})$ where $\RV{Y}=(\RV{Y}_i)_{i\in \mathbb{N}}$ for $\RV{Y}_i:\Omega\to Y$ -- i.e. $\proc{S}$ yields a countably infinite sequence of values from the set $Y$. Consider an alternative observed variable $(\RV{Y}'\circ\proc{S},\RV{Y}')$ where $\RV{Y}'=\text{swap}\circ\RV{Y}$. $\proc{Y}'$ is the procedure ``do whatever is done to measure $\proc{Y}$, then shuffle the results according to $\text{swap}$''.

$\proc{S}$ is modeled by some ordinary probability model $(\mu,\Omega,\sigalg{F})$, with marginal distribution $\mu^{\RV{Y}}$ of $\RV{Y}$. It may be the case that we think that the additional shuffle operation involved in $\proc{Y}'$ does not in any way alter the appropriate probability model associated with the variable. Thus $\mu^{\RV{Y}}=\mu^{\RV{Y}'}$, and the model $\mu$ is exchangeable with respect to $\RV{Y}$.

In the following section, we will apply this approach to considering whether the assumption of exchange commutativity is justified for a given decision procedure. The approach outlined can be used to assess whether measurement procedures support exchange commutativity or consequence locality. However, we will focus on exchange commutativity because, out of the two assumptions, we have more to say about it.

% As an aside, consequence contractibility means that a model $\prob{P}_\square$ must say precisely the same thing about any subsequence of (choice, outcome) pairs, provided both subsequences have a matching distribution over choices. Suppose there is some experiment in which some pairs arise from passive observations and others from active interventions. If we are to suppose that the whole sample supports ``fixed but unknown probabilistic causal effects'' -- that is, it is consequence contractible -- then we are equivalently claiming that a subsequence of $m+1$ $(\RV{X}_i,\RV{Y}_i)$ pairs generated only by active interventions is modeled identically to a sequence of $m$ pairs generated by passive observation and $1$ pair generated by active intervention. This suggests -- loosely speaking -- that assuming ``fixed but unknown probabilistic causal effects'' are identified from observations is the same as assuming that observational evidence is precisely as good as experimental evidence for learning said causal effects. This is an idle observation and not a rigorous argument.

\subsection{Decision procedures with response conditionals}

Suppose we have a decision procedure $\proc{S}_A$. We consider an exchange commutative model appropriate if we think that the experiment given by
\begin{itemize}
    \item Suppose $\alpha$ has been decided on
    \item Obtain a sequence of choices according to $\proc{D}$ (which depends on $\alpha$) and enact them
    \item Measure the consequences according to $\proc{Y}$
\end{itemize}
is equivalent to $\proc{S}'$ (in the sense of requiring the same model):
\begin{itemize}
    \item Decide on $\alpha'$ such that $\prob{P}_\alpha^{\RV{D}}\text{swap}=\prob{P}_{\alpha'}^{\RV{D}}$
    \item Obtain a sequence of choices according to $\proc{D}$ (which depends on $\alpha'$) and enact them
    \item Measure preliminary consequences according to $\proc{Y}$
    \item Apply the inverse shuffle $\text{swap}^{-1}$ to the preliminary consequences to get the consequences of interest $\proc{Y}'$
\end{itemize}

Unlike in the case of ordinary exchangeability, we need to consider two measurement procedures in which different decisions are made. Conditional probability models of decision problems express judgements that hold whatever decision is made. Under this interpretation, a conditional probability model $\prob{P}_\square^{\RV{Y}|\RV{D}}$ can tell us about the comparison of $\proc{S}$ and $\proc{S}'$ (see Section \ref{sec:cp_model}). In particular, it expresses the following attitude:

\begin{itemize}
    \item If we decide on $\alpha$, then the consequences are described by $\prob{P}_\alpha^{\RV{D}} \prob{P}_\square^{\RV{Y}|\RV{D}}$
    \item If we decide on $\alpha'$ such that $\prob{P}_{\alpha'}^{\RV{D}}=\prob{P}_\alpha^{\RV{D}}\text{swap}$ then then the consequences are described by $\prob{P}_\alpha^{\RV{D}}\text{swap}\prob{P}_\square^{\RV{Y}|\RV{D}}$
\end{itemize}

Thus if we hold that $\proc{S}$ is equivalent to $\proc{S}'$ for all $\alpha\in A$, we conclude that $\prob{P}_\square$ commutes with exchange.


There is a related kind of symmetry that involves shuffling ``experimental units''. This is not an operation that can be described purely mathematically, but there may be a more intuitively appealing case available that this kind of shuffle yields an equivalent experiment. For example, if an ``experimental unit'' is a patient who receives a treatment and whose recovery is then followed up on as a consequences, we might be willing to regard an experiment equivalent if the order of patients in the experiment is shuffled. We think it is sometimes reasonable to deduce exchange commutativity from this kind of symmetry, but as we will see care is needed when doing so.

\subsection{Example: exchange commutativity in the context of treatment choices}

Consider the following two scenarios:

\begin{enumerate}
    \item Dr Alice is going to see two patients who are both complaining of lower back pain. Prior to seeing either, she decides deterministically on $(\proc{D}_1,\proc{D}_2)$ for treatment decisions $\proc{D}_1$ and $\proc{D}_2$
    \item As before, but $\proc{D}_1$ is chosen after examining patient 1, and $\proc{D}_2$ after examining patient 2
\end{enumerate}

Alice could model either situation with a conditional probability model $(\prob{P}_\square^{\RV{Y}_1\RV{Y}_2|\RV{D}_1\RV{D}_2},A)$. In either situation, might the model be exchange commutative?

For each scenario, we want to consider two measurement procedures: first, the ``original'' measurement procedure, and then the measurement procedure with a swapped choices and consequences of interest. The question we want to consider is whether both procedures should be considered equivalent.

We will make two assumptions about measurement procedure equivalence: first, a measurement procedure is equivalent to an identical procedure in which patients are interchanged. Second, a measurement procedure is equivalent to an identical procedure in which the order of treatment and measurement of outcomes is interchanged.

We will describe measurement procedures using pseudocode, because this offers the opportunity to be precise about operations like swaps. Note that descriptions of measurement procedures, in pseudocode or otherwise, are incomplete descriptions.

Suppose the first scenario corresponds to the following procedure $\proc{S}$.
\begin{algorithmic}
    \Procedure{$\proc{S}$}{}
    \State $(\proc{D}_1,\proc{D}_2) \gets \mathrm{choose\_treatments}$
    \State $\proc{Y}_1\gets \mathrm{apply}(\proc{D}_1,\mathrm{patient\;A})$
    \State $\proc{Y}_2\gets \mathrm{apply}(\proc{D}_2,\mathrm{patient\;B})$
    \State \Return $(\alpha,\proc{D}_1,\proc{D}_2,\proc{Y}_1,\proc{Y}_2)$
    \EndProcedure
\end{algorithmic}

Our assumption of patient interchangeability means that the following procedure is equivalent

\begin{algorithmic}
    \Procedure{$\proc{S}$}{}
    \State $(\proc{D}_1,\proc{D}_2) \gets \mathrm{choose\_treatments}$
    \State $\proc{Y}_1\gets \mathrm{apply}(\proc{D}_1,\mathrm{patient\;B})$
    \State $\proc{Y}_2\gets \mathrm{apply}(\proc{D}_2,\mathrm{patient\;A})$
    \State \Return $(\alpha,\proc{D}_1,\proc{D}_2,\proc{Y}_1,\proc{Y}_2)$
    \EndProcedure
\end{algorithmic}

Consider the swapped procedure

\begin{algorithmic}
    \Procedure{$\proc{S}$}{}
    \State $(\proc{D}_1,\proc{D}_2) \gets \mathrm{choose\_treatments}$
    \State $\proc{Y}_1\gets \mathrm{apply}(\proc{D}_1,\mathrm{patient\;A})$
    \State $\proc{Y}_2\gets \mathrm{apply}(\proc{D}_2,\mathrm{patient\;B})$
    \State \Return $(\alpha,\proc{D}_1,\proc{D}_2,\proc{Y}_1,\proc{Y}_2)$
    \EndProcedure
\end{algorithmic}


Make the assumption that, on the basis that the patients are indistinguishable to Alice at the time of model construction, the same model is appropriate for the original measurement procedure and a modified measurement procedure in which the patients are swapped (we say the measurement procedures are ``equivalent''). Assume also that swapping the order of treatment and swapping the order in which outcomes are recorded yields an equivalent measurment procedure (in \citet{walley_statistical_1991}'s language, the first assumption is based on ``symmetry of evidence'' and the second on ``evidence of symmetry''). Putting these two assumptions together, the following procedure $\proc{S}'$ is equivalent to the original:

\begin{algorithmic}
    \Procedure{$\proc{S}'$}{}
    \Assert{patient A knowledge=patient B knowledge}
    \State $\alpha \gets \mathrm{choose\_alpha}$
    \State $(\proc{D}_1,\proc{D}_2) \gets \mathrm{decisions}(\alpha)$
    \State $\proc{Y}_2\gets \mathrm{apply}(\proc{D}_2,\mathrm{patient\;A})$
    \State $\proc{Y}_1\gets \mathrm{apply}(\proc{D}_1,\mathrm{patient\;B})$
    \State \Return $(\alpha,\proc{D}_1,\proc{D}_2,\proc{Y}_1,\proc{Y}_2)$
    \EndProcedure
\end{algorithmic}

Consider another measurement procedure $\proc{S}''$, which is a modified version of $\proc{S}$ where steps are added to swap decisions after they are chosen, then outcomes are swapped back once they have been observed:

\begin{algorithmic}
    \Procedure{$\proc{S}''$}{}
    \Assert{patient A knowledge=patient B knowledge}
    \State $\alpha \gets \mathrm{choose\_alpha}$
    \State $(\proc{D}_1,\proc{D}_2) \gets \mathrm{decisions}(\alpha)$
    \State $(\proc{D}_1^{\mathrm{swap}},\proc{D}_2^{\mathrm{swap}}) \gets (\proc{D}_2,\proc{D}_1)$
    \State $\proc{Y}^{\mathrm{swap}}_1\gets \mathrm{apply}(\proc{D}_1^{\mathrm{swap}},\mathrm{patient A})$
    \State $\proc{Y}_2^{\mathrm{swap}}\gets \mathrm{apply}(\proc{D}_2^{\text{swap}},\text{patient B})$
    \State $(\proc{Y}_1,\proc{Y}_2)\gets (\proc{Y}^{\mathrm{swap}}_2,\proc{Y}^{\mathrm{swap}}_1)$
    \State \Return $(\alpha,\proc{D}_1,\proc{D}_2,\proc{Y}_1,\proc{Y}_2)$
    \EndProcedure
\end{algorithmic}

Instead of explicitly performing the swaps, we can substitute $\proc{D}_2$ for $\proc{D}_1^{\text{swap}}$, $\proc{Y}_2$ for $\proc{Y}_1^{\text{swap}}$ and so on. The result is a procedure identical to $\proc{S}'$

\begin{algorithmic}
    \Procedure{$\proc{S}''$}{}
    \Assert{patient A knowledge=patient B knowledge}
    \State $\alpha \gets \mathrm{choose\_alpha}$
    \State $(\proc{D}_1,\proc{D}_2) \gets \mathrm{decisions}(\alpha)$
    \State $\proc{Y}_2\gets \mathrm{apply}(\proc{D}_2,\mathrm{patient\;A})$
    \State $\proc{Y}_1\gets \mathrm{apply}(\proc{D}_1,\mathrm{patient\;B})$
    \State \Return $(\alpha,\proc{D}_1,\proc{D}_2,\proc{Y}_1,\proc{Y}_2)$
    \EndProcedure
\end{algorithmic}

Thus $\proc{S}''$ is exactly the same as $\proc{S}'$, which by assumption is equivalent to the original $\proc{S}$, and so the assumptions of interchangeable patients and reversible order of treatment application imply the model should commute with exchange. Thus, if we could extend this example to an infinite sequence of patients, there would exist a Markov kernel $\prob{P}_\square^{\RV{Y}|\RV{DH}}:D\times H\kto Y$ representing a ``definite but unknown causal consequence'' shared by all experimental units.

This argument does \emph{not} hold for scenario 2. In the absence of a deterministic function $\text{decisions}(\alpha)$ which defines the procedure for obtaining $\proc{D}_1$ and $\proc{D}_2$, there is some flexibility for how exactly these variables are measured (or chosen). In particular, we can posit measurement procedures such that permuting patients is not equivalent to permuting decisions and then appying the reverse permutation to outcomes.

For example, procedure $\proc{T}$ is compatible with scenario 2 (note that there are many procedures compatible with the given description)

\begin{algorithmic}
    \Procedure{$\proc{T}$}{}
    \Assert{patient A knowledge=patient B knowledge}
    \State $\alpha \gets \mathrm{choose\_alpha}$
    \State patient A knowledge$\gets \mathrm{inspect}$(patient A)
    \State patient B knowledge$\gets \mathrm{inspect}$(patient B)
    \State $(\proc{D}_1,\proc{D}_2) \gets \mathrm{vagueDecisions}(\alpha$, patient A knowledge, patient B knowledge)
    \State $\proc{Y}_1\gets \mathrm{apply}(\proc{D}_1,\mathrm{patient\;A})$
    \State $\proc{Y}_2\gets \mathrm{apply}(\proc{D}_2,\mathrm{patient\;B})$
    \State \Return $(\alpha,\proc{D}_1,\proc{D}_2,\proc{Y}_1,\proc{Y}_2)$
    \EndProcedure
\end{algorithmic}

Permutation of patients and treatment order now yields

\begin{algorithmic}
    \Procedure{$\proc{T}'$}{}
    \Assert{patient A knowledge=patient B knowledge}
    \State $\alpha \gets \mathrm{choose\_alpha}$
    \State patient B knowledge$\gets \mathrm{inspect}$(patient B)
    \State patient A knowledge$\gets \mathrm{inspect}$(patient A)
    \State $(\proc{D}_1,\proc{D}_2) \gets \mathrm{vagueDecisions}(\alpha$, patient B knowledge, patient A knowledge)
    \State $\proc{Y}_2\gets \mathrm{apply}(\proc{D}_2,\mathrm{patient\;A})$
    \State $\proc{Y}_1\gets \mathrm{apply}(\proc{D}_1,\mathrm{patient\;B})$
    \State \Return $(\alpha,\proc{D}_1,\proc{D}_2,\proc{Y}_1,\proc{Y}_2)$
    \EndProcedure
\end{algorithmic}

While paired permuation of decisions and outcomes yields

\begin{algorithmic}
    \Procedure{$\proc{T}''$}{}
    \Assert{patient A knowledge=patient B knowledge}
    \State $\alpha \gets \mathrm{choose\_alpha}$
    \State patient A knowledge$\gets \mathrm{inspect}$(patient A)
    \State patient B knowledge$\gets \mathrm{inspect}$(patient B)
    \State $(\proc{D}_1,\proc{D}_2) \gets \mathrm{vagueDecisions}(\alpha$, patient A knowledge, patient B knowledge)
    \State $\proc{Y}_2\gets \mathrm{apply}(\proc{D}_2,\mathrm{patient\;A})$
    \State $\proc{Y}_1\gets \mathrm{apply}(\proc{D}_1,\mathrm{patient\;B})$
    \State \Return $(\alpha,\proc{D}_1,\proc{D}_2,\proc{Y}_1,\proc{Y}_2)$
    \EndProcedure
\end{algorithmic}

$\proc{T}'$ is not the same as $\proc{T}''$. In scenario 1, because decisions were deterministic on $\alpha$, there was no room to pick anything different once $\alpha$ was chosen, so it doesn't matter if we add patient inspection steps or not. In scenario 2, decisions are not deterministic and there is vagueness in the procedure, so it is possible to describe compatible procedures where decisions depend on patient characteristics, and this dependence is not ``undone'' by swapping decisions.

\todo[inline]{I've started but not finished revising the previous}

\subsection{Causal consequences of non-deterministic variables}

In the previous section we gave an example of how commutativity of exchange can hold when we have a sequence of decisions such that we accept the follwing:

\begin{itemize}
    \item Reordering the time at which decisions are made is held to be of no consequence
    \item The available information relevant to each decision is symmetric at the time the decision function is adopted
    \item The decision function deterministically prescribes which decisions are taken
\end{itemize}

We also discussed how the absence of determinism undermines the argument for exchange commutativity.

The determinism assumption rules out choosing decisions randomly. However, if we have response conditionals with a particular conditioning variable, response conditionals for other conditioning variables may exist if a certain conditional independence that we refer to as \emph{proxy control} holds. That is, if we have a response conditional for $(\RV{X},\RV{Y})$ given $\RV{D}$, $\RV{D}$ is deterministic for all choices and $\RV{Y}$ is independent of $\RV{D}$ given $\RV{X}$, then we also have a response conditional for $\RV{Y}$ given $\RV{X}$ and $\RV{X}$ may not be deterministic.

We also show that proxy control is necessary for the existence of additional response conditionals if $\RV{D}$ is deterministically controllable; that is, if it can be forced to take on any deterministic probability distribution. If the judgements underpinning the existence of response conditionals ultimately rest on decision variables that are deterministic for each choice that can be made, and we claim that a response conditional for $\RV{Y}$ given $\RV{X}$ exists where $\RV{X}$ is just some not-necessarily-deterministic variable, then $\RV{X}$ must be a proxy for controlling $\RV{Y}$ given $\RV{D}$.

\begin{definition}[Deterministically controllable]
Given a probabilty gap model $(\prob{P}_\square,\{\prob{P}_{\tilde{\alpha}}^{\RV{D}}\}_A,f)$ on $(\Omega,\sigalg{F})$ and a variable $\RV{X}:\Omega\to X$, if for any $x\in X$ there exists $\alpha\in A$ such that $\prob{P}_\alpha^{\RV{X}}=\delta_x$ then $\RV{X}$ is determinstically controllable.
\end{definition}

\begin{theorem}\label{lem:proxy_control}
Given $(\prob{P}_\square^{\RV{XY}|\RV{D}[M]},\{\prob{P}_{\tilde{\alpha}}^{\RV{D}}\}_A,f)$ with decisions $\RV{D}_M$ and consequences $\RV{Y}_M$, $\RV{X}_M$, if $\prob{P}_\square^{\RV{Y}_M\RV{X}_M|\RV{D}_M}$ is causally contractible with response conditional $\prob{P}_\square^{\RV{Y}_0\RV{X}_0|\RV{D}_0\RV{H}}$ such that $\RV{Y}_i\CI_{\prob{P}_\square} \RV{D}_i|\RV{HX}_i$ for all $i\in M$, then a causally contractible conditional probability $\prob{P}_\square^{\RV{Y}_M|\RV{X}_M}$ exists. If $\RV{D}_M$ is deterministically controllable and $D$ countable, then $\RV{Y}_i\CI_{\prob{P}_\square} \RV{D}_i|\RV{HX}_i$ is also necessary for the existence of $\prob{P}_\square^{\RV{Y}_M|\RV{X}_M}$.
\end{theorem}

\begin{proof}
\textbf{Sufficiency:}
We want to show that $\RV{Y}_i\CI_{\prob{P}_\square} \RV{Y}_{\{i\}^C}\RV{X}_{\{i\}^C} |\RV{H}\RV{X}_i$ for all $i\in M$, that $\prob{P}_\square^{\RV{Y}_i|\RV{H}\RV{X}_i}$ exists for all $i\in M$ and that $\prob{P}_\square^{\RV{Y}_i|\RV{H}\RV{X}_i}=\prob{P}_\square^{\RV{Y}_j|\RV{H}\RV{X}_j}$.

From causal contractibility we have
\begin{align}
(\RV{X}_i,\RV{Y}_i)\CI_{\prob{P}_\square} (\RV{X}_{\{i\}^C},\RV{Y}_{\{i\}^C},\RV{D}_{\{i\}^C}) |\RV{H}\RV{D}_i\label{eq:cc}\\
\RV{Y}_i\CI_{\prob{P}_\square} (\RV{Y}_{\{i\}^C},\RV{X}_{\{i\}^C}) |\RV{H}\RV{D}_i\RV{X}_i\label{eq:wu}
\end{align}

Where Eq. \ref{eq:wu} follows from \ref{eq:cc} by weak union.

Thus by contraction, $\RV{Y}_i\CI_{\prob{P}_\square} \RV{Y}_{\{i\}^C}\RV{D}_{M} |\RV{H}\RV{X}_i$.

By Corollary \ref{cor:ci_cp_exist} and the existence of $\prob{P}^{\RV{Y}_i\RV{X}_i|\RV{H}\RV{D}_i}$ for all $i\in M$, $\prob{P}_\square^{\RV{Y}_i|\RV{H}\RV{X}_i}$ exists for all $i$. Furthermore, because $\prob{P}^{\RV{Y}_i\RV{X}_i|\RV{H}\RV{D}_i}=\prob{P}^{\RV{Y}_j\RV{X}_j|\RV{H}\RV{D}_j}$ for all $i,j\in M$, $\prob{P}_\square^{\RV{Y}_i|\RV{H}\RV{X}_i}=\prob{P}_\square^{\RV{Y}_j|\RV{H}\RV{X}_j}$ for all $i,j\in M$.
\textbf{Necessity:}
We will show for all $\alpha\in A$, $B\in \sigalg{Y}$, $(x,d,h)\in X\times D\times H$ that 
\begin{align}
    \prob{P}_\alpha^{\RV{Y}_0|\RV{X}_0\RV{D}_0\RV{H}}(B|x,d,h)=\prob{P}_\alpha^{\RV{Y}_0|\RV{X}_0\RV{H}}(B|x,h)
\end{align}

By assumption, we have the conditionals $\prob{P}_\square^{\RV{Y}_i\RV{X}_i|\RV{D}_i\RV{H}}$ and $\prob{P}_\square^{\RV{X}_i|\RV{H}\RV{D}_i}$ for all $i\in M$. We can conclude that $\prob{P}_\square^{\RV{Y}_i|\RV{X}_i\RV{D}_0i\RV{H}}$ also exists, as it is a higher order conditional with respect to $\prob{P}_\square^{\RV{Y}_i\RV{X}_i|\RV{D}_i\RV{H}}$.

For arbitrary $d\in D$, let $\alpha_d\in A$ be such that $\prob{P}_{\alpha_d}^{\RV{D}_i}=\delta_{d}$. For every version of $\prob{P}_{\alpha_d}^{\RV{Y}_i|\RV{X}_i\RV{D}_i\RV{H}}$ and $\prob{P}_{\alpha_d}^{\RV{Y}_i|\RV{X}_i\RV{D}_i\RV{H}}$
\begin{align}
    \prob{P}_{\alpha_d}^{\RV{Y}_i|\RV{X}_i\RV{H}}(B|x,h) &= \int_{D}\prob{P}_{\alpha_d}^{\RV{Y}_i|\RV{X}_i\RV{D}_i\RV{H}}(B|x,d',h)\delta_d(\mathrm{d}d')\\
                                                         &= \prob{P}_{\alpha_d}^{\RV{Y}_i|\RV{X}_i\RV{D}_i\RV{H}}(B|x,d,h)
\end{align}
For all $x\in X$, $h\in H$ $B\subset\sigalg{Y}$ except on a set of points $C\subset X\times H$ of uniform $\prob{P}_{\alpha_d}$ measure 0.
\todo[inline]{Need to add independence of hypothesis to representation theorem}
However, note that for any $\alpha$
\begin{align}
    \prob{P}_\alpha^{\RV{X}_i\RV{H}\RV{D}_i}(E\times F\times G) &= \sum_{d\in G} \prob{P}_\alpha^{\RV{D}_i}(d)\prob{P}_\square^{\RV{X}_i\RV{H}|\RV{D}_i}(E\times F|d)\\
                                                                  &= \sum_{d\in G} \prob{P}_\alpha^{\RV{D}_i}(d)\sum_{d'\in D}\prob{P}_\square^{\RV{X}_i\RV{H}|\RV{D}_i}(E\times F|d')\prob{P}_{\alpha_d}^{\RV{D}_i}(\{d'\}\\
                                                                  &= \sum_{d\in G} \prob{P}_\alpha^{\RV{D}_i}(d)\prob{P}_{\alpha_d}^{\RV{X}_i\RV{H}\RV{D}_i}(E\times F\times\{d\})
\end{align}

Thus for each $d\in D$ the set $\{d\}\times C\subset D\times X\times H$ is of uniform $\prob{P}_\alpha$ measure 0 for any $\alpha\in A$. Because $\prob{P}_\square=\cup_{\alpha\in A}\prob{P}_\alpha$, it is also of uniform $\prob{P}_\square$ measure 0. Thus 

\begin{align}
    \prob{P}_{\square}^{\RV{Y}_0|\RV{X}_0\RV{H}}(B|x,h) &= \prob{P}_{\square}^{\RV{Y}_0|\RV{X}_0\RV{D}_0\RV{H}}(B|x,d,h)
\end{align}
as desired.
\end{proof}

As an example of this, suppose $\RV{X}:\Omega\to X$ is a source of random numbers, the set of decisions $D$ is a set of functions $X\to T$ for treatments $\RV{T}:\Omega\to T$ and $\RV{W}:\Omega\to W$ are the ultimate patient outcomes, with $\RV{Y}_i=(\RV{W}_i,\RV{T}_i)$. Then it may be reasonable to assume that $\RV{W}_i\CI(\RV{D}_i,\RV{X}_i)|\RV{T}_i\RV{H}$ (where conditioning on $\RV{H}$ can be thought of as saying that this independence holds under infinite sample size). In this case, $\RV{T}_i$ is a proxy for controlling $\RV{Y}_i$, and there exists a causal consequence $\prob{P}_\square^{\RV{Y}_0|\RV{T}_0\RV{H}}$.

A ``causal consequence of body mass index'' is unlikely to exist on the basis of symmetric information and deterministic decisions because there are no actions available to set body mass index deterministically. However, given an underlying problem where we have symmetric information over a collection of patients and some kind of decision that can be made deterministically, causal consequences of body mass index may exist if body mass index is a proxy for controlling the outcomes of interest.

\subsection{Body mass index revisited}

We return briefly to consider the question: given some collection of people indexed by $M$, with body mass index $\RV{B}_i$ and health outcomes of interest $\RV{Y}_i$ and some choices $\RV{D}_i$ a decision maker is contemplating relevant to these characteristics, suppose we have a conditional probability model $(\prob{P}_\square^{\RV{BY}|\RV{D}[M]},\{\prob{P}_{\tilde{\alpha}}^{\RV{D}}\}_A,f)$ causally contractible with respect to $(\RV{D},\RV{Y})$ (for example, perhaps decision maker is contemplating a treatment plan to apply to every individual).

Do response conditionals $\prob{P}_\square^{\RV{Y}_i|\RV{B}_i}$ exist? We have by Lemma \ref{lem:proxy_control} that this exists if and only if $\RV{Y}_i\CI_{\prob{P}_\square} \RV{D}_i|\RV{B}_i$. Thus we have reduced the question of the existence of response conditionals for BMI (or ``causal effects'' of BMI) to an empirical question. We might guess this is unlikely to hold; not only are there multiple ways we could imagine affecting a person's BMI with possibly different health implications, but it seems unlikely that the ultimate health outcome someone experiences can be predicted from BMI alone.

However, there might be something to be said for a ``causal effect of BMI''. In particular, while it seems unlikely that BMI is a precise proxy for controlling health outcomes, it seems to at least be a reasonable empirical question to ask if BMI is an \emph{approximate} proxy for health outcomes.

\todo[inline]{Do I prove a theorem about approximate proxy control?}

\subsection{Response consitionals}

If a decision problem model $\prob{P}_\square$ has a uniform conditional probability $\prob{P}_\square^{\RV{Y}|\RV{X}}$ but no uniform marginal $\prob{P}_\square^{\RV{X}}$, we could think of it a situation where $\RV{X}$ is directly controllable and $\RV{Y}$ is controllable via choosing the distribution of $\RV{X}$. Most statistical inference, including statistical causal inference, is done in a setting where we don't just have a probability model and data, but where the probability model is in some sense repeatable. This includes settings where the data model is assumed to be independent and identically distributed, or exchangeable.



The motivating question we introduced at the beginning of this paper was ``when are potential outcomes well-defined?''. This is not written using the potential outcomes framework, so we cannot directly address this question. However, we can ask ``when do probability gap models feature response conditionals?''.

\subsection{Response conditionals and potential outcomes}\label{sec:curry}

There is a connection between the question of when a particular conditional probability exists and when potential outcomes are wel-defined. Given any Markov kernel there is an operation akin to function currying termed ``randomness pushback'' that represents the kernel as the product of a probability measure and a deterministic Markov kernel. We observe that potential outcomes models share a number of features in common with a Markov kernel represented using a randomness pushback.

Unlike function currying, there are many different randomness pushbacks that represent the same Markov kernel. The interpretation of potential outcomes models seems to require that exactly one of these possible pushbacks is a genuine potential outcomes model. Several works including \citet{dawid_causal_2000} and \citet{richardson2013single} have expressed the view that some of the degrees of freedom in potential outcomes models are superfluous; both propose that the degrees of freedom that can be tested using some idealised experiment are the degrees that should be kept.

Notably, the single world intervention graphs of \citet{richardson2013single} feature an operation that splits an ``intervenable'' variable $\RV{X}$ into two versions, $\RV{X}$ and $\RV{X}'$, representing ``the actual $\RV{X}$'' and ``the unobseved value $\RV{X}$ would have taken absent intervention'' respectively (this interpretation is ours, not theirs). Thus Richardson and Robins might argue that they are interested in the existence of response conditionals of the form $\prob{P}_\square^{\RV{Y}|\RV{X}\RV{X}'}$ rather than $\prob{P}_{\square}^{\RV{Y}|\RV{X}}$. 

We do not take a position on which degrees of freedom are good or bad in a typical potential outcomes model, nor do we explore conditionals of the form $\prob{P}^{\RV{Y}|\RV{X}\RV{X}'}$. We can, however, distinguish two different questions:

\begin{itemize}
    \item How are response conditionals represented?
    \item Which response conditionals are of interest?
\end{itemize}

The first question seems to be an inconsequential stylistic matter, while the second question may determine the direction of one's analysis.

\subsection{Randomness pushbacks}

Given a function $f:X\times Y\to Z$, we can obtain a curried version $\lambda f:Y\to Z^X$. In particular, if $Y=\{*\}$ then $\lambda f:\{*\}\to Z^X$. At least for countable $X$, we can apply this construction to Markov kernels: given a kernel $\kernel{K}:X\kto Y$, define $\kernel{L}: \{*\}\kto Y^X$ by 
\begin{align}
    \kernel{L} ((y_i)_{i\in X}) &= \prod_{i\in X} \kernel{K}(y_i|i)
\end{align}

We can then define an evaluation map $\text{ev}:Y^X\times X\to Y$ by $\text{ev}((y_i)_{i\in X},x)=y_x$. Then

\begin{align}
    \kernel{K} &= \tikzfig{curry_kernel_definition} \label{eq:curry_identity}\\
    &\iff\\
    \kernel{K}(A|x) &= \int_{Y^X} \delta_{\text{ev}(y^X,x)}(A) \kernel{L}(\mathrm{d}y^X|x)
\end{align}

Unlike the case of function currying, $\kernel{L}$ is not the unique Markov kernel for which \ref{eq:curry_identity} holds. In fact, we can substitute any $\kernel{M}$ such that, for any $i\in X$

\begin{align}
    \sum_{y_{\{i\}^C}\in Y^{|X|-1}} \kernel{M}((y_i)_{i\in X}) = \kernel{K}(y_i|i)
\end{align}

This representation of a Markov kernel is called a \emph{randomness pushback} by \citet{fritz_synthetic_2020}. The idea is that the randomness in the orignal Markov kernel $\kernel{L}$ has been ``pushed back'' to $\kernel{L}$, which now passes through the deterministic Markov kernel $\kernel{F}_{\mathrm{ev}}$.

Randomness pushbacks have a few features in common with potential outcomes causal models. For our purposes, we will say a potential outcomes model is a probability set $(\Omega,\sigalg{F},\prob{P}_{\{\}})$ along with variables $\RV{X}$, $\RV{Y}$, $\RV{Y}^X$ such that 

\begin{align}
    \prob{P}_{\{\}}^{\RV{Y}|\RV{XY}^X} &= \kernel{F}_{\text{ev}}
\end{align}

More commonly, this property is expressed as

\begin{align}
    \RV{Y}\overset{a.s.}{=}\text{ev}(\RV{X},\RV{Y}^X)
\end{align}

We consider a potential outcomes model to be a probability set here, but we can formally recover a ``traditional'' potential outcomes model by considering probability sets of size $1$.

If we additionally have the existence of $\prob{P}_{\{\}}^{\RV{Y}^X|\RV{X}}$ and $\RV{Y}^\RV{X}\CI_{\prob{P}_{\{\}}} \RV{X}$ then 

\begin{align}
    \prob{P}_{\{\}}^{\RV{Y}|\RV{X}} &= \tikzfig{curry_kernel_copied}\label{eq:curry_identity_po}
\end{align}

Equation \ref{eq:curry_identity_po} is clearly a version of \ref{eq:curry_identity}. As we have established, provided $\prob{P}_{\{\}}^{\RV{Y}|\RV{X}}$ exists, we can always introduce some variable $\RV{Y}^X$ and corresponding $\prob{P}_{\{\}}^{\RV{Y}^X}$ such that Equation \ref{eq:curry_identity_po} holds.