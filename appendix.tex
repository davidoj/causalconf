%!TEX root = main.tex

\section{Appendix:see-do model representation}\label{sec:see-do-rep}

\todo[inline]{Update notation}

\begin{theorem}[See-do model representation]\label{th:see_do_rep}
Suppose we have a decision problem that provides us with an observation $x\in X$, and in response to this we can select any decision or stochastic mixture of decisions from a set $D$; that is we can choose a ``strategy'' as any Markov kernel $\kernel{S}:X\to \Delta(D)$. We have a utility function $u:Y\to \mathbb{R}$ that models preferences over the potential consequences of our choice. Furthermore, suppose that we maintain a denumerable set of hypotheses $H$, and under each hypothesis $h\in H$ we model the result of choosing some strategy $\kernel{S}$ as a joint probability over observations, decisions and consequences $\prob{P}_{h,\kernel{S}}\in \Delta(X\times D\times Y)$.

Define $\RV{X},\RV{Y}$ and $\RV{D}$ such that $\RV{X}_{xdy} = x$, $\RV{Y}_{xdy}=y$ and $\RV{D}_{xdy} = d$. Then making the following additional assumptions:
\begin{enumerate}
    \item Holding the hypothesis $h$ fixed the observations as have the same distribution under any strategy: $\prob{P}_{h,\kernel{S}}[\RV{X}]=\prob{P}_{h,\kernel{S}''}[\RV{X}]$ for all $h,\kernel{S},\kernel{S}'$ (observations are given ``before'' our strategy has any effect)
    \item The chosen strategy is a version of the conditional probability of decisions given observations: $\kernel{S}=\prob{P}_{h,\kernel{S}}[\RV{D}|\RV{X}]$
    \item There exists some strategy $\kernel{S}$ that is strictly positive
    \item For any $h\in H$ and any two strategies $\kernel{Q}$ and $\kernel{S}$, we can find versions of each disintegration such that $\prob{P}_{h,\kernel{Q}}[\RV{Y}|\RV{D}\RV{X}]=\prob{P}_{h,\kernel{S}}[\RV{Y}|\RV{D}\RV{X}]$ (our strategy tells us nothing about the consequences that we don't already know from the observations and decisions)
\end{enumerate}

Then there exists a unique see-do model $(\kernel{T},\RV{H}',\RV{D}',\RV{X}',\RV{Y}')$ such that $\prob{P}_{h,\kernel{S}}[\RV{XDY}]^{ijk} = \kernel{T}[\RV{X'}|\RV{H'}]_{h}^{i} \kernel{S}_i^j  \kernel{T}[\RV{Y'}|\RV{X'H'D'}]_{ijk}^k$.
\end{theorem}

\begin{proof}
Consider some probability $\prob{P}\in \Delta(X\times D\times Y)$. By the definition of disintegration (section \ref{ssec:disintegration}), we can write

\begin{align}
 \prob{P}[\RV{XDY}]^{ijk} = \prob{P}[\RV{X}]^i\prob{P}[\RV{D}|\RV{X}]_i^{j} \prob{P}[\RV{Y}|\RV{XD}]_{ij}^{k} \label{eq:disint}
\end{align}

Fix some $h\in H$ and some strictly positive strategy $\kernel{S}$ and define $\kernel{T}:H\times D\to \Delta(X\times Y)$ by
\begin{align}
    \kernel{T}_{hj}^{kl} &= \prob{P}_{h,\kernel{S}}[\RV{X}]^k \prob{P}_{h,\kernel{S}}[\RV{Y}|\RV{X}\RV{D}]^l_{kj} \label{eq:comb_disint}
\end{align}

Note that because $\kernel{S}$ is strictly positive and by assumption $\kernel{S}=\prob{P}_{h,\kernel{S}}[\RV{D}|\RV{X}]$, $\prob{P}_{h,\kernel{S}}[\RV{D}]$ is also strictly positive. Therefore $\prob{P}_{h,\kernel{S}}[\RV{Y}|\RV{D}]$ is unique and therefore $\kernel{T}$ is also unique.

Define $\RV{X}'$ and $\RV{Y}'$ by $\RV{X}'_{xy}=x$ and $\RV{Y}'_{xy}=y$. Define $\RV{H}'$ and $\RV{D}'$ by $\RV{H}'_{hd} = h$ and $\RV{D}'_{hd} = d$.

We then have
\begin{align}
    \kernel{T}[\RV{X'}|\RV{H'D'}]_{hj}^{k} &= \kernel{T}\underline{\RV{X}'}_{hj}^k\\
                                           &= \sum_l \kernel{T}_{hj}^{kl} \\
                                           &= \prob{P}_{h,\kernel{S}}[\RV{X}]^k\\
                                           &= \kernel{T}[\RV{X'}|\RV{H'D'}]_{hj'}^{k}
\end{align}

Thus $\RV{X}'\CI_{\kernel{T}} \RV{D}'|\RV{H}'$ and so $\kernel{T}[\RV{X}'|\RV{H}']$ exists (section \ref{ssec:cond_indep}) and $(\kernel{T},\RV{H}',\RV{D}',\RV{X}',\RV{Y}')$ is a see-do model.

Applying Equation \ref{eq:disint} to $\prob{P}_{h,\kernel{S}}$:

\begin{align}
    \prob{P}_{h,\kernel{S}}[\RV{XDY}]^{ijk} &= \prob{P}_{h,\kernel{S}}[\RV{X}]^i\prob{P}_{h,\kernel{S}}[\RV{D}|\RV{X}]_i^{j} \prob{P}_{h,\kernel{S}}[\RV{Y}|\RV{XD}]_{ij}^{k}\label{eq:t_is_comb_disint_start}\\
     &=  \prob{P}_{h,\kernel{S}}[\RV{X}]^i\prob{P}_{h,\kernel{S}}[\RV{Y}|\RV{XD}]_{ij}^{k}\\
     &= \prob{P}_{h,\kernel{S}}[\RV{D}|\RV{X}]_i^{j} \kernel{T}[\RV{X'Y'}|\RV{H'D'}]_{hj}^{ik}\\
     &= \kernel{S}_i^j \kernel{T}[\RV{X'Y'}|\RV{H'D'}]_{hj}^{ik}\\
     &= \kernel{S}_i^j \kernel{T}[\RV{X'}|\RV{H'D'}]_{hj}^{i} \kernel{T}[\RV{Y'}|\RV{X'H'D'}]_{ihj}^k\\
     &= \kernel{T}[\RV{X'}|\RV{H'}]_{h}^{i} \kernel{S}_i^j  \kernel{T}[\RV{Y'}|\RV{X'H'D'}]_{ihj}^k\label{eq:t_is_comb_disint_end}
\end{align}

Consider some arbitrary alternative strategy $\kernel{Q}$. By assumption

\begin{align}
    \prob{P}_{h,\kernel{S}}[\RV{X}]^{i} &= \prob{P}_{h,\kernel{Q}}[\RV{X}]^{i}\\
    \prob{P}_{h,\kernel{S}}[\RV{Y}|\RV{XD}]_{ij}^k &= \prob{P}_{h,\kernel{Q}}[\RV{Y}|\RV{XD}]_{ij}^k\text{ for some version of }\prob{P}_{h,\kernel{Q}}[\RV{Y}|\RV{XD}]
\end{align}

It follows that, for some version of $\prob{P}_{h,\kernel{Q}}[\RV{Y}|\RV{XD}]$,
\begin{align}
    \kernel{T}_{hj}^{kl} &= \prob{P}_{h,\kernel{Q}}[\RV{X}]^k \prob{P}_{h,\kernel{Q}}[\RV{Y}|\RV{X}\RV{D}]^l_{kj} \label{eq:comb_disint_nonuniq}
\end{align}

Then by substitution of $\kernel{Q}$ for $\kernel{S}$ in Equation \ref{eq:t_is_comb_disint_start} and working through the same steps

\begin{align}
    \prob{P}_{h,\kernel{S}}[\RV{XDY}]^{ijk} &= \kernel{T}[\RV{X'}|\RV{H'}]_{h}^{i} \kernel{Q}_i^j  \kernel{T}[\RV{Y'}|\RV{X'H'D'}]_{ihj}^k
\end{align}

As $\kernel{Q}$ was arbitrary, this holds for all strategies.
\end{proof}

\section{Appendix: Counterfactual representation}\label{sec:pot_rep}


\begin{definition}[Parallel potential outcomes]\label{def:pa_pot_outcomes}
Given a Markov kernel space $(\kernel{K},E,F)$, a collection of variables $\{\RV{Y}_i, \RV{Y}(W), \RV{W}_i\}$, $i\in [n]$, where $\RV{Y}_i$ and $\RV{Y}(W)$ are random variables and $\RV{W}_i$ could be either a state or random variables is a \emph{parallel potential outcome submodel} if $\kernel{K}[\RV{Y}_i|\RV{W}_i\RV{Y}(W)]$ exists and $\kernel{K}[\RV{Y}_i|\RV{W}_i\RV{Y}(W)]_{k j_1 j_2 ... j_{|W|}} = \delta[j_k]$.
\end{definition}

\todo[inline]{How this will change: a parallel potential outcomes model is a comb $\model{K}[\RV{Y}(W)|\RV{H}]\rightrightarrows \model{K}[\RV{Y}_i|\RV{W}_i\RV{Y}(W)]$.}

A parallel potential outcomes model features a sequence of $n$ ``parallel'' outcome variables $\RV{Y}_i$ and $n$ ``regime proposals'' $\RV{W}_i$, with the property that if the regime proposal $\RV{W}_i=w_i$ then the corresponding outcome $\RV{Y}_i\overset{a.s.}{=} \RV{Y}(w_i)$. We can identify a particular index, say $n=1$, with the actual world and the rest of the indices with supposed worlds. Thus $\RV{Y}_1$ represents the value of TYT in the actual world and $\RV{Y}_i$ $i\neq 1$ represents TYT under a supposed regime $\RV{W}_i$. Given such an interpretation, the fact that $\RV{Y}_i\overset{a.s.}{=} \RV{Y}(w_i)$ can be interpreted as assuming ``for all $w$, if the supposed regime $\RV{W}_i$ is $w$ then the corresponding outcome will be almost surely equal to $\RV{Y}(w)$, regardless of the value of the actual regime $\RV{W}_1$'', which is our original counterfactual assumption.

We do not intend to defend this as the only way that counterfactuals can be modeled, or even that it is appropriate to capture the idea of counterfactuals at all. It is simply a way that we can model the counterfactual assumption typically associated with potential outcomes. We will show show that parallel potential outcome submodels correspond precisely to \emph{extendably exchangeable} and \emph{deterministically reproducible} submodels of Markov kernel spaces.


\subsection{Parallel potential outcomes representation theorem}

Exchangeble sequences of random variables are sequences whose joint distribution is unchanged by permutation. Independent and identically distributed random variables are one example: if $\RV{X}_1$ is the result of the first flip of a coin that we know to be fair and $\RV{X}_2$ is the second flip then $\prob{P}[\RV{X}_1\RV{X}_2]=\prob{P}[\RV{X}_2\RV{X}_1]$. There are also many examples of exchangeable sequences that are not mutually independent and identically distributed -- for example, if we want to use random variables $\RV{Y}_1$ and $\RV{Y}_2$ to model our subjective uncertainty regarding two flips of a coin of unknown fairness, we regard our initial uncertainty for each flip to be equal $\prob{P}[\RV{Y}_1]=\prob{P}[\RV{Y}_2]$ and we our state of knowledge of the second flip after observing only the first will be the same as our state of knowledge of the first flip after observing only the second $\prob{P}[\RV{Y}_2|\RV{Y}_1]=\prob{P}[\RV{Y}_1|\RV{Y}_2]$, then our model of subjective uncertainty is exchangeable.

De Finetti's representation theorem establishes the fact that any infinite exchangeable sequence $\RV{Y}_1, \RV{Y}_2, ...$ can be modeled by the product of a \emph{prior} probability $\prob{P}[\RV{J}]$ with $\RV{J}$ taking values in the set of marginal probabilities $\Delta(Y)$ and a conditionally independent and identically distributed Markov kernel $\prob{P}[\RV{Y}_A|\RV{J}]_j^{y_A} = \prod_{i\in A} \prob{P}[\RV{Y}_1|\RV{J}]_j^{y_i}$.

We extend the idea of exchangeable sequences to cover both random variables and state variables, and we show that a similar representation theorem holds for potential outcomes. De Finetti's original theorem introduced the variable $\RV{J}$ that took values in the set of marginal distributions over a single observation; the set of potential outcome variables plays an analagous role taking values in the set of functions from propositions to outcomes.

The representation theorem for potential outcomes is somewhat simpler that De Finetti's original theorem due to the fact that potential outcomes are usually assumed to be \emph{deterministically reproducible}; in the parallel potential outcomes model, this means that for $j\neq i$, if $\RV{W}_j$ and $\RV{W}_i$ are equal then $\RV{Y}_j$ and $\RV{Y}_i$ will be almost surely equal. This assumption of determinism means that we can avoid appeal to a law of large numbers in the proof of our theorem.

\todo[inline]{An interesting question is whether there is a similar representation theorem for potential outcomes without the assumption of deterministic reproducibility. I'm reasonably confident that this is a straightforward corollary of the representation theorem proved in my thesis. However, this requires maths not introduced in this draft of the paper.}

Extendably exchangeable sequences can be permuted without changing their conditional probabilities, and can be extended to arbitrarily long sequences while maintaining this property. We consider here sequences that are exchangeable conditional on some variable; this corresponds to regular exchageability if the conditioning variable is $\stopper{0.2}$ where $\stopper{0.2}_i = 1$.

\begin{definition}[Exchangeability]\label{def:exchangeable}
Given a Markov kernel space $(\kernel{K},E,F)$, a sequence of variables $\left((\RV{D}_i,\RV{Y}_i)\right)_{i\in [n]}$ with $\RV{Y}_i$ random variables is \emph{exchangeable} conditional on $\RV{Z}$ if, defining $\RV{Y}_{[n]} = (\RV{Y}_i)_{i\in [n]}$ and $\RV{D}_{[n]}= (\RV{D}_i)_{i\in [n]}$, $\kernel{K}[\RV{Y}_{[n]}|\RV{D}_{[n]}\RV{Z}]$ exists and for any bijection $\pi:[n]\to [n]$ $\kernel{K}[\RV{Y}_{\pi([n])}|\RV{D}_{\pi([n])}\RV{Z}]=\kernel{K}[\RV{Y}_{[n]}|\RV{D}_{[n]}\RV{Z}]$.
\end{definition}

\begin{definition}[Extension]
Given a Markov kernel space $(\kernel{K},E,F)$, $(\kernel{K}',E',F')$ is an \emph{extension} of $(\kernel{K},E,F)$ if there is some random variable $\RV{X}$ and some state variable $\RV{U}$ such that $\kernel{K}'[\RV{X}|\RV{U}]$ exists and $\kernel{K}'[\RV{X}|\RV{U}]=\kernel{K}$.
\end{definition}

If $(\kernel{K}',E',F')$ is an extension of $(\kernel{K},E,F)$ we can identify any random variable $\RV{Y}$ on $(\kernel{K},E,F)$ with $ \RV{Y}\circ \RV{X}$ on $(\kernel{K}',E',F')$ and any state variable $\RV{D}$ with $\RV{D}\circ \RV{U}$ on $(\kernel{K}',E',F')$ and under this identification $\kernel{K}'[\RV{Y}\circ\RV{X}|\RV{D}\circ\RV{E}]$ exists iff $\kernel{K}[\RV{Y}|\RV{D}]$ exists and $\kernel{K}'[\RV{Y}\circ\RV{X}|\RV{D}\circ\RV{E}]=\kernel{K}[\RV{Y}|\RV{D}]$. To avoid proliferation of notation, if we propose $(\kernel{K},E,F)$ and later an extension $(\kernel{K}',E',F')$, we will redefine $\kernel{K}:=\kernel{K}'$ and $\RV{Y}:=\RV{Y}\circ \RV{X}$ and $\RV{D}:=\RV{D}\circ\RV{E}$.

\todo[inline]{I think this is a very standard thing to do -- propose some $\RV{X}$ and $\prob{P}(\RV{X})$ then introduce some random variable $\RV{Y}$ and $\prob{P}(\RV{X}\RV{Y})$ as if the sample space contained both $\RV{X}$ and $\RV{Y}$ all along.}

\begin{definition}[Extendably exchangeable]\label{def:ext_exchangeable}
Given a Markov kernel space $(\kernel{K},E,F)$, a sequence of variables $\left((\RV{D}_i,\RV{Y}_i)\right)_{i\in [n]}$ and a state variable $\RV{Z}$ with $\RV{Y}_i$ random variables is \emph{extendably exchangeable} if there exists an extension of $\kernel{K}$ with respect to which $\left((\RV{D}_i,\RV{Y}_i)\right)_{i\in \mathbb{N}}$ is exchangeable conditional on $\RV{Z}$.
\end{definition}

Here that we identify $\RV{Z}$ and $\left((\RV{D}_i,\RV{Y}_i)\right)_{i\in [n]}$ defined on the extension with the original variables defined on $(\kernel{K},E,F)$ while $\left((\RV{D}_i,\RV{Y}_i)\right)_{i\in \mathbb{N}\setminus[n]}$ may be defined only on the extension.

Deterministically reproducible sequences have the property that repeating the same decision gets the same response with probability 1. This could be a model of an experiment that exhibits no variation in results (e.g. every time I put green paint on the page, the page appears green), or an assumption about collections of ``what-ifs'' (e.g. if I went for a walk an hour ago, just as I actually did, then I definitely would have stubbed my toe, just like I actually did). Incidentally, many consider that this assumption is false concering what-if questions about things that exhibit quantum behaviour.

\begin{definition}[Deterministically reproducible]
Given a Markov kernel space $(\kernel{K},E,F)$, a sequence of variables $\left((\RV{D}_i,\RV{Y}_i)\right)_{i\in [n]}$ with $\RV{Y}_i$ random variables is \emph{deterministically reproducible} conditional on $\RV{Z}$ if $n\geq 2$, $\kernel{K}[\RV{Y}_{[n]}|\RV{D}_{[n]}\RV{Z}]$ exists and $\kernel{K}[\RV{Y}_{\{i,j\}}|\RV{D}_{\{i,j\}}\RV{Z}]_{kk}^{lm} = \llbracket l=m \rrbracket \kernel{K}[\RV{Y}_{i}|\RV{D}_{i}\RV{Z}]_{k}^{l}$ for all $i,j,k,l,m$.
\end{definition}

\begin{theorem}[Potential outcomes representation]\label{th:cfac_rep}
Given a Markov kernel space $(\kernel{K},E,F)$ along with a sequence of variables $\left((\RV{D}_i,\RV{Y}_i)\right)_{i\in [n]}$ with $n\geq 2$ and a conditioning variable $\RV{Z}$, $(\kernel{K},E,F)$ can be extended with a set of variables $\RV{Y}(D):=(\RV{Y}(i))_{i\in D}$ such that $\{\RV{Y}_i, \RV{Y}(D), \RV{D}_i\}$ is a parallel potential outcome submodel if and only if $\left((\RV{D}_i,\RV{Y}_i)\right)_{i\in [n]}$ is extendably exchangeable and deterministically reproducible conditional on $\RV{Z}$.
\end{theorem}

\begin{proof}
If:
Because $\left((\RV{D}_i,\RV{Y}_i)\right)_{i\in [n]}$ is extendably exchangeable, we can without loss of generality assume $n\geq |D|$.

Let $e=(e_i)_{i\in {[|D|]}}$. Introduce the variable $\RV{Y}(i)$ for $i\in D$ such that $\kernel{K}[\RV{Y}(D)|\RV{D}_{[D]}\RV{Z}]_{ez}=\kernel{K}[\RV{Y}_D|\RV{D}_D \RV{Z}]_{ez}$ and introduce $\RV{X}_i$, $i\in D$ such that $\kernel{K}[\RV{X}_i|\RV{D}_i\RV{Z}\RV{Y}(D)]_{e_izj_1...j_{|D|}}^{x_i} = \delta[j_{e_i}]^{x_i}$. Clearly $\{\RV{X}_{[n]},\RV{D}_{[n]},\RV{Y}(D)\}$ is a parallel potential outcome submodel. We aim to show that $\kernel{K}[\RV{Y}_{[n]}|\RV{D}_{[n]}\RV{Z}]=\kernel{K}[\RV{X}_{[n]}|\RV{D}_{[n]}\RV{Z}]$.

Let $y:=(y_i)_{i\in |D|}\in Y^{|D|}$, $d:=(d_i)_{i\in [n]}\in D^{[n]}$, $x:=(x_i)_{i\in [n]}\in Y^{[n]}$.
\begin{align}
    \kernel{K}[\RV{X}_n|\RV{D}_n\RV{Z}]^x_{dz} &= \sum_{y\in Y^{|D|}} \kernel{K}[\RV{X}_{[n]}|\RV{D}_n\RV{Z}\RV{Y}(D)]_{dzy}^x \kernel{K}[\RV{Y}(D)|\RV{D}_{[n]}\RV{Z}]_{dz}^y\\
                                               &= \sum_{y\in Y^{|D|}} \prod_{i\in [n]} \delta[y_{d_i}]^{x_i} \kernel{K}[\RV{Y}(D)|\RV{D}_n\RV{Z}]_{dz}^y
\end{align}

Wherever $d_i=d_j:=\alpha$, every term in the above expression will contain the product $\delta[\alpha]^{x_i}\delta[\alpha]^{x_j}$. If $x_i\neq x_j$, this will always be zero. By deterministic reproducibility, $d_i=d_j$ and $x_i\neq x_j$ implies $\kernel{K}[\RV{Y}_{[n]}|\RV{D}_{[n]}\RV{Z}]_dz^x=0$ also. We need to check for equality for sequences $x$ and $d$ such that wherever $d_i=d_j$, $x_i=x_j$. In this case, $\delta[\alpha]^{x_i}\delta[\alpha]^{x_j}=\delta[\alpha]^{x_i}$. Let $Q_d\subset[n]:=\{i|\not\exists i\in [n]: j<i \And d_j=d_i\}$, i.e. $Q$ is the set of all indices such that $d_i$ is the first time this value appears in $d$. Note that $Q_d$ is of size at most $|D|$. Let $Q_d^C=[n]\setminus Q_d$, let $R_d\subset D:\{d_i|i\in Q_d\}$ i.e. all the elements of $D$ that appear at least once in the sequence $d$ and let $R^C_d=D\setminus R_d$. 

Let $y'=(y_i)_{i\in Q_d^C}$, $x_{Q_d} = (x_i)_{i\in Q_d}$, $\RV{Y}(R_d)=(\RV{Y}_d)_{d\in R_d}$ and $\RV{Y}(S_d)=(\RV{Y}_d)_{d\in S_d}$.
\begin{align}
    \kernel{K}[\RV{X}_{[n]}|\RV{D}_{[n]}\RV{Z}]^x_{dz} &= \sum_{y\in Y^{|D|}} \prod_{i\in Q_d} \delta[y_{d_i}]^{x_i} \kernel{K}[\RV{Y}(D)|\RV{D}_{[n]}\RV{Z}]_{dz}^y\\
                                               &= \sum_{y'\in Y^{|R^C_d|}} \kernel{K}[\RV{Y}(R_d)\RV{Y}(R^C_d)|\RV{D}_{Q_d}\RV{D}_{Q_d^C}\RV{Z}]_{d_{Q_d}d_{Q_d}^Cz}^{x_{Q_d}y'}\\
                                               &= \sum_{y'\in Y^{|R^C_d|}} \kernel{K}[\RV{Y}_{R_d}\RV{Y}_{R^C_d}|\RV{D}_{Q_d}\RV{D}_{Q_d^C}\RV{Z}]_{dz}^{x_{Q_d}y'}\\
                                               &= \sum_{y'\in Y^{|R^C_d|}} \kernel{K}[\RV{Y}_{[n]}|\RV{D}_{[n]}\RV{Z}]_{dz}^{x_{Q_d}y'}\qquad\text{ (using exchangeability})
\end{align}

Note that 



Only if:
We aim to show that the sequences $\RV{Y}_{[n]}$ and $\RV{D}_{[n]}$ in a parallel potential outcomes submodel are exchangeable and deterministically reproducible.
\end{proof}

\section{Appendix: Connection is associative}\label{sec:connect_associative}

This will be proven with string diagrams, and consequently generalises to the operation defined by Equation \ref{eq:extn2} in other Markov kernel categories.

Define

\begin{align}
    \RV{I}_{K\cdot\cdot}&:=\RV{I}_K\setminus\RV{I}_L\setminus\RV{I}_J\\
    \RV{I}_{KL\cdot}&:=\RV{I}_K\cap\RV{I}_L\setminus\RV{I}_J\\
    \RV{I}_{K\cdot J}&:=\RV{I}_K\cap\RV{I}_J\setminus\RV{I}_L\\
    \RV{I}_{KL J}&:=\RV{I}_K\cap \RV{I}_L\cap\RV{I}_J\\
    \RV{I}_{\cdot L \cdot} &:= \RV{I}_L\setminus\RV{I}_K\setminus \RV{I}_J\\
    \RV{I}_{\cdot L J} &:= \RV{I}_L\cap \RV{I}_J\setminus \RV{I}_K\\
    \RV{I}_{\cdot\cdot J} &:= \RV{I}_J\setminus\RV{I}_K\setminus\RV{I}_L\\
    \RV{O}_{K\cdot\cdot} &:= \RV{O}_K\setminus\RV{I}_N\setminus\RV{I}_J\\
    \RV{O}_{KL\cdot} &:= \RV{O}_K\cap\RV{I}_L\setminus\RV{I}_J\\
    \RV{O}_{K\cdot J} &:= \RV{O}_K\cap\RV{I}_J\setminus\RV{I}_L\\
    \RV{O}_{KLJ} &:= \RV{O}_K\cap\RV{I}_L\cap\RV{I}_J\\
    \RV{O}_{L\cdot} &:= \RV{O}_L\setminus\RV{I}_J\\
    \RV{O}_{LJ} &:= \RV{O}_L\cap\RV{I}_J
\end{align}

Also define
\begin{align}
    (\kernel{P},\RV{I}_P,\RV{O}_P)&:=\kernel{K}\rightrightarrows \kernel{L}\\
    (\kernel{Q},\RV{I}_Q,\RV{O}_Q)&:=\kernel{L}\rightrightarrows \kernel{J}
\end{align}

Then

\begin{align}
    (\kernel{K}\rightrightarrows \kernel{L})\rightrightarrows \kernel{J} &= \kernel{P}\rightrightarrows \kernel{J}\\
                                                                         &= \begin{tikzpicture}[baseline={([yshift=-.2ex]current bounding box.center)}]
        \path (0,0) node (Y) {$\RV{I}_{P\cdot}$}
        + (0,-0.3) node (Q) {$\RV{I}_{PJ}$}
        + (0,-0.8) node (R) {$\RV{I}_{\cdot J}$}
        ++ (0.5,-0.3) node[copymap] (copy0) {}
        ++ (0.5,0.15) node[kernel] (K) {$\kernel{P}$}
        ++ (0.5,-0.15) node[copymap] (copy1) {}
        ++ (0.6,-0.5) node[kernel] (L) {$\kernel{J}$}
        ++ (0.6, 0.8) node (Z) {$\RV{O}_{P\cdot}$}
        + (0,-0.3) node (X) {$\RV{O}_{PJ}$}
        + (0,-0.8) node (W) {$\RV{O}_J$};
        \draw (Y) -- ($(K.west) + (0,0.15)$) (Q) -- ($(K.west) + (0,-0.15)$);
        \draw (copy0) to [out=-45,in=180] ($(L.west) + (0,0)$) (copy1) to [out=-60,in=180] ($(L.west) + (0,0.15)$);
        \draw (R) to [out=0,in=180] ($(L.west) + (0,-0.15)$);
        \draw ($(K.east) + (0,-0.15)$) to (copy1);
        \draw ($(K.east) + (0,0.15)$) -- (Z) (copy1) to [out=0,in=180] (X) (L) -- (W);
    \end{tikzpicture}\\
    &= \begin{tikzpicture}[baseline={([yshift=-.2ex]current bounding box.center)}] \path (0,0) node (IKdd) {$\RV{I}_{K\cdot\cdot}$}
        + (0,-0.4) node (IKLd) {$\RV{I}_{KL\cdot}$}
        + (0,-0.8) node (IdLd) {$\RV{I}_{\cdot L \cdot}$}
        + (0,-1.2) node (IKdJ) {$\RV{I}_{K\cdot J}$}
        + (0,-1.6) node (IKLJ) {$\RV{I}_{KLJ}$}
        + (0,-2) node (IdLJ) {$\RV{I}_{\cdot LJ}$}
        + (0,-2.4) node (IddJ) {$\RV{I}_{\cdot\cdot J}$}
        + (0.7,-0.4) node[copymap] (copyKL) {}
        + (0.7,-1.2) node[copymap] (copyKJ) {}
        + (0.7,-1.6) node[copymap] (copyKLJ) {}
        + (0.7,-2) node[copymap] (copyLJ) {}
        ++ (1.5,-0.3) node[kernel,inner sep=5pt] (K) {$\kernel{K}$}
        ++ (1.2,0.15) node[copymap] (copyOKL) {}
        +  (0,-0.3) node[copymap] (copyOKJ) {}
        + (0,-0.45) node[copymap] (copyOKLJ) {}
        ++ (1.2,-0.9) node[kernel,inner sep=6pt] (L) {$\kernel{L}$}
        ++ (1.2,-0.15) node[copymap] (copyOLJ) {}
        ++ (1.2,-0.9) node[kernel,inner sep=6pt] (J) {$\kernel{J}$}
        ++ (2.1, 2.1) node (OKdd) {$\RV{O}_{K\cdot\cdot}$}
        + (0,-0.4) node (OKLd) {$\RV{O}_{KL\cdot}$}
        + (0,-0.8) node (OKdJ) {$\RV{O}_{K\cdot J}$}
        + (0,-1.2) node (OKLJ) {$\RV{O}_{KLJ}$}
        + (0,-1.6) node (OLd) {$\RV{O}_{L\cdot}$}
        + (0,-2) node (OLJ) {$\RV{O}_{LJ}$}
        + (0,-2.4) node (OJ) {$\RV{O}_{J}$};
        \draw (IKdd) to [out=0,in=180] ($(K.west) + (0,0.25)$) ($(K.east) + (0,0.25)$) to [out=0,in=180] (OKdd);
        \draw (IKLd) -- (copyKL) to [out=-45,in=180] ($(L.west) + (0,0.1)$) (copyKL) to [out=55,in=180] ($(K.west) + (0,0.125)$)
        ($(K.east)+(0,0.125)$) to [out=0,in=180] (copyOKL) to [out=-90,in=180] ($(L.west) + (0,0.3)$)
        (copyOKL) to [out=0,in=180] (OKLd);
        \draw (IKLJ) to [out=0,in=180] (copyKLJ) to [out=40,in=180] ($(K.west) + (0,-0.25)$) 
        (copyKLJ) to [out=10,in=180] ($(L.west) + (0,0)$)
        (copyKLJ) to [out=-25,in=180] ($(J.west) + (0,-0.2)$);
        \draw (IKdJ) to [out=0,in=180] (copyKJ) to [out=65,in=180] ($(K.west) + (0,-0.125)$)
        (copyKJ) to [out=-30,in=180] ($(J.west) + (0,-0.1)$);
        \draw (IdLd) to [out=0,in=160] ($(IdLd)+(0.8,-0.1)$) to [out=-20,in=180] ($(L.west)+(0,-0.1)$);
        \draw (IdLJ) to [out=0,in=180] (copyLJ) to[out=15,in=180] ($(L.west)+(0,-0.2)$)
        (copyLJ) to [out=-10,in=180] ($(J.west) + (0,0.)$);
        \draw (IddJ) to [out=0,in=180] ($(IddJ)+ (0.5,0)$) to [out=-15,in=180] ($(J.west) + (0,-0.3)$);
        \draw ($(K.east)+(0,-0.125)$) to (copyOKJ) to [out=-75,in=180] ($(J.west) + (0,0.2)$)
        (copyOKJ) to [out=0,in=180] (OKdJ)
        (copyOKLJ) to [out=-80,in=180] ($(J.west) + (0,0.1)$);
        \draw ($(K.east) + (0,-0.25)$) to [out=0,in=180] (copyOKLJ) to [out=0,in=180] (OKLJ)
        (copyOKLJ) to [out=-35,in=180] ($(L.west) + (0,0.2)$);
        \draw ($(L.east) + (0,0.2)$) to [out=0,in=180] (OLd)
        ($(L.east) + (0,0)$) to [out=0,in=170] (copyOLJ) to [out=-10,in=180] (OLJ)
        (copyOLJ) to [out=-45,in=180] ($(J.west) + (0,0.3)$);
        \draw (J) to [out=0,in=180] (OJ);
    \end{tikzpicture}\\
    &\overset{perm}{=} \begin{tikzpicture}[baseline={([yshift=-.2ex]current bounding box.center)}] \path (0,0) node (IKdd) {$\RV{I}_{K\cdot\cdot}$}
        + (0,-0.4) node (IKLd) {$\RV{I}_{KL\cdot}$}
        + (0,-0.8) node (IKdJ) {$\RV{I}_{K\cdot J}$}
        + (0,-1.2) node (IKLJ) {$\RV{I}_{KLJ}$}
        + (0,-1.6) node (IdLd) {$\RV{I}_{\cdot L \cdot}$}
        + (0,-2) node (IdLJ) {$\RV{I}_{\cdot LJ}$}
        + (0,-2.4) node (IddJ) {$\RV{I}_{\cdot\cdot J}$}
        + (0.7,-0.4) node[copymap] (copyKL) {}
        + (0.7,-.8) node[copymap] (copyKJ) {}
        + (0.7,-1.2) node[copymap] (copyKLJ) {}
        + (0.7,-2) node[copymap] (copyLJ) {}
        ++ (1.5,-0.3) node[kernel,inner sep=5pt] (K) {$\kernel{K}$}
        ++ (1.2,0.15) node[copymap] (copyOKL) {}
        +  (0,-0.3) node[copymap] (copyOKJ) {}
        + (0,-0.45) node[copymap] (copyOKLJ) {}
        ++ (1.2,-0.9) node[kernel,inner sep=6pt] (L) {$\kernel{L}$}
        ++ (1.2,-0.15) node[copymap] (copyOLJ) {}
        ++ (1.2,-0.9) node[kernel,inner sep=6pt] (J) {$\kernel{J}$}
        ++ (2.1, 2.1) node (OKdd) {$\RV{O}_{K\cdot\cdot}$}
        + (0,-0.4) node (OKLd) {$\RV{O}_{KL\cdot}$}
        + (0,-0.8) node (OKdJ) {$\RV{O}_{K\cdot J}$}
        + (0,-1.2) node (OKLJ) {$\RV{O}_{KLJ}$}
        + (0,-1.6) node (OLd) {$\RV{O}_{L\cdot}$}
        + (0,-2) node (OLJ) {$\RV{O}_{LJ}$}
        + (0,-2.4) node (OJ) {$\RV{O}_{J}$};
        \draw (IKdd) to [out=0,in=180] ($(K.west) + (0,0.25)$) ($(K.east) + (0,0.25)$) to [out=0,in=180] (OKdd);
        \draw (IKLd) -- (copyKL) to [out=-45,in=180] ($(L.west) + (0,0.1)$) (copyKL) to [out=55,in=180] ($(K.west) + (0,0.125)$)
        ($(K.east)+(0,0.125)$) to [out=0,in=180] (copyOKL) to [out=-90,in=180] ($(L.west) + (0,0.3)$)
        (copyOKL) to [out=0,in=180] (OKLd);
        \draw (IKLJ) to [out=0,in=180] (copyKLJ) to [out=40,in=180] ($(K.west) + (0,-0.25)$) 
        (copyKLJ) to [out=10,in=180] ($(L.west) + (0,0)$)
        (copyKLJ) to [out=-25,in=180] ($(J.west) + (0,-0.2)$);
        \draw (IKdJ) to [out=0,in=180] (copyKJ) to [out=65,in=180] ($(K.west) + (0,-0.125)$)
        (copyKJ) to [out=-30,in=180] ($(J.west) + (0,-0.1)$);
        \draw (IdLd) to [out=0,in=180] ($(L.west)+(0,-0.1)$);
        \draw (IdLJ) to [out=0,in=180] (copyLJ) to[out=15,in=180] ($(L.west)+(0,-0.2)$)
        (copyLJ) to [out=-10,in=180] ($(J.west) + (0,0.)$);
        \draw (IddJ) to [out=0,in=180] ($(IddJ)+ (0.5,0)$) to [out=-15,in=180] ($(J.west) + (0,-0.3)$);
        \draw ($(K.east)+(0,-0.125)$) to (copyOKJ) to [out=-75,in=180] ($(J.west) + (0,0.2)$)
        (copyOKJ) to [out=0,in=180] (OKdJ)
        (copyOKLJ) to [out=-80,in=180] ($(J.west) + (0,0.1)$);
        \draw ($(K.east) + (0,-0.25)$) to [out=0,in=180] (copyOKLJ) to [out=0,in=180] (OKLJ)
        (copyOKLJ) to [out=-35,in=180] ($(L.west) + (0,0.2)$);
        \draw ($(L.east) + (0,0.2)$) to [out=0,in=180] (OLd)
        ($(L.east) + (0,0)$) to [out=0,in=170] (copyOLJ) to [out=-10,in=180] (OLJ)
        (copyOLJ) to [out=-45,in=180] ($(J.west) + (0,0.3)$);
        \draw (J) to [out=0,in=180] (OJ);
    \end{tikzpicture}\\
    &= \begin{tikzpicture}[baseline={([yshift=-.2ex]current bounding box.center)}]
        \path (0,0) node (Y) {$\RV{I}_{K\cdot}$}
        + (0,-0.3) node (Q) {$\RV{I}_{KQ}$}
        + (0,-0.8) node (R) {$\RV{I}_{\cdot Q}$}
        ++ (0.5,-0.3) node[copymap] (copy0) {}
        ++ (0.5,0.15) node[kernel] (K) {$\kernel{K}$}
        ++ (0.5,-0.15) node[copymap] (copy1) {}
        ++ (0.6,-0.5) node[kernel] (L) {$\kernel{Q}$}
        ++ (0.6, 0.8) node (Z) {$\RV{O}_{K\cdot}$}
        + (0,-0.3) node (X) {$\RV{O}_{KQ}$}
        + (0,-0.8) node (W) {$\RV{O}_Q$};
        \draw (Y) -- ($(K.west) + (0,0.15)$) (Q) -- ($(K.west) + (0,-0.15)$);
        \draw (copy0) to [out=-45,in=180] ($(L.west) + (0,0)$) (copy1) to [out=-60,in=180] ($(L.west) + (0,0.15)$);
        \draw (R) to [out=0,in=180] ($(L.west) + (0,-0.15)$);
        \draw ($(K.east) + (0,-0.15)$) to (copy1);
        \draw ($(K.east) + (0,0.15)$) -- (Z) (copy1) to [out=0,in=180] (X) (L) -- (W);
    \end{tikzpicture}\\
    &= \kernel{K}\rightrightarrows (\kernel{L}\rightrightarrows \kernel{J})
\end{align}

\section{Appendix: String Diagram Examples}

Recall the definition of \emph{connection}:
\begin{definition}[Connection]
\begin{align}
    \kernel{K}\rightrightarrows \kernel{L} &:=  \begin{tikzpicture}[baseline={([yshift=-.2ex]current bounding box.center)}]
        \path (0,0) node (Y) {$\RV{I}_{F\cdot}$}
        + (0,-0.3) node (Q) {$\RV{I}_{FS}$}
        + (0,-0.8) node (R) {$\RV{I}_{\cdot S}$}
        ++ (0.5,-0.3) node[copymap] (copy0) {}
        ++ (0.5,0.15) node[kernel] (K) {$\kernel{K}$}
        ++ (0.5,-0.15) node[copymap] (copy1) {}
        ++ (0.6,-0.5) node[kernel] (L) {$\kernel{L}$}
        ++ (0.6, 0.8) node (Z) {$\RV{O}_{F\cdot}$}
        + (0,-0.3) node (X) {$\RV{O}_{FS}$}
        + (0,-0.8) node (W) {$\RV{O}_S$};
        \draw (Y) -- ($(K.west) + (0,0.15)$) (Q) -- ($(K.west) + (0,-0.15)$);
        \draw (copy0) to [out=-45,in=180] ($(L.west) + (0,0)$) (copy1) to [out=-60,in=180] ($(L.west) + (0,0.15)$);
        \draw (R) to [out=0,in=180] ($(L.west) + (0,-0.15)$);
        \draw ($(K.east) + (0,-0.15)$) to (copy1);
        \draw ($(K.east) + (0,0.15)$) -- (Z) (copy1) to [out=0,in=180] (X) (L) -- (W);
    \end{tikzpicture}\label{eq:extn_definition1}\\
    &:= \kernel{J}\\
    \kernel{J}_{yqr}^{zxw} &= \kernel{K}_{yq}^{zx} \kernel{L}_{xqr}^{w}\label{eq:extn_definition2}
\end{align}
\end{definition}

Equation \ref{eq:extn_definition1} can be broken down to the product of four Markov kernels, each of which is itself a tensor product of a number of other Markov kernels:
\begin{align}
    (\kernel{J},(\RV{I}_{F\cdot},\RV{I}_{FS},\RV{I}_{\cdot S}), (\RV{O}_{F\cdot},\RV{O}_{FS},\RV{O}_S)) &= \left[ \begin{tikzpicture}[baseline={([yshift=-.5ex]current bounding box.center)}]
        \path (0,0) node (Y) {$\RV{I}_{F\cdot}$}
        + (0,-0.3) node (Q) {$\RV{I}_{FS}$}
        + (0,-0.75) node (R) {$\RV{I}_{\cdot S}$}
        ++ (0.5,-0.3) node[copymap] (copy1) {}
        ++ (0.5,0.3) node (Z) {}
        ++ (0,-0.15) node (Q1) {}
        ++ (0,-0.3) node (Q2) {}
        ++ (0,-0.3) node (R2) {};
        \draw (Y) -- (Z) (Q) -- (copy1) to [out=45,in=180] (Q1);
        \draw (copy1) to [out=-45,in=180] (Q2);
        \draw (R) -- (R2); \end{tikzpicture}\right]
        \left[\begin{tikzpicture}[baseline={([yshift=-.5ex]current bounding box.center)}]
        \path (0,0)  node (Z) {}
        ++ (0,-0.15) node (Q1) {}
        ++ (0,-0.3) node (Q2) {}
        ++ (0,-0.3) node (R) {}
        ++ (0.5,0.65) node[kernel] (K) {$\model{K}$}
        ++ (0.5,0.1) node (Z1) {}
        +  (0,-0.15) node (W) {}
        + (0,-0.45) node (Q3) {}
        + (0,-0.75) node (R2) {};
        \draw (Z) -- ($(K.west) + (0,0.1)$) (Q1) -- ($(K.west) + (0,-0.05)$);
        \draw (Q2) -- (Q3) (R) -- (R2) ($(K.east) + (0,0.1)$) -- (Z1); 
        \draw ($(K.east) + (0,-0.05)$) -- (W);\end{tikzpicture}\right] 
        \left[ \begin{tikzpicture}[baseline={([yshift=-.5ex]current bounding box.center)}]
        \path (0,0) node (Z) {}
        + (0,-0.15) node (X) {}
        + (0,-0.45) node (Q) {}
        + (0,-0.75) node (R) {}
        ++ (0.5,-0.3) node[copymap] (copy1) {}
        ++ (0.5,0.3) node (Z1) {}
        ++ (0,-0.15) node (X1) {}
        ++ (0,-0.3) node (X2) {}
        ++ (0,-0.15) node (Q2) {}
        ++ (0,-0.15) node (R2) {};
        \draw (Z) -- (Z1) (X) to [out=0,in=180] (copy1) to [out=45,in=180] (X1);
        \draw (copy1) to [out=-45,in=180] (X2);
        \draw (Q) to [out=0,in=180] (Q2);
        \draw (R) -- (R2); \end{tikzpicture}\right]
        \left[\begin{tikzpicture}[baseline={([yshift=-.5ex]current bounding box.center)}]
        \path (0,0) node (Z) {}
        ++ (0,-0.15) node (X1) {}
        ++ (0,-0.3) node (X2) {}
        ++ (0,-0.15) node (Q) {}
        ++ (0,-0.15) node (R) {}
        ++ (0.5,0.15) node[kernel] (L) {$\model{L}$}
        ++ (0.7,0) node (W) {$\RV{O}_{F\cdot}$}
        ++ (0,0.35) node (X3) {$\RV{O}_{FS}$}
        ++ (0,0.25) node (Z1) {$\RV{O}_S$};
        \draw (X1) to [out=0,in=180] (X3) (Z) -- (Z1);
        \draw (X2) to [out=0,in=180] ($(L.west) + (0,0.15)$);
        \draw (Q) to [out=0,in=180] ($(L.west) + (0,0)$);
        \draw (R) to [out=0,in=180] ($(L.west) + (0,-0.15)$);
        \draw (L) -- (W);\end{tikzpicture}\right]\\
\end{align}

\section{Markov variable maps and variables form a Markov category}\label{sec:app_mcat}

In the following, given \emph{arbitrary measurable sets} $(X,\sigalg{X})$ and $(Y,\sigalg{Y})$, a Markov kernel is a function $\kernel{K}:X\times\sigalg{Y}\to [0,1]$ such that

\begin{itemize}
    \item For every $A\in \sigalg{Y}$, the function $x\mapsto \kernel{K}(x,A)$ is $\sigalg{X}$-measurable
    \item For every $x\in X$, the function $A\mapsto \kernel{K}(x,A)$ is a probability measure on $(Y,\sigalg{Y})$
\end{itemize}

Note that this is a more general definition than the one used in the main paper; the version in the main paper is the restriction of this definition to finite sets.

The \emph{delta function} $\delta:X\to\Delta(\sigalg{X})$ is the Markov kernel defined by
\begin{align}
    \delta(x,A) = \begin{cases}
        1 & x\in A\\
        0 & \text{otherwise}
    \end{cases}
\end{align}

\citet{fritz_synthetic_2020} defines Markov categories in the following way:

\begin{definition}
A Markov category $C$ is a symmetric monoidal category in which every object $X \in C$ is equipped with a commutative comonoid structure given by a comultiplication $\text{copy}_X : X\to X\otimes X$ and a counit $\text{del}_X : X \to I$, depicted in string diagrams as
\begin{align}
    \text{del}_X&:=\begin{tikzpicture}[baseline={([yshift=-.5ex]current bounding box.center)}]
    \path (0,0) ++ (1,0) node (B) {};
    \draw[-{Rays[n=8]}] (A) -- (B);
\end{tikzpicture}
    \text{copy}_X&:=\begin{tikzpicture}[baseline={([yshift=-.5ex]current bounding box.center)}]
    \path (0,0) node (A) {} 
    ++ (0.5,0) node[copymap] (copy0) {}
    ++ (0.5,0.15) node (B) {}
    + (0,-0.3) node (C) {};
    \draw (A) -- (copy0) to [out=45,in=180] (B) (copy0) to [out=-45, in=180] (C);
\end{tikzpicture}
\end{align}
and satisfying the commutative comonoid equations
\begin{align}
    \tikzfig{ccom_lhs} = \tikzfig{ccom_rhs}\label{eq:ccom_1}
\end{align}
\begin{align}
    \tikzfig{ccom2_lhs} = \tikzfig{ccom2_mhs} = \tikzfig{ccom2_rhs}
\end{align}
\begin{align}
    \tikzfig{ccom3_lhs} = \tikzfig{ccom3_rhs}
\end{align}
as well as compatibility with the monoidal structure
\begin{align}
    \tikzfig{mstruct1_lhs} &= \tikzfig{mstruct1_rhs}\\
    \tikzfig{mstruct2_lhs} &= \tikzfig{mstruct2_rhs}
\end{align}
and the naturality of \emph{del}, which means that
\begin{align}
    \tikzfig{naturality_lhs} &= \tikzfig{naturality_rhs}\label{eq:nat}
\end{align}
for every morphism $f$.
\end{definition}

The category of labeled Markov kernels is the category consisting of labeled measurable sets as objects and labeled Markov kernels as morphisms. Given $\kernel{K}:\RV{X}\to \Delta(\RV{Y})$ and $\kernel{L}:\RV{Y}\to \Delta(\RV{Z})$, sequential composition is given by

\begin{align}
    \kernel{K}\kernel{L}:\RV{X}\to \Delta(\RV{Z})\\
    \text{defined by } (\kernel{KL})(x,A) = \int_Y \kernel{L}(y,A)\kernel{K}(x,dy)
\end{align}

For $\kernel{K}:\RV{X}\to \Delta(\RV{Y})$ and $\kernel{L}:\RV{W}\to\Delta(\RV{Z})$, parallel composition is given by

\begin{align}
    \kernel{K}\otimes\kernel{L}: (\RV{X},\RV{W}) &\to \Delta(\RV{Y},\RV{Z})\\
    \text{defined by } \kernel{K}\otimes\kernel{L}(x,w,A\times B) = \kernel{K}(x,A)\kernel{L}(w,B)
\end{align}

The identity map is

\begin{align}
    \text{Id}_{\RV{X}}: \RV{X}&\to\Delta(\RV{X})\\
    \text{defined by} (\text{Id}_X) (x,A) &= \delta(x,A)
\end{align}

We take an arbitrary single element labeled set $I=(*,\{*\})$ to be the unit, which we note satisfies $I\otimes X=X\otimes I=X$ by Lemma \ref{lem:se_id}.

The swap map is given by

\begin{align}
    \text{swap}_{\RV{X},\RV{Y}}: (\RV{X},\RV{Y})&\to \Delta(\RV{Y},\RV{X})\\
    \text{defined by} (\text{swap}_{\RV{X},\RV{Y}})(x,y,A\times B) &= \delta(x,B)\delta(y,A)
\end{align}

And we use the standard associativity isomorphisms for Cartesian products such that $(A\times B)\times C\cong A\times (B\times C)$, which in turn implies $(\RV{X},(\RV{Y},\RV{Z}))\cong ((\RV{X},\RV{Y}),\RV{Z})$.

The copy map is given by

\begin{align}
    \text{copy}_{\RV{X}}: \RV{X}&\to \Delta(\RV{X},\RV{X})\\
    \text{defined by} (\text{copy}_X)(x,A\times B) &= \delta_x(A)\delta_x(B)
\end{align}

and the erase map by

\begin{align}
    \text{del}_{\RV{X}}: \RV{X}&\to \Delta(*)\\
    \text{defined by} (\text{del}_X)(x,A) &= \delta(*,A)\\
\end{align}

Note that the category formed by taking the underlying unlabeled sets and the underlying unlabeled morphisms is identical to the category of measurable sets and Markov kernels described in \citet{fong_causal_2013,cho_disintegration_2019,fritz_synthetic_2020}.

\begin{theorem}[The category of labeled Markov kernels and labeled measurable sets is a Markov category]
The category described above is a Markov category.
\end{theorem}

\begin{proof}

\todo[inline]{I'm not sure how to formally argue that it is monoidal and symmetric as the relevant texts I've checked all gloss over the functors with respect to which the relevant isomorphisms should be natural, but labels with products were intentionally made to act just like sets with cartesian products which are symmetric monoidal}

Equations \ref{eq:ccom_1} to \ref{eq:nat} are known to be satisfied for the underlying unlabeled Markov kernels. We need to show is that they hold given our stricter criterion of labeled Markov kernel equality; that the underlying kernels \emph{and the label sets} match. It is sufficient to check the label sets only.



\end{proof}