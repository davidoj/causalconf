%!TEX root = main.tex

\section{Decision theoretic causal inference}\label{sec:seedo_models}

People very often have to make decisions with some information they may consult to help them make the decision. We are going to examine how gappy probability models can formally represent problems of this type, which in turn allows us to make use of the theory of probability to help guide us to a good decision. Probabilistic models have a long history of being used to represent decision problems, and there exist a number of coherence theorems that show that preferences that satisfy certain kinds of constraints must admit representation by a probability model and a utility function of the appropriate type. Particularly noteworthy are the theorems of \citet{ramsey_truth_2016} and \citet{savage_foundations_1954}, which together yield a method for representing decision problems known as ``Savage decision theory'', and the theorem of \citet{bolker_functions_1966,jeffrey_logic_1990} which yields a rather different method for representing decision problems known as ``evidential decision theory''. \citet{joyce_foundations_1999} extends Jeffrey and Bolker's result to a representation theorem that subsumes both ``causal decision theory'' and ``evidential decision theory''.

None of these representation theorems explicitly concern themselves with probability gaps. One may try to find gappy probability models inside the theories, for example in the way that the Savage theory extends a probability distribution over \emph{states} to a probability distribution over \emph{consequence} when given an \emph{act}. However, the connection to probability gaps is not explicit and may not follow precisely. We could make similar comments about causal or evidential decision theories: perhaps ``causal conditionals'' are gappy probability models, but we don't pursue the question to answer it one way or another.

We do not have a comparable axiomatisation of preferences that yield a representation of decision problems in terms of utility and gappy probability. Such an undertaking could potentially clarify some choices that can be made in setting up a gappy probability model of decision making, but it is the subject of future work. Instead, we suppose that we are satisfied with a particular probabilistic model of a decision problem, a supposition based on convention rather than axiomatisation.

\subsection{Decision problems}

Suppose we have an observation process $\proc{X}$, modelled by $\RV{X}$ taking values in $X$ (we are \emph{informed}). Given an observation $x\in X$, we suppose that we can choose a decision from a known set $D$ (the set of decisions is \emph{transparent}), and we suppose that choosing a decision results in some action being taken in the real world. As with processes of observation, we will mostly ignore the details of what ``taking an action'' involves. The process of choosing a decision that yields an element of $D$ is a decision making process $\proc{D}$ modelled by $\RV{D}$. We might be able to introduce randomness to the choice, in which case the relation between $\RV{X}$ and $\RV{D}$ may be stochastic.  We will assume that there is some $\proc{Y}$ modelled by $\RV{Y}$ such that $(\RV{X},\RV{D},\RV{Y})$ tell us everything we want to know for the purposes of deciding which outcomes are better than others.

We want a model that allows us to compare different stochastic \emph{decision functions} $\kernel{Q}^{\RV{D}|\RV{X}}_\alpha:X\kto D$. That is, we need a probability gap model $\prob{P}$ that takes a decision function $\kernel{Q}^{\RV{X}|\RV{D}}_\alpha$ and returns a probabilistic model of the consequences of selecting that decision function $\kernel{P}_\alpha^{\RV{DXY}}$. An order 2 model $\kernel{P}^{\RV{X}\square \RV{Y}|\RV{D}}$ is such a function, but there are many such functions that are not order 2 models; the key features of order 2 models are the fact that $\prob{P}_\alpha^{\RV{X}}=\prob{P}^{\RV{X}}$, $\prob{P}_\alpha^{\RV{D}|\RV{X}}=\prob{Q}_\alpha^{\RV{D}|\RV{X}}$ and $\prob{P}_\alpha^{\RV{Y}|\RV{DX}}=\prob{P}^{\RV{Y}||RV{DX}}$ for all $\alpha$. If we don't think all of these properties hold, then we want something other than an order 2 model. We will define \emph{ordinary decision problems} to be those for which the desired model $\prob{P}$ is an order 2 probability gap model.

We consider an additional kind of gap in our probability model. The nature of this gap is: we don't know exactly which order 2 model $\kernel{P}^{\RV{X}\square \RV{Y}|\RV{D}}$ we ``ought'' to use. To represent this gap we include an unobserved variable $\RV{H}$, the \emph{hypothesis}. We can interpret $\RV{H}$ as expressing the fac that, if we knew the value of $\RV{H}$ then we would know that our decision problem was represented by a unique order 2 model $\kernel{P}_h^{\RV{X}\square \RV{Y}|\RV{D}}$. However, $\RV{H}$ is not known and in fact we do not know how to determine $\RV{H}$ (this is the nature of an \emph{unoserved} variable -- there is no process available to find the value it yields). Our model is thus given by $\kernel{P}^{\RV{X}|\RV{H}\square \RV{Y}|\RV{D}}$.

\todo[inline]{$\RV{H}$ parametrises observations and consequences together; not just a function of observations so req. slight generalisation of De Finetti}

\begin{definition}[Ordinary decision problem]
An ordinary decision problem $(\prob{P},\Omega,\RV{H},(\RV{X},\proc{X}),(\RV{D},\proc{D}),(\RV{Y},\proc{Y}))$ consists of a fundamental probability set $\Omega$, hypotheses $\RV{H}:\Omega\to H$, observations $\RV{X}\Omega\to X$, decisions $\RV{D}:\Omega\to D$ and consequences $\RV{Y}:\Omega\to Y$, and the latter three random variables are associated with measurement processes. It is equipped with a probability gap model $\prob{P}:\Delta(D)^X\to \Delta(\Omega)^H$ where $\Delta(D)^X$ is the set of valid $\RV{D}|\RV{X}$ Markov kernels $X\kto D$ and $\Delta(\Omega)^H$ is the set of valid Markov kernels $H\kto \Omega$. We require of $\prob{P}$:
\begin{enumerate}
    \item $\kernel{P}_\alpha^{\RV{D}|\RV{X}}=\model{Q}_\alpha^{\RV{D}|\RV{X}}$ for all decision functions $\model{Q}_\alpha^{\RV{D}|\RV{X}}\in \Delta(D)^X$
    \item $\prob{P}^{\RV{X}|\RV{H}}=\model{P}_\alpha^{\RV{X}|\RV{H}}$ for all $\model{P}_\alpha:=\model{P}(\model{Q}_\alpha^{\RV{D}|\RV{X}})$
    \item $\prob{P}^{\RV{Y}|\RV{XDH}}=\model{P}_\alpha^{\RV{Y}|\RV{XDH}}$ for all $\model{P}_\alpha:=\model{P}(\model{Q}_\alpha^{\RV{D}|\RV{X}})$
\end{enumerate}
\end{definition}

(1) reflects the assumption that the ``probability of $\RV{D}$ given $\RV{X}$'' based on the induced model is equal to the ``probability of $\RV{D}$ given $\RV{X}$'' based on the chosen decision function. (2) reflects the assumption that the observations should be modelled identically no matter which decision function is chosen. (3) reflects the assumption that given hypothesis, the observations and the decision, the model of $\RV{Y}$ does not depend any further on the decision function $\alpha$.

Under these assumptions $\prob{P}$ is a second order moel $\model{P}^{\RV{X}|\RV{H}\square \RV{Y}|\RV{D}}$ which we call a ``see-do model''. The proof is given in Appendix \ref{sec:see-do-rep}.

\subsection{What should a probability model represent? Controversies about decision theories}

We do not prove any representation theorems showing that a particular type of decision problem must be representable by a see-do model. There is a long history of using probabilistic models for decision making, and we show that see-do models are closely related to classical statistical decision theory.

We could consider problems in which one or more of these assumptions could be argued to be unreasonable. Such questions are related to a number of controversies in decision theory. Newcomb's problem, for example, invites us to consider a problem where a second party has predicted our choice of decision function $\alpha$ before we have made the choice, and we have good reasons to believe this prediction is correct \citep{nozick_newcombs_1969}. \emph{The predictor} may then make choices that affect the consequences we expect to see, and this could mean that it is appropriate to consider models in which consequences to depend on $\alpha$ in addition to $\RV{D}$. The question of which kind of model \emph{should} be adopted in such a situation is controversial \citet{weirich_causal_2016,lewis_causation_1986}, and two prominent views on the correct answer are \emph{causal decision theory} and \emph{evidential decision theory}.

This work does not propose normative rules for getting from a description of the world to a see-do model, and so the question of ``which decision theory?'' is not addressed here. We could ask if causal and evidential decision theory can be operationalised as different (vague) rules for getting from descriptions of the world to probability 2-combs $\model{P}^{\RV{X}|\RV{H}\square \RV{Y}|\RV{D}}$, but we leave this question open.

\subsection{Decisions as measurement procedures}

We want to note a potentially interesting feature of our setup here: specifying how we will choose decisions amounts to defining a measurement procedure for decisions.

We have previously posited that observed variables are variables $\RV{X}$ -- themselves purely mathematical objects -- associated with a measurment process $\proc{X}$ that has ``one foot in the real world''. Are decisions observed variables? That is, can we associate a measurement process $\proc{D}$ with the decision variable $\RV{D}$? We can imagine that the decision maker might take note of whatever decision they've made when they make it; furthermore, we have supposed that the kinds of decision problems under consideration here fix a set $D$ of decisions for the decision maker to contemplate, so this seems like a reasonable measurement process, although one that may generally only be available to the decision maker.

There is a complication regarding this account of decision ``measurement procedures''. Suppose for the sake of simplifying the issue that we are only contemplating deterministic choices from $\RV{D}$. We are assuming here that we will choose $\RV{D}$ according to a model, a utility function on $X\times D\times Y$ and a decision rule; that is, we have a function $f$ from models, utility functions and decision rules to decisions. How do we acquire models, utility functions and decision rules in the first place? Given that these are all well-defined mathematical objects, it appears that some measurement procedure $\proc{M}$ yields them. But then $f\circ \proc{M}$ -- that is, the function that yields a decision -- is itself a measurement procedure and it seems quite reasonable to call it $\proc{D}$, i.e. the measurement procedure associated with the variable $\RV{D}$. In this view ``choosing the decision'' and ``measuring the decision'' are actually exactly the same thing. If we move to considering a stochastic selection of decisions then we could make the same comments about the procedure that selects a probability distribution from which to sample the decision.


% \subsection{Unresponsiveness}

% Given a see-do model $\model{P}^{\RV{X}|\RV{H}\square \RV{Y}|\RV{D}}$, $\RV{A}\CI_{\prob{P}}^{2|*} \RV{D}|\RV{H}$ can be interpreted to express the property, given any hypothesis, $\RV{A}$ does not depend on $\RV{D}$ when we choose $\RV{D}$ randomly.

% This might sound like it expresses a property of ``causal independence''. However, it isn't quite satisfactory for this term. Consider $\RV{A}\in\{0,1\}$ representing the outcome of a fair coin toss, $\RV{D}\in\{0,1\}$ representing a bet on the coin toss and $\RV{B}\in\{0,1\}$ representing the outcome of the bet. There is one hypothesis -- ``the coin is fair'' -- and no observations. We construct a see-do model $\prob{P}$ in the obvious way given these assumptions. Then $\RV{A}\CI_{\prob{P}}^{2|*} \RV{D}|\RV{H}$ (``the coin toss is independent of the decision, for any decision rule'') and $\RV{B}\CI_{\prob{P}}^{2|*}\RV{D}|\RV{H}$ (``the outcome of the bet is independent of the decision, for any decision rule''). However, it is not the case that $(\RV{A},\RV{B})\CI_{\prob{P}}^{2|*}\RV{D}|\RV{H}$. Were both $\RV{A}$ and $\RV{B}$ causally independent of $\RV{D}$, then $(\RV{A},\RV{B})$ also ought to be independent of $\RV{D}$. This example is from \citet{heckerman_decision-theoretic_1995}.

% We will borrow the terminology of \emph{unresponsiveness} from \citet{heckerman_decision-theoretic_1995} to refer to independences like $\RV{A}\CI_{\prob{P}}^{2|*} \RV{D}|\RV{H}$; specifically, where a variable is independent of $\RV{D}$ given $\RV{H}$ under the restricted 2-comb $\model{P}^{\RV{X}|\RV{H}\framebox{*} \RV{Y}|\RV{D}}$.

\subsection{Causal models similar to see-do models}

\citet{lattimore_causal_2019} and \citet{lattimore_replacing_2019} consider an observational probability model and a collection of indexed interventional probability models, with the probability model tied to the interventional models by shared parameters. In these papers, they show how such a model can reproduce inferences made using Causal Bayesian Networks. This kind of model can be identified with a type of see-do model, where what we call hypotheses $\RV{H}$ are identified with the sequence of what Rohde and Lattimore call parameter variables.

The approach to decision theoretic causal inference described by \citet{dawid_decision-theoretic_2020} is somewhat different:

\begin{quote}
A fundamental feature of the DT approach is its consideration of the relationships between the various probability distributions that govern different regimes of interest. As a very simple example, suppose that we have a binary treatment variable $\RV{T}$, and a response variable $\RV{Y}$. We consider three different regimes [...] the first two regimes may be described as interventional, and the last as observational.
\end{quote}

The difference between the model described here and a see-do model is that a see-do model uses different variables $\RV{X}$ and $\RV{Y}$ to represent observations and consequences, while Dawid's model uses the same variable $(\RV{T},\RV{Y})$ to represent outcomes in interventional and observational regimes. In this work we associate one observed variable with each measurement process, while in Dawid's approach $(\RV{T},\RV{Y})$ seem to be doing double duty, representing mesurement processes carried out during observations and after taking action. This can be thought of as the causal analogue of the difference betwen saying we have a sequence $(\RV{X}_1,\RV{X}_2,\RV{X}_3)$ of observations independent and identically distributed according to $\mu\in \Delta(X)$ and saying that we have some observations distributed according to $\prob{P}^{\RV{X}}\in \Delta(X)$. People usually understand what is meant by the latter, but if one is trying to be careful the former is a more precise statement of the model in question.

\citet{heckerman_decision-theoretic_1995} also explore a decision theoretic approach to causal inference. Our approach is quite close to their approach if we identify what we call hypotheses with what they call states and allow for probabilistic dependence between states, decisions and consequences. It is an open question whether their notion of limited unresponsiveness corresponds to any notion of conditional independence in our work.

\citet{jacobs_causal_2019} has used a comb decomposition theorem to prove a sufficient identification condition similar to the identification condition given by \citet{tian2002general}. This theorem depends on the particular inductive hypotheses made by causal Bayesian networks.

\subsection{See-do models and classical statistics}

See-do models are capable of expressing the expected results of a particular choice of decision strategy, but they cannot by themselves tell us which strategies are more desirable than others. To do this, we need some measure of the desirability of our collection of results $\{\prob{P}_\alpha|\alpha\in A\}$. A common way to do this is to employ the principle of expected utility. The classic result of \citet{von_neumann_theory_1944} shows that all preferences over a collection of probability models that obey their axioms of completeness, transitivity, continuity and independence of irrelevant alternatives must be able to be expressed via the principle of expected utility. This does not imply that anyone knows what the appropriate utility function is.

A further property that may hold for some see-do models $\prob{P}^{\RV{X}|\RV{H}\square \RV{Y}|\RV{D}}$ is $\RV{Y}\CI^2_{\prob{P}} \RV{X}|(\RV{H},\RV{D})$. This expresses the view that the consequences are independent of the observations, once the hypothesis and the decision are fixed. Such a situation could hold in our scenario above, where the observations are trial data, the decisions are recommendations to care providers and the consequences are future patient outcomes. In such a situation, we might suppose that the trial data are informative about the consequences only via some parameter such as effect size; if the effect size can be deduced from $\RV{H}$ then our assumption corresponds to the conditional independence above.

Given a see-do model $\prob{P}^{\RV{X}|\RV{H}\square \RV{Y}|\RV{D}}$ along with the principle of expected utility to evaluate strategies, and the assumption $\RV{Y}\CI^2_{\prob{P}} \RV{X}|(\RV{H},\RV{D})$ we obtain a statistical decision problem in the form introduced by \citet{wald_statistical_1950}.

A \emph{statistical model} (or \emph{statistical experiment}) is a collection of probability distributions $\{\prob{P}_\theta\}$ indexed by some set $\Theta$. A statistical decision problem gives us an observation variable $\RV{X}:\Omega\kto X$ and a statistical experiment $\{\prob{P}^{\RV{X}}_\theta\}_\Theta$, a decision set $D$ and a loss $l:\Theta\times D\to \mathbb{R}$. A strategy $\model{S}^{\RV{D}|\RV{X}}_\alpha$ is evaluated according to the risk functional $R(\theta,\alpha):=\sum_{x\in X}\sum_{d\in D} \prob{P}^{\RV{X}}_\theta(x) S^{\RV{D}|\RV{X}}_\alpha (d|x) l(h,d)$. A strategy $\model{S}^{\RV{D}|\RV{X}}_\alpha$ is considered more desirable than $\model{S}^{\RV{D}|\RV{X}}_\beta$ if $R(\theta,\alpha)<R(\theta,\beta)$.

Suppose we have a see-do model $\prob{P}^{\RV{X}|\RV{H}\square \RV{Y}|\RV{D}}$ with $\RV{Y}\CI_{\model{P}} \RV{X}|(\RV{H,D})$, and suppose that the random variable $\RV{Y}$ is a ``negative utility'' function taking values in $\mathbb{R}$ for which \emph{low} values are considered desirable. Define a loss $l:H\times D\to \mathbb{R}$ by $l(h,d) = \sum_{y\in \mathbb{R}} y\model{P}^{\RV{Y}|\RV{H}\RV{D}}(y|h,d)$, we have 

\begin{align}
    \mathbb{E}_{\model{P}_{\alpha}}[\RV{Y}|h] &= \sum_{x\in X} \sum_{d\in D} \sum_{y\in Y} \model{P}^{\RV{X}|\RV{H}}(x|h) \model{Q}_\alpha^{\RV{D}|\RV{X}}(d|x) \model{P}^{\RV{Y}|\RV{HD}}(y|h,d)\\
    &= \sum_{x\in X} \sum_{d\in D} \model{P}^{\RV{X}|\RV{H}}(x|h) \model{Q}_\alpha^{\RV{D}|\RV{X}}(d|x) l(h,d)\\
    &= R(h,\alpha)
\end{align}

If we are given a see-do model where we interpret $\{\model{P}^{\RV{X}|\RV{H}}(\cdot|h)|h\in H\}$ as a statistical experiment and $\RV{Y}$ as a negative utility, the expectation of the utility under the strategy forecast given in equation \ref{eq:see_do_query} is the risk of that strategy under hypothesis $h$.

