%!TEX root = main.tex

\section{Why do causal investigations?}

When someone undertakes a causal investigation, they're often aiming to improve the decisions of some decision maker. Often the identity of the decision maker and the exact parameters of the decisions at hand are vague. Consider \citet{banerjee_mainstreaming_2016}: a large scale experiment was conducted trialling a number of different strategies all aiming to increase the amount of learning level appropriate instruction available to students in four Indian states. It is not clear who, exactly, is going to make a decision on the basis of this information, but one can guess:

\begin{itemize}
    \item They're someone with interest in and authority to make large scale changes to a school system
    \item They consider the evidence of effectiveness of teaching at the right level relevant to their situation
    \item They consider the evidence regarding which strategies work to implement this approach relevant to their situation
\end{itemize}

Such a situation seems to resist straightforward formalisation. For example, not knowing which set of decisions will be considered, it seems that the analyst cannot straightforwardly posit a measurement procedure $\proc{D}$ and accompanying variable $\RV{D}$ to represent the decision that will be made (refer to section \ref{sec:actions}).

Our aim in this paper is quite limited compared to the prospect of a formal analysis of a real causal inference problem like the one outlined above. We still think it is helpful to consider such an example, if only to make the point that the relationship between theories of causal inference and the real-world aims these theories serve can be subtle.

\section{Probabilistic models for decision making}\label{sec:seedo_models}

Probabilistic models have a long history of being used to represent decision problems. Decision problems feature, at a minimum, a collection of different decisions or actions that may be chosen, and some notion of preference over these decisions. The connection between choices and preferences is often indirect -- for example, choices may be associated with probability distributions over states of the world, and preferences may be defined on states of the world, which can then induce preferences over choices via the principle of expected utility.

There is a body of work that has produced a number of representation theorems showing that preferences that satisfy certain constraints must be representable by certain families of probabilistic models. Notable examples are the theorems of \citet{ramsey_truth_2016} and \citet{savage_foundations_1954} (held to be roughly equivalent), and the theorem of \citet{bolker_functions_1966,jeffrey_logic_1990}. A notable difference (among other differences) between the two is the fact that the former result represents preferences as a probability distribution over ``consequence functions'' from decisions to outcomes along with a ``utility function'' from outcomes to real numbers, while the latter result represents preferences as a probability distribution over propositions and a desirability function from propositions to real numbers. That is, Savage's theory gives decisions a special role in the represenation of preferences, while Jeffrey and Bolker's theory treats decisions as propositions just like any other proposition, and the fact that they can be chosen in some sense that other propositions cannot is a fact external to the preference representation.

Probability sets also feature in the literature of decision theory under the term ``imprecise probability''. One role that probability sets play is to represent the preferences of an agent who, presented with a bet, would rather not to take either side of it \citet{bradley_imprecise_2019,walley_statistical_1991}. More generally, probability sets can be used to represent an epistemic state of witholding judgement on certain propositions. Here, we use probability sets to represent a decision maker who, while considering which choice to make, may withold judgement about propositions regarding which choice they are going to make.

\subsection{Decision problems}

Suppose we have an observation process $\proc{X}$, modelled by $\RV{X}$ taking values in $X$ (we are \emph{informed}). Given an observation $x\in X$, we suppose that we can choose a decision from a known set $D$ (the set of decisions is \emph{transparent}), and we suppose that choosing a decision results in some action being taken in the real world. As with processes of observation, we will mostly ignore the details of what ``taking an action'' involves. The process of choosing a decision that yields an element of $D$ is a decision making process $\proc{D}$ modelled by $\RV{D}$. We might be able to introduce randomness to the choice, in which case the relation between $\RV{X}$ and $\RV{D}$ may be stochastic.  We will assume that there is some $\proc{Y}$ modelled by $\RV{Y}$ such that $(\RV{X},\RV{D},\RV{Y})$ tell us everything we want to know for the purposes of deciding which outcomes are better than others.

We want a model that allows us to compare different stochastic \emph{decision functions} $\kernel{Q}^{\RV{D}|\RV{X}}_\alpha:X\kto D$, letting $A$ be the set of all such functions available to be chosen. That is, we need a higher order function $f$ that takes a decision function $\kernel{Q}^{\RV{X}|\RV{D}}_\alpha$ and returns a probabilistic model of the consequences of selecting that decision function $\kernel{P}_\alpha^{\RV{DXY}}$. An order 2 model $(\kernel{P}_\square^{\RV{X},\prob{P}_\square^\RV{Y}|\RV{XD}},A)$ defines such a function, though there are many such functions that are not order 2 models. The key feature of probability gap models is that the map is by intersection of probability sets, so for example the conditional probability of $\RV{X}|\RV{D}$ given a decision function $\kernel{Q}^{\RV{X}|\RV{D}}_\alpha$ must actually be equal to $\kernel{Q}^{\RV{X}|\RV{D}}_\alpha$, and we can say the same for $\kernel{P}_\square^{\RV{X}}$ and $\prob{P}_\square^{\RV{Y}|\RV{XD}}$. If we don't think all of these conditional probabilities are fixed, then we want something other than an order 2 model of the type discussed. We will define \emph{ordinary decision problems} to be those for which the desired model $\prob{P}_\square$ is this type of order 2 probability gap model.

\todo[inline]{I think adding hypotheses at this point might make things unnecessarily confusing; on the other hand, they are useful for the connection to classical statistical decision theory. The "repeatable experiments" section shows how see-do models with certain assumptions induce an easier to understand class of hypotheses, and I could just save the idea of a hypothesis until I get there}

We consider an additional kind of gap in our probability model. The nature of this gap is: we don't know exactly which order 2 model $(\kernel{P}_\square^{\RV{X},\prob{P}_\square^\RV{Y}|\RV{XD}},A)$ we ``ought'' to use. To represent this gap we include an unobserved variable $\RV{H}$, the \emph{hypothesis}. We can interpret $\RV{H}$ as expressing the fac that, if we knew the value of $\RV{H}$ then we would know that our decision problem was represented by a unique order 2 model $(\kernel{P}_{h,\square}^{\RV{X},\prob{P}_{h,\square}^\RV{Y}|\RV{XD}},A)$. However, $\RV{H}$ is not known and in fact we do not know how to determine $\RV{H}$ (this is the nature of an \emph{unoserved} variable -- there is no process available to find the value it yields). Our model is thus given by $$(\kernel{P}_\square^{\RV{X}|\RV{H},\prob{P}_\square^\RV{Y}|\RV{HXD}},A)$$.

\begin{definition}[Ordinary decision problem]
An ordinary decision problem $(\prob{P},\Omega,\RV{H},(\RV{X},\proc{X}),(\RV{D},\proc{D}),(\RV{Y},\proc{Y}))$ consists of a fundamental probability set $\Omega$, hypotheses $\RV{H}:\Omega\to H$, observations $\RV{X}\Omega\to X$, decisions $\RV{D}:\Omega\to D$ and consequences $\RV{Y}:\Omega\to Y$, and the latter three random variables are associated with measurement processes. It is equipped with a probability gap model $\prob{P}:\Delta(D)^X\to \Delta(\Omega)^H$ where $\Delta(D)^X$ is the set of valid $\RV{D}|\RV{X}$ Markov kernels $X\kto D$ and $\Delta(\Omega)^H$ is the set of valid Markov kernels $H\kto \Omega$. We require of $\prob{P}$:
\begin{enumerate}
    \item $\kernel{P}_\alpha^{\RV{D}|\RV{X}}=\model{Q}_\alpha^{\RV{D}|\RV{X}}$ for all decision functions $\model{Q}_\alpha^{\RV{D}|\RV{X}}\in \Delta(D)^X$
    \item $\prob{P}^{\RV{X}|\RV{H}}=\model{P}_\alpha^{\RV{X}|\RV{H}}$ for all $\model{P}_\alpha:=\model{P}(\model{Q}_\alpha^{\RV{D}|\RV{X}})$
    \item $\prob{P}^{\RV{Y}|\RV{XDH}}=\model{P}_\alpha^{\RV{Y}|\RV{XDH}}$ for all $\model{P}_\alpha:=\model{P}(\model{Q}_\alpha^{\RV{D}|\RV{X}})$
\end{enumerate}
\end{definition}

(1) reflects the assumption that the ``probability of $\RV{D}$ given $\RV{X}$'' based on the induced model is equal to the ``probability of $\RV{D}$ given $\RV{X}$'' based on the chosen decision function. (2) reflects the assumption that the observations should be modelled identically no matter which decision function is chosen. (3) reflects the assumption that given hypothesis, the observations and the decision, the model of $\RV{Y}$ does not depend any further on the decision function $\alpha$.

Under these assumptions $\prob{P}_\square$ is an order 2 model $(\kernel{P}_\square^{\RV{X},\prob{P}_\square^\RV{Y}|\RV{XD}},A)$ which we call a ``see-do model''.\todo{I need to update the proof for this claim}

% \subsection{What should a probability model represent? Controversies about decision theories}

% There are some decision problems problems in which one or more of these assumptions may be unreasonable, and the correct way to approach to these problems is controversial. Newcomb's problem, for example, invites us to consider a problem where a second party has predicted our choice of decision function $\alpha$ before we have made it, and we have good reasons to believe this prediction is correct \citep{nozick_newcombs_1969}. \emph{The predictor} may then make choices that affect the consequences we expect to see, and this could mean that it is appropriate to consider models in which consequences to depend on $\alpha$ in addition to $\RV{D}$. The question of which kind of model \emph{should} be adopted in such a situation is controversial \citet{weirich_causal_2016,lewis_causation_1986}, and two prominent views on the correct answer are \emph{causal decision theory} and \emph{evidential decision theory}.

% This work does not propose normative rules for getting from a description of the world to a see-do model, and so the question of ``which decision theory?'' is not addressed here. We could ask if causal and evidential decision theory can be operationalised as different (vague) rules for getting from descriptions of the world to probability 2-combs $\model{P}^{\RV{X}|\RV{H}\square \RV{Y}|\RV{D}}$, but we leave this question open.

\subsection{Decisions as measurement procedures}

We have previously posited that observed variables are variables $\RV{X}$ -- themselves purely mathematical objects -- associated with a measurment process $\proc{X}$ that has ``one foot in the real world''. In the framework we have proposed here, decisions correspond to a special class of measurement procedure.

Suppose that we are only contemplating decision functions that map deterministically to $\RV{D}$. Suppose furthermore that we will $\RV{D}$ according to a model $\prob{P}_\square$, a utility function on $X\times D\times Y\to \mathbb{R}$ and a decision rule which is a function $f$ from models, utility functions and decision rules to decisions. Note that models, utility functions and decision rules are all well-defined mathematical objects. If we are confident that our choice will in the end be an element of a well-defined set of objects of the appropriate type, then we are positing that we have a ``measurement procedure'' $\proc{M}$ that yield models, utilities and decision rules. If so, $f\circ \proc{M}$ -- that is, the function that yields a decision -- is itself a measurement procedure. This is what is unique about decisions: proposing a complete decision problem with models, utilities and decision rules, defines a measurement procedure for decisions. Other quantities of interest do not seem to have this property -- we \emph{require} a measurement process for observations in order to make the whole setup work, but we do not \emph{define} it in the course of setting up a model for our decision problem.

\todo[inline]{I don't know how important this observation is, but the fact that $\proc{D}$ is an output of a formal decision making system makes it different from other things we might call decisions, and I wonder if I should call it something else in order to avoid ambiguity. The vague reason I think this matters is: whatever you might want to measure, you won't learn more about $\proc{D}$ from it than you already know once you have the model, the utility and the decision rule, this is not a property that other things we call ``decisons'' share and this distinction might be important regarding judgements of causal contractibilty.}

% \subsection{Unresponsiveness}

% Given a see-do model $\model{P}^{\RV{X}|\RV{H}\square \RV{Y}|\RV{D}}$, $\RV{A}\CI_{\prob{P}}^{2|*} \RV{D}|\RV{H}$ can be interpreted to express the property, given any hypothesis, $\RV{A}$ does not depend on $\RV{D}$ when we choose $\RV{D}$ randomly.

% This might sound like it expresses a property of ``causal independence''. However, it isn't quite satisfactory for this term. Consider $\RV{A}\in\{0,1\}$ representing the outcome of a fair coin toss, $\RV{D}\in\{0,1\}$ representing a bet on the coin toss and $\RV{B}\in\{0,1\}$ representing the outcome of the bet. There is one hypothesis -- ``the coin is fair'' -- and no observations. We construct a see-do model $\prob{P}$ in the obvious way given these assumptions. Then $\RV{A}\CI_{\prob{P}}^{2|*} \RV{D}|\RV{H}$ (``the coin toss is independent of the decision, for any decision rule'') and $\RV{B}\CI_{\prob{P}}^{2|*}\RV{D}|\RV{H}$ (``the outcome of the bet is independent of the decision, for any decision rule''). However, it is not the case that $(\RV{A},\RV{B})\CI_{\prob{P}}^{2|*}\RV{D}|\RV{H}$. Were both $\RV{A}$ and $\RV{B}$ causally independent of $\RV{D}$, then $(\RV{A},\RV{B})$ also ought to be independent of $\RV{D}$. This example is from \citet{heckerman_decision-theoretic_1995}.

% We will borrow the terminology of \emph{unresponsiveness} from \citet{heckerman_decision-theoretic_1995} to refer to independences like $\RV{A}\CI_{\prob{P}}^{2|*} \RV{D}|\RV{H}$; specifically, where a variable is independent of $\RV{D}$ given $\RV{H}$ under the restricted 2-comb $\model{P}^{\RV{X}|\RV{H}\framebox{*} \RV{Y}|\RV{D}}$.

\subsection{Causal models similar to see-do models}

\citet{lattimore_causal_2019} and \citet{lattimore_replacing_2019} consider an observational probability model and a collection of indexed interventional probability models, with the probability model tied to the interventional models by shared parameters. In these papers, they show how such a model can reproduce inferences made using Causal Bayesian Networks. This kind of model can be identified with a type of see-do model, where what we call hypotheses $\RV{H}$ are identified with the sequence of what Rohde and Lattimore call parameter variables.

The approach to decision theoretic causal inference described by \citet{dawid_decision-theoretic_2020} is somewhat different:

\begin{quote}
A fundamental feature of the DT approach is its consideration of the relationships between the various probability distributions that govern different regimes of interest. As a very simple example, suppose that we have a binary treatment variable $\RV{T}$, and a response variable $\RV{Y}$. We consider three different regimes [...] the first two regimes may be described as interventional, and the last as observational.
\end{quote}

The difference between the model described here and a see-do model is that a see-do model uses different variables $\RV{X}$ and $\RV{Y}$ to represent observations and consequences, while Dawid's model uses the same variable $(\RV{T},\RV{Y})$ to represent outcomes in interventional and observational regimes. In this work we associate one observed variable with each measurement process, while in Dawid's approach $(\RV{T},\RV{Y})$ seem to be doing double duty, representing mesurement processes carried out during observations and after taking action. This can be thought of as the causal analogue of the difference betwen saying we have a sequence $(\RV{X}_1,\RV{X}_2,\RV{X}_3)$ of observations independent and identically distributed according to $\mu\in \Delta(X)$ and saying that we have some observations distributed according to $\prob{P}^{\RV{X}}\in \Delta(X)$. People usually understand what is meant by the latter, but if one is trying to be careful the former is a more precise statement of the model in question.

\citet{heckerman_decision-theoretic_1995} also explore a decision theoretic approach to causal inference. Our approach is quite close to their approach if we identify what we call hypotheses with what they call states and allow for probabilistic dependence between states, decisions and consequences. It is an open question whether their notion of limited unresponsiveness corresponds to any notion of conditional independence in our work.

\citet{jacobs_causal_2019} has used a comb decomposition theorem to prove a sufficient identification condition similar to the identification condition given by \citet{tian2002general}. This theorem depends on the particular inductive hypotheses made by causal Bayesian networks.

\subsection{See-do models and classical statistics}

See-do models are capable of expressing the expected results of a particular choice of decision strategy, but they cannot by themselves tell us which strategies are more desirable than others. To do this, we need some measure of the desirability of our collection of results $\{\prob{P}_\alpha|\alpha\in A\}$. A common way to do this is to employ the principle of expected utility. The classic result of \citet{von_neumann_theory_1944} shows that all preferences over a collection of probability models that obey their axioms of completeness, transitivity, continuity and independence of irrelevant alternatives must be able to be expressed via the principle of expected utility. This does not imply that anyone knows what the appropriate utility function is.

A further property that may hold for some see-do models $\prob{P}^{\RV{X}|\RV{H}\square \RV{Y}|\RV{D}}$ is $\RV{Y}\CI^2_{\prob{P}} \RV{X}|(\RV{H},\RV{D})$. This expresses the view that the consequences are independent of the observations, once the hypothesis and the decision are fixed. Such a situation could hold in our scenario above, where the observations are trial data, the decisions are recommendations to care providers and the consequences are future patient outcomes. In such a situation, we might suppose that the trial data are informative about the consequences only via some parameter such as effect size; if the effect size can be deduced from $\RV{H}$ then our assumption corresponds to the conditional independence above.

Given a see-do model $\prob{P}^{\RV{X}|\RV{H}\square \RV{Y}|\RV{D}}$ along with the principle of expected utility to evaluate strategies, and the assumption $\RV{Y}\CI^2_{\prob{P}} \RV{X}|(\RV{H},\RV{D})$ we obtain a statistical decision problem in the form introduced by \citet{wald_statistical_1950}.

A \emph{statistical model} (or \emph{statistical experiment}) is a collection of probability distributions $\{\prob{P}_\theta\}$ indexed by some set $\Theta$. A statistical decision problem gives us an observation variable $\RV{X}:\Omega\kto X$ and a statistical experiment $\{\prob{P}^{\RV{X}}_\theta\}_\Theta$, a decision set $D$ and a loss $l:\Theta\times D\to \mathbb{R}$. A strategy $\model{S}^{\RV{D}|\RV{X}}_\alpha$ is evaluated according to the risk functional $R(\theta,\alpha):=\sum_{x\in X}\sum_{d\in D} \prob{P}^{\RV{X}}_\theta(x) S^{\RV{D}|\RV{X}}_\alpha (d|x) l(h,d)$. A strategy $\model{S}^{\RV{D}|\RV{X}}_\alpha$ is considered more desirable than $\model{S}^{\RV{D}|\RV{X}}_\beta$ if $R(\theta,\alpha)<R(\theta,\beta)$.

Suppose we have a see-do model $\prob{P}^{\RV{X}|\RV{H}\square \RV{Y}|\RV{D}}$ with $\RV{Y}\CI_{\model{P}} \RV{X}|(\RV{H,D})$, and suppose that the random variable $\RV{Y}$ is a ``negative utility'' function taking values in $\mathbb{R}$ for which \emph{low} values are considered desirable. Define a loss $l:H\times D\to \mathbb{R}$ by $l(h,d) = \sum_{y\in \mathbb{R}} y\model{P}^{\RV{Y}|\RV{H}\RV{D}}(y|h,d)$, we have 

\begin{align}
    \mathbb{E}_{\model{P}_{\alpha}}[\RV{Y}|h] &= \sum_{x\in X} \sum_{d\in D} \sum_{y\in Y} \model{P}^{\RV{X}|\RV{H}}(x|h) \model{Q}_\alpha^{\RV{D}|\RV{X}}(d|x) \model{P}^{\RV{Y}|\RV{HD}}(y|h,d)\\
    &= \sum_{x\in X} \sum_{d\in D} \model{P}^{\RV{X}|\RV{H}}(x|h) \model{Q}_\alpha^{\RV{D}|\RV{X}}(d|x) l(h,d)\\
    &= R(h,\alpha)
\end{align}

If we are given a see-do model where we interpret $\{\model{P}^{\RV{X}|\RV{H}}(\cdot|h)|h\in H\}$ as a statistical experiment and $\RV{Y}$ as a negative utility, the expectation of the utility under the strategy forecast given in equation \ref{eq:see_do_query} is the risk of that strategy under hypothesis $h$.



\subsection{Randomness pushback}\label{sec:curry}

Given a function $f:X\times Y\to Z$, we can obtain a curried version $\lambda f:Y\to Z^X$. In particular, if $Y=\{*\}$ then $\lambda f:\{*\}\to Y^X$. At least for countable $X$, we can apply this construction to Markov kernels: given a kernel $\kernel{K}:X\kto Y$, define $\kernel{L}: \{*\}\kto Y^X$ by 
\begin{align}
    \lambda \kernel{K} ((y_i)_{i\in X}) &= \prod_{i\in X} \kernel{K}(y_i|i)
\end{align}

We can then define an evaluation map $\text{ev}:Y^X\times X\to Y$ by $\text{ev}((y_i)_{i\in X},x)=y_x$. Then

\begin{align}
    \kernel{K} &= \tikzfig{curry_kernel_definition} \label{eq:curry_identity}\\
    &\iff\\
    \kernel{K}(A|x) &= \int_{Y^X} \delta_{\text{ev}(y^X,x)}(A) \kernel{L}(\mathrm{d}y^X|x)
\end{align}

Unlike the case of function currying, $\lambda \kernel{K}$ is not the unique Markov kernel for which \ref{eq:curry_identity} holds. In fact, we can substitute any $\kernel{M}$ such that, for any $i\in X$

\begin{align}
    \sum_{y_{\{i\}^C\in Y^{|X|-1}}} \kernel{M}((y_i)_{i\in X}) = \kernel{K}(y_i|i)
\end{align}

This representation of a Markov kernel is called a \emph{randomness pushback} by \citet{fritz_synthetic_2020}.

\subsection{Connections to potential outcomes}

Randomness pushbacks have a few features in common with potential outcomes causal models. For our purposes, we will say a potential outcomes model is a probability set $(\Omega,\sigalg{F},\prob{P}_{\{\}})$ along with variables $\RV{X}$, $\RV{Y}$, $\RV{Y}^X$ such that 

\begin{align}
    \prob{P}_{\{\}}^{\RV{Y}|\RV{XY}^X} &= \kernel{F}_{\text{ev}}
\end{align}

More commonly, this property is expressed as

\begin{align}
    \RV{Y}\overset{a.s.}{=}\text{ev}(\RV{X},\RV{Y}^X)
\end{align}

We consider a potential outcomes model to be a probability set here, but we can formally recover a ``traditional'' potential outcomes model by considering probability sets of size $1$.

If we additionally have the existence of $\prob{P}_{\{\}}^{\RV{Y}^X|\RV{X}}$ and $\RV{Y}^\RV{X}\CI_{\prob{P}_{\{\}}} \RV{X}$ then 

\begin{align}
    \prob{P}_{\{\}}^{\RV{Y}|\RV{X}} &= \tikzfig{curry_kernel_copied}\label{eq:curry_identity_po}
\end{align}

Equation \ref{eq:curry_identity_po} is clearly a version of \ref{eq:curry_identity}. As we have established, provided $\prob{P}_{\{\}}^{\RV{Y}|\RV{X}}$ exists, we can always introduce some variable $\RV{Y}^X$ and corresponding $\prob{P}_{\{\}}^{\RV{Y}^X}$ such that Equation \ref{eq:curry_identity_po} holds.

\todo[inline]{I feel this is a bit confusing, but I'm struggling to come up with something better}

The point that we want to make is that representing some conditional probability with a randomness pushback is not itself at all special - in fact, it's possible to do it with any conditional probability. We suspect that more important than questions about the choice of representation is the question: what is the conditional probability that is being represented? That is, what is the measurement procedure that gives us $\proc{X}$ and $\proc{Y}$ which we model with the random variables $\RV{X}$ and $\RV{Y}$?

A similar view is expressed by \citet{dawid_causal_2000} and \citet{richardson2013single}, who both indicate that avoiding assumptions that cannot be tested by an ideal experiment is a desirable feature of a causal model. \citet{richardson2013single} consider models that make use of the potential outcomes approach that go beyond Equation \ref{eq:curry_identity_po}, but a Single World Intervention Graph (SWIG) fundamentally represents a collection of probability measures indexed by different values of a possible intervention. If we suppose that deterministically choosing an intervention $x$ yields the probability model corresponding to the $x$-indexed SWIG, and allow that we can have measurement procedures that give us intervention values (Section \label{sec:actions}), then such an indexed collection of probability models is a conditional probability -- namely, the probability of outcomes conditional on interventions.

The question we want to ask is: in a setting where we can make choices that ``probabilistically determine $\RV{X}$'', when does a conditional probability $\prob{P}_{\{\}}^{\RV{Y}|\RV{X}}$ exist?
