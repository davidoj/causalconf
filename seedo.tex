%!TEX root = main.tex

\section{Decision theoretic causal inference}\label{sec:seedo_models}

People very often have to make decisions with some information they may consult to help them make the decision. We are going to examine how gappy probability models can formally represent problems of this type, which in turn allows us to make use of the theory of probability to help guide us to a good decision. Probabilistic models have a long history of being used to represent decision problems, and there exist a number of coherence theorems that show that preferences that satisfy certain kinds of constraints must admit representation by a probability model and a utility function of the appropriate type. Particularly noteworthy are the theorems of \citet{ramsey_truth_2016} and \citet{savage_foundations_1954}, which together yield a method for representing decision problems known as ``Savage decision theory'', and the theorem of \citet{bolker_functions_1966,jeffrey_logic_1990} which yields a rather different method for representing decision problems known as ``evidential decision theory''. \citet{joyce_foundations_1999} extends Jeffrey and Bolker's result to a representation theorem that subsumes both ``causal decision theory'' and ``evidential decision theory''.

None of these representation theorems explicitly concern themselves with probability gaps. One may try to find gappy probability models inside the theories, for example in the way that the Savage theory extends a probability distribution over \emph{states} to a probability distribution over \emph{consequence} when given an \emph{act}. However, the connection to probability gaps is not explicit and may not follow precisely. We could make similar comments about causal or evidential decision theories: perhaps ``causal conditionals'' are gappy probability models, but perhaps they aren't exactly.

We do not have a comparable axiomatisation of preferences that yield a representation of decision problems in terms of utility and gappy probability. Such an undertaking could potentially clarify some choices that can be made in setting up a gappy probability model of decision making, but it is the subject of future work. Instead, we suppose that we are satisfied with a particular probabilistic model of a decision problem, a supposition based on convention rather than axiomatisation.

\subsection{Decision problems}

We will consider a particular kind of decision problem. It has the following characteristics:
\begin{itemize}
    \item We are given some observation to help us make a better decision; the problem is an \emph{informed} decision problem
    \item We select decisions from a set that is known at the outset; the set of possible decisions is \emph{transparent}
    \item The consequences are linearly dependent on the chosen decision function ; the problem is an \emph{ordinary} decision problem
\end{itemize}

In more detail, suppose we have an observation process $\proc{X}$, modelled by $\RV{X}$ taking values in $X$ (we are \emph{informed}). Given an observation $x\in X$, we suppose that we can choose a decision from a known set $D$ (the set of decisions is \emph{transparent}), and we suppose that choosing a decision results in some action being taken in the real world. As with processes of observation, we will mostly ignore the details of what ``taking an action'' involves. The process of choosing a decision that yields an element of $D$ is a decision making process $\proc{D}$ modelled by $\RV{D}$. We might be able to introduce randomness to the choice, in which case the relation between $\RV{X}$ and $\RV{D}$ may be stochastic.  We will assume that there is some $\proc{Y}$ modelled by $\RV{Y}$ such that $(\RV{X},\RV{D},\RV{Y})$ tell us everything we want to know for the purposes of deciding which outcomes are better than others.

We want a model that allows us to compare different stochastic \emph{decision functions} $\kernel{P}^{\RV{X}|\RV{D}}_\alpha:X\kto D$. What we mean by compare is, precisely: we need a function that takes a decision function $\kernel{P}^{\RV{X}|\RV{D}}_\alpha$ and returns a probabilistic model of consequences $\kernel{P}^{\RV{DXY}}$. As we have seen in the previous chapter, if we further require that this function is \emph{linear}, what we have is a model with a second order gap for $\RV{X}|\RV{D}$; that is, we have a 2-comb $\kernel{P}^{\RV{X}\square \RV{Y}|\RV{D}}$. If we have linear dependence, we call the result an \emph{ordinary} decision problem.

We will actually consider a slightly more general kind of model than this. Another common feature of probabilistic modelling is to distinguish two types of uncertainty -- roughly, uncertainty that is represented by a probability model, and uncertainty over which probability model we should adopt. Uncertainty of the second type 

For different observations and decisions we will generally expect different consequences. We will assume that we expect the same observations whatever strategy we choose. We will also assume that given the same observations and the same decision, we expect the same consequences regardless of the strategy. These assumptions rule out certain classes of decision problem where, for example, there is controversy over whether the strategy chosen should depend on the time at which it is chosen \citet{weirich_causal_2016,lewis_causal_1981,paul_f_christiano_edt_2018}.

We will entertain a collection of probabilistic models to represent postulated relationships between $\RV{X}$, $\RV{D}$ and $\RV{Y}$ for each strategy $\alpha$; to do this, we will introduce a latent variable $\RV{H}$ such that each value of $\RV{H}$ corresponds to a particular probabilistic model of $\RV{X}$, $\RV{D}$ and $\RV{Y}$. Concretely, for each strategy $\alpha$ our forecast will be represented by a probability model $\kernel{P}_{\alpha}:\RV{I}\kto (\RV{H},\RV{X},\RV{D},\RV{Y})$. We assume that -- holding the hypothesis fixed -- the same observations are expected whatever strategy we choose: $\model{P}_{\alpha}^{\RV{X}|\RV{H}}=P_{\beta}^{\RV{X}|\RV{H}}$ for all $\alpha,\beta$. We assume that under each hypothesis, the decision chosen is always modelled by the the chosen strategy: $\model{P}_{\alpha}^{\RV{D}|\RV{HX}}=\kernel{S}_\alpha \otimes \text{erase}_{\RV{H}}$. Finally, we assume that, holding the hypothesis fixed, the same consequences are expected under any strategy given the same observations and the same decision: $\model{P}_{\alpha}^{\RV{Y}|\RV{XHD}}=P_{\beta}^{\RV{Y}|\RV{XHD}}$ for all $\alpha,\beta$.

Under these assumptions, there exists a ``see-do model'' $\model{T}^{\RV{XY}|\RV{HD}}$ such that $\RV{X}\CI_{\model{T}}\RV{D}|\RV{H}$ and for all $\alpha$, 

\begin{align}
    \model{P}_{\alpha} = \tikzfig{seedo} \label{eq:see_do_query}
\end{align}

The proof is given in Appendix \ref{sec:see-do-rep}. Note that $\model{T}^{\RV{X}|\RV{H}}$ exists by virtue of the fact $\RV{X}\CI_{\model{T}}\RV{D}|\RV{H}$. 

We will call the the see-do model along with the collection of strategies $\{\model{T}^{\RV{XY}|\RV{HD}},\{\model{S}_\alpha|\alpha\in \mathscr{A}\}\}$ a \emph{standard decision problem}.

\subsection{Combs}

The conditional independence $\RV{X}\CI_{\model{T}}\RV{D}|\RV{H}$ of $\model{T}$ is the property that allows us to write Equation \ref{eq:see_do_query}, but it also implies that $\model{T}$ is \emph{not} a submodel of $\prob{P}_\alpha$ for most strategies $\alpha$, because for most such strategies $\RV{X}$ and $\RV{D}$ are not independent. Instead, $\model{T}$ is a \emph{comb}. This structure was introduced by \citet{chiribella_quantum_2008} in the context of quantum circuit architecture, and \citet{jacobs_causal_2019} adapted the concept to causal modelling.

We don't formally define any special operations with combs here, but because they come up multiple times we will explain the notion a little. A comb is a Markov kernel with an ``insert'' operation; to obtain the probability model associated with a particular strategy, we ``insert'' the strategy into our see-do model.

\begin{align}
\model{T} &= \begin{tikzpicture}
    \path (0,0) node (H) {$\RV{H}$}
     ++ (0.5,0) node[copymap] (copy0) {}
     ++ (0.5,0) node[kernel] (XH) {$\model{T}^{\RV{X}|\RV{H}}$}
     ++ (0.5,0) node[copymap] (copy1) {}
     ++ (0.5,0) node (X) {$\RV{X}$}
     ++ (0.5,0.) node (D) {$\RV{D}$}
     ++ (0.7,-0.15) node[kernel,inner sep=5pt] (YDXH) {$\model{T}^{\RV{Y}|\RV{XDH}}$}
     ++ (0.7,0.15) node (Y) {$\RV{Y}$};
     \draw (H) -- (XH) -- (X) (D) to [out=0,in=180] ($(YDXH.west) + (0,0.15)$) ($(YDXH.east) + (0,0.15)$) -- (Y);
     \draw (copy0) to [out=-90,in=180] ($(copy0.east) + (0.8,-0.5)$) to [out=0,in=180] ($(YDXH.west) + (0,-0.2)$) (copy1) to [out=-90,in=180] ($(YDXH.west) + (0,-0.05)$);
\end{tikzpicture}\\
&= \begin{tikzpicture}
    \path (0,0) node (H) {$\RV{H}$}
     ++ (0.7,0) node (XH) {$\model{T}$}
     ++ (0.7,0) node (X) {$\RV{X}$}
     ++ (0.5,0) node (D) {$\RV{D}$}
     ++ (0.5,0) node[inner sep=4pt] (YDXH) {}
     ++ (0.5,0) node (Y) {$\RV{Y}$};
     \draw (H) -- (XH) -- (X) (D) -- (YDXH) -- (Y);
     \draw ($(XH.west) + (0,0.2)$) -- ($(XH.west) + (0,-0.6)$) -- ($(YDXH.east) + (0,-0.6)$)
     -- ($(YDXH.east) + (0,0.2)$) -- ($(YDXH.west) + (0,0.2)$) -- ($(YDXH.west) + (0,-0.4)$)
     -- ($(XH.east) + (0,-0.4)$) -- ($(XH.east) + (0,0.2)$) -- ($(XH.west) + (0,0.2)$);
\end{tikzpicture}\label{eq:kernel_with_hole}
\end{align}

A key feature of a comb is that a strategy can be chosen such that $\RV{D}$ is independent of any variable on the ``upper arm'' ($\RV{X}$ in this example) conditional on $\RV{H}$. There is an intuitive appeal to the notion that, with access to a randomiser, we could if we wanted to choose a decision independent of all of our observations. We may wish to introduce additional variables that we do not observe, but we can nonetheless choose $\RV{D}$ independent of them. Such variables we will call \emph{pre-choice variables}

\begin{definition}[Pre-choice variable]
Given a see-do model $\model{T}$, $\RV{W}$ is a pre-choice variable iff for every other pre-choice variable $\RV{V}$, $(\RV{W},\RV{V})\CI_{\model{T}} \RV{D}|\RV{H}$. The hypothesis $\RV{H}$ is always a pre-choice variable, and we also assume the same is true of the observation $\RV{X}$.
\end{definition}

Given that $\RV{H}$ is necessarily a pre-choice variable, we wonder if it may be possible to define a hypothesis $\RV{H}$ such that all pre-choice variables are functions of it. This would reduce the number of different elements of our theory, as we would no longer distinguish between ``hypotheses'' and ``pre-choice variables''. The reason why we have not done so thus far is that hypotheses are motivated by classical statistics while pre-choice variables are motivated by approaches to causal inference, and we haven't yet investigated whether the two can be identified without losing anything important.

\citet{lattimore_causal_2019} and \citet{lattimore_replacing_2019} describe a novel approach to causal inference: they consider an observational probability model and a collection of indexed interventional probability models, with the probability model tied to the interventional models by shared parameters. In these papers, they show how such a model can reproduce inferences made using Causal Bayesian Networks. This kind of model is very close to a type of see-do model, where we identify the hypotheses $\RV{H}$ with the parameter variables in that work. The only difference is that we consider interventional maps (see-do models represent a map $(\RV{D},\RV{H})\kto \RV{Y}$) rather than interventional probability models, and this is a superficial difference as an indexed collection of probability models is a map.

\citet{dawid_decision-theoretic_2020} describes a different version of a decision theoretic approach to causal inference:

\begin{quote}
A fundamental feature of the DT approach is its consideration of the relationships between the various probability distributions that govern different regimes of interest. As a very simple example, suppose that we have a binary treatment variable $\RV{T}$, and a response variable $\RV{Y}$. We consider three different regimes [...] the first two regimes may be described as interventional, and the last as observational.
\end{quote}

This is somewhat different to a see-do model, as it features a probabilistic model that uses the same random variables $\RV{T}$ and $\RV{Y}$ to represent both interventional and observational regimes, while a see-do model uses different random variables. This difference can be thought of as the difference between positing a sequence $(\RV{X}_1,\RV{X}_2,\RV{X}_3)$ distributed according to $\prob{P}^{\RV{X}}$, or saying that the $\RV{X}_i$ are distributed according to $\prob{P}$ such that they are mutually independent ($i\not\in A\subset[3]\implies \RV{X}_i\CI_{\prob{P}} (\RV{X}_j)_{j\in A}$) and identically distributed ($\prob{P}^{\RV{X}_i}=\prob{P}^{\RV{X}_j}$ for all $i,j$). The former can be understood as a shorthand of the latter, but because in this paper we are particularly interested in problems that arise regarding the relation between the map and the territory, we favour the second approach because it is more explicit.

\citet{jacobs_causal_2019} has used a comb decomposition theorem to prove a sufficient identification condition similar to the identification condition given by \citet{tian2002general}. This theorem depends on the particular inductive hypotheses made by causal Bayesian networks.

\subsection{See-do models and classical statistics}

See-do models are capable of expressing the expected results of a particular choice of decision strategy, but they cannot by themselves tell us which strategies are more desirable than others. To do this, we need some measure of the desirability of our collection of results $\{\prob{P}_\alpha|\alpha\in A\}$. A common way to do this is to employ the principle of expected utility. The classic result of \citet{von_neumann_theory_1944} shows that all preferences over a collection of probability models that obey their axioms of completeness, transitivity, continuity and independence of irrelevant alternatives must be able to be expressed via the principle of expected utility. This does not imply that anyone knows what the appropriate utility function is.

We introduced the hypothesis $\RV{H}$ as a latent variable to allow us to postulate multiple different models of obsevations, decisions and consequences. In general, both the hypothesis and the observation $\RV{X}$ may influence our views about the consequences $\RV{Y}$ that are likely to follow from a given decision. It is very common to model sequences of observations as independent and identically distributed given some parameter or latent variable. In such cases, we can identify $\RV{H}$ with this latent variable (our setup does not preclude introducing a prior over $\RV{H}$, nor does it require it). Furthermore, in such cases where we have a collection of $\RV{X}_i$ such that $\RV{X}_i\CI_{\model{T}} \RV{X}_j|\RV{H}$, it may be reasonable to expect that $\RV{Y} \CI_{\RV{T}} \RV{X}|\RV{H}$ also. In fact, this is the standard view in causal modelling -- given ``the probability distribution over observations'' (which is to say, conditional on $\RV{H}$), interventional distributions have no additional dependence on \emph{particular} observations. We can find exceptions with questions like ``given what actually happened, what would have happened if a different action had been taken?'' \citep{pearl_causality:_2009,tian_probabilities_2000,mueller_causes_2021}, but this is not the kind of question we are considering here.

Given these two choices -- to use the principle of expected utility to evaluate strategies, and to use a see-do model $\model{T}$ with the conditional independence $\RV{Y}\CI_{\model{T}} \RV{X}|\RV{H,D}$ -- we obtain a statistical decision problem in the form introduced by \citet{wald_statistical_1950}.

A \emph{statistical model} (or \emph{statistical experiment}) is a collection of probability distributions $\{\prob{P}_\theta\}$ indexed by some set $\Theta$. A statistical decision problem gives us an observation variable $\RV{X}:\Omega\kto X$ and a statistical experiment $\{\prob{P}^{\RV{X}}_\theta\}_\Theta$, a decision set $D$ and a loss $l:\Theta\times D\to \mathbb{R}$. A strategy $\model{S}^{\RV{D}|\RV{X}}_\alpha$ is evaluated according to the risk functional $R(\theta,\alpha):=\sum_{x\in X}\sum_{d\in D} \prob{P}^{\RV{X}}_\theta(x) S^{\RV{D}|\RV{X}}_\alpha (d|x) l(h,d)$. A strategy $\model{S}^{\RV{D}|\RV{X}}_\alpha$ is considered more desirable than $\model{S}^{\RV{D}|\RV{X}}_\beta$ if $R(\theta,\alpha)<R(\theta,\beta)$.

Suppose we have a see-do model $\model{T}^{\RV{X}\RV{Y}|\RV{HD}}$ with $\RV{Y}\CI_{\model{T}} \RV{X}|(\RV{H,D})$, and suppose that the random variable $\RV{Y}$ is a ``reverse utility'' function taking values in $\mathbb{R}$ for which low values are considered desirable. Then, defining a loss $l:H\times D\to \mathbb{R}$ by $l(h,d) = \sum_{y\in \mathbb{R}} y\model{T}^{\RV{Y}|\RV{H}\RV{D}}(y|h,d)$, we have 

\begin{align}
    \mathbb{E}_{\model{P}_{\alpha}}[\RV{Y}|h] &= \sum_{x\in X} \sum_{d\in D} \sum_{y\in Y} \model{T}^{\RV{X}|\RV{H}}(x|h) \model{S}_\alpha^{\RV{D}|\RV{X}}(d|x) \model{T}^{\RV{Y}|\RV{HD}}(y|h,d)\\
    &= \sum_{x\in X} \sum_{d\in D} \model{T}^{\RV{X}|\RV{H}}(x|h) \model{S}_\alpha ^{\RV{D}|\RV{X}}(d|x) l(h,d)\\
    &= R(h,\alpha)
\end{align}

If we are given a see-do model where we interpret $\model{T}^{\RV{X}|\RV{H}}$ as a statistical experiment and $\RV{Y}$ as a reversed utility, the expectation of the utility under the strategy forecast given in equation \ref{eq:see_do_query} is the risk of that strategy under hypothesis $h$.

