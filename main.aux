\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand*\HyPL@Entry[1]{}
\citation{pearl_causality:_2009}
\citation{rubin_causal_2005}
\HyPL@Entry{0<</S/D>>}
\citation{korzybski_science_1933}
\citation{hernan_does_2008}
\citation{pearl_does_2018}
\citation{shahar_association_2009}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}{section.1}\protected@file@percent }
\citation{feynman_feynman_1979}
\@writefile{toc}{\contentsline {section}{\numberline {2}Variables and Probability Models}{3}{section.2}\protected@file@percent }
\newlabel{sec:vague_variables}{{2}{3}{Variables and Probability Models}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Why are variables functions?}{3}{subsection.2.1}\protected@file@percent }
\newlabel{sec:variables}{{2.1}{3}{Why are variables functions?}{subsection.2.1}{}}
\citation{pearl_causality:_2009}
\citation{menger_random_2003}
\newlabel{eq:def_X}{{2}{5}{Why are variables functions?}{equation.2.2}{}}
\newlabel{eq:def_Y}{{3}{5}{Why are variables functions?}{equation.2.3}{}}
\citation{pearl_causality:_2009}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Probability, variables and composition}{6}{subsection.2.2}\protected@file@percent }
\newlabel{eq:truncated_fac}{{4}{6}{Probability, variables and composition}{equation.2.4}{}}
\citation{shahar_association_2009}
\citation{fritz_synthetic_2020,cho_disintegration_2019}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Probability and composition without variables: Markov categories}{7}{subsection.2.3}\protected@file@percent }
\citation{fritz_synthetic_2020,cho_disintegration_2019,fong_causal_2013}
\citation{selinger_survey_2010}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Truncated factorisation with Markov kernels}{9}{subsection.2.4}\protected@file@percent }
\newlabel{eq:tfac_setted}{{13}{9}{Truncated factorisation with Markov kernels}{equation.2.13}{}}
\newlabel{eq:double_label}{{14}{9}{Truncated factorisation with Markov kernels}{equation.2.14}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Composition and probability with variables}{10}{subsection.2.5}\protected@file@percent }
\citation{hajek_what_2003}
\newlabel{lem:nocopy1}{{2.6}{12}{Output copies of the same variable are identical}{theorem.2.6}{}}
\newlabel{lem:nocopy2}{{2.7}{12}{Copies shared between input and output are identical}{theorem.2.7}{}}
\newlabel{lem:uniq_model}{{2.8}{13}{Uniqueness of models with the sample space as a domain}{theorem.2.8}{}}
\newlabel{def:pairing}{{2.9}{14}{Pairing}{theorem.2.9}{}}
\newlabel{lem:pushforward}{{2.10}{14}{Pushforward model}{theorem.2.10}{}}
\citation{constantinou_extended_2017}
\newlabel{corr:pushforward}{{2.11}{15}{Pushforward probability model}{theorem.2.11}{}}
\newlabel{lem:var_indep}{{2.13}{15}{Consistency via variation conditional independence}{theorem.2.13}{}}
\@writefile{tdo}{\contentsline {todo}{I think Lemmas \ref  {lem:nocopy1} and \ref  {lem:nocopy2} might be sufficient to offer diagrammatic checks of consistency if all variables that are not identical are variation independent. This is probably an interesting result, but I'm not sure if it's a higher priority than filling out the rest of the content.}{15}{section*.2}\protected@file@percent }
\pgfsyspdfmark {pgfid33}{20088093}{22845130}
\newlabel{lem:avoid_contradic}{{2.14}{15}{Consistency via positive models}{theorem.2.14}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6}Truncated factorisation with variables}{16}{subsection.2.6}\protected@file@percent }
\newlabel{eq:tfac_labeled}{{55}{16}{Truncated factorisation with variables}{equation.2.55}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7}Sample space models and submodels}{16}{subsection.2.7}\protected@file@percent }
\newlabel{lem:ss_exist}{{2.15}{16}{Existence of sample space model}{theorem.2.15}{}}
\newlabel{def:submodel}{{2.17}{17}{Submodel}{theorem.2.17}{}}
\newlabel{eq:submodel}{{61}{17}{Submodel}{equation.2.61}{}}
\newlabel{lem:subm_exist}{{2.18}{17}{Submodel existence}{theorem.2.18}{}}
\citation{fritz_synthetic_2020}
\citation{constantinou_extended_2017}
\citation{cho_disintegration_2019}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.8}Conditional independence}{18}{subsection.2.8}\protected@file@percent }
\newlabel{ssec:cond_indep}{{2.8}{18}{Conditional independence}{subsection.2.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Decision theoretic causal inference}{18}{section.3}\protected@file@percent }
\newlabel{sec:seedo_models}{{3}{18}{Decision theoretic causal inference}{section.3}{}}
\citation{weirich_causal_2016,lewis_causal_1981,paul_f_christiano_edt_2018}
\citation{chiribella_quantum_2008}
\citation{jacobs_causal_2019}
\newlabel{eq:see_do_query}{{70}{19}{Decision theoretic causal inference}{equation.3.70}{}}
\citation{lattimore_causal_2019}
\citation{lattimore_replacing_2019}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Combs}{20}{subsection.3.1}\protected@file@percent }
\newlabel{eq:kernel_with_hole}{{72}{20}{Combs}{equation.3.72}{}}
\citation{dawid_decision-theoretic_2020}
\citation{jacobs_causal_2019}
\citation{tian2002general}
\citation{von_neumann_theory_1944}
\citation{pearl_causality:_2009,tian_probabilities_2000,mueller_causes_2021}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}See-do models and classical statistics}{21}{subsection.3.2}\protected@file@percent }
\citation{wald_statistical_1950}
\citation{pearl_causality:_2009}
\@writefile{toc}{\contentsline {section}{\numberline {4}Causal Bayesian Networks}{22}{section.4}\protected@file@percent }
\newlabel{sec:CBN}{{4}{22}{Causal Bayesian Networks}{section.4}{}}
\newlabel{eq:truncated_fac2}{{76}{23}{Causal Bayesian Networks}{equation.4.76}{}}
\newlabel{eq:what_is_p}{{77}{23}{Causal Bayesian Networks}{equation.4.77}{}}
\newlabel{eq:what_is_do}{{78}{23}{Causal Bayesian Networks}{equation.4.78}{}}
\newlabel{eq:truncated_fac3}{{79}{23}{Causal Bayesian Networks}{equation.4.79}{}}
\newlabel{eq:cbn_maybe_comb}{{82}{24}{Causal Bayesian Networks}{equation.4.82}{}}
\newlabel{eq:cbn_2comb}{{83}{24}{Causal Bayesian Networks}{equation.4.83}{}}
\newlabel{eq:consistent}{{85}{25}{Causal Bayesian Networks}{equation.4.85}{}}
\@writefile{tdo}{\contentsline {todo}{I think reusing the same $\ensuremath  {\mathsf  {H}}$ between $\ensuremath  {\mathsf  {U}}$ and $\ensuremath  {\mathsf  {T}}$ is a mistake here. Maybe not a big problem, but ideally one would check!}{25}{section*.3}\protected@file@percent }
\pgfsyspdfmark {pgfid60}{20088093}{32933643}
\newlabel{th:seedo_rep}{{4.1}{25}{}{theorem.4.1}{}}
\newlabel{eq:not_indep}{{98}{26}{Causal Bayesian Networks}{equation.4.98}{}}
\citation{heckerman_decision-theoretic_1995}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Proxy control}{27}{subsection.4.1}\protected@file@percent }
\citation{pearl_does_2018}
\@writefile{toc}{\contentsline {section}{\numberline {5}Potential outcomes}{28}{section.5}\protected@file@percent }
\newlabel{sec:potential_outcomes}{{5}{28}{Potential outcomes}{section.5}{}}
\newlabel{def:potential_outcomes}{{5.2}{29}{Potential outcomes: formal requirement}{theorem.5.2}{}}
\newlabel{lem:po_triviality}{{5.3}{29}{Trivial formal potential outcomes}{theorem.5.3}{}}
\newlabel{def:po_whatmodel}{{5.4}{29}{What potential outcomes model: counterfactual extension}{theorem.5.4}{}}
\citation{rubin_causal_2005}
\newlabel{eq:po_probmodel}{{112}{30}{Potential outcomes}{equation.5.112}{}}
\newlabel{lem:selector}{{5.5}{30}{Selector kernel}{theorem.5.5}{}}
\newlabel{eq:multiplex2}{{113}{30}{Selector kernel}{equation.5.113}{}}
\newlabel{eq:multiplex1}{{115}{30}{Selector kernel}{equation.5.115}{}}
\citation{hernan_does_2008}
\newlabel{def:po_dte}{{5.6}{31}{}{theorem.5.6}{}}
\newlabel{eq:different_ys}{{116}{31}{}{equation.5.116}{}}
\newlabel{eq:po_exist_con1}{{119}{31}{}{equation.5.119}{}}
\newlabel{eq:po_exist_con2}{{123}{32}{}{equation.5.123}{}}
\@writefile{tdo}{\contentsline {todo}{I think I asked the wrong question here -- should've asked when I can extend a see-do model with additonal pre-choice variables. I think it's possible to always choose some deterministic potential outcomes.}{32}{section*.4}\protected@file@percent }
\pgfsyspdfmark {pgfid79}{20088093}{29241510}
\newlabel{eq:po_proxycontrol}{{124}{32}{}{equation.5.124}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Appendix:see-do model representation}{32}{section.6}\protected@file@percent }
\newlabel{sec:see-do-rep}{{6}{32}{Appendix:see-do model representation}{section.6}{}}
\@writefile{tdo}{\contentsline {todo}{Update notation}{32}{section*.5}\protected@file@percent }
\pgfsyspdfmark {pgfid80}{20088093}{8376220}
\newlabel{th:see_do_rep}{{6.1}{33}{See-do model representation}{theorem.6.1}{}}
\newlabel{eq:disint}{{125}{33}{Appendix:see-do model representation}{equation.6.125}{}}
\newlabel{eq:comb_disint}{{126}{33}{Appendix:see-do model representation}{equation.6.126}{}}
\newlabel{eq:t_is_comb_disint_start}{{131}{34}{Appendix:see-do model representation}{equation.6.131}{}}
\newlabel{eq:t_is_comb_disint_end}{{136}{34}{Appendix:see-do model representation}{equation.6.136}{}}
\newlabel{eq:comb_disint_nonuniq}{{139}{34}{Appendix:see-do model representation}{equation.6.139}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Appendix: Counterfactual representation}{34}{section.7}\protected@file@percent }
\newlabel{sec:pot_rep}{{7}{34}{Appendix: Counterfactual representation}{section.7}{}}
\newlabel{def:pa_pot_outcomes}{{7.1}{34}{Parallel potential outcomes}{theorem.7.1}{}}
\@writefile{tdo}{\contentsline {todo}{How this will change: a parallel potential outcomes model is a comb $\ensuremath  {\mathbb  {K}}[\ensuremath  {\mathsf  {Y}}(W)|\ensuremath  {\mathsf  {H}}]\rightrightarrows \ensuremath  {\mathbb  {K}}[\ensuremath  {\mathsf  {Y}}_i|\ensuremath  {\mathsf  {W}}_i\ensuremath  {\mathsf  {Y}}(W)]$.}{35}{section*.6}\protected@file@percent }
\pgfsyspdfmark {pgfid81}{20088093}{42946477}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}Parallel potential outcomes representation theorem}{35}{subsection.7.1}\protected@file@percent }
\@writefile{tdo}{\contentsline {todo}{An interesting question is whether there is a similar representation theorem for potential outcomes without the assumption of deterministic reproducibility. I'm reasonably confident that this is a straightforward corollary of the representation theorem proved in my thesis. However, this requires maths not introduced in this draft of the paper.}{36}{section*.7}\protected@file@percent }
\pgfsyspdfmark {pgfid82}{20088093}{37790980}
\newlabel{def:exchangeable}{{7.2}{36}{Exchangeability}{theorem.7.2}{}}
\@writefile{tdo}{\contentsline {todo}{I think this is a very standard thing to do -- propose some $\ensuremath  {\mathsf  {X}}$ and $\ensuremath  {\mathbb  {P}}(\ensuremath  {\mathsf  {X}})$ then introduce some random variable $\ensuremath  {\mathsf  {Y}}$ and $\ensuremath  {\mathbb  {P}}(\ensuremath  {\mathsf  {X}}\ensuremath  {\mathsf  {Y}})$ as if the sample space contained both $\ensuremath  {\mathsf  {X}}$ and $\ensuremath  {\mathsf  {Y}}$ all along.}{36}{section*.8}\protected@file@percent }
\pgfsyspdfmark {pgfid85}{20088093}{18472437}
\newlabel{def:ext_exchangeable}{{7.4}{36}{Extendably exchangeable}{theorem.7.4}{}}
\newlabel{th:cfac_rep}{{7.6}{37}{Potential outcomes representation}{theorem.7.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8}Appendix: Connection is associative}{38}{section.8}\protected@file@percent }
\newlabel{sec:connect_associative}{{8}{38}{Appendix: Connection is associative}{section.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {9}Appendix: String Diagram Examples}{39}{section.9}\protected@file@percent }
\citation{fritz_synthetic_2020}
\newlabel{eq:extn_definition1}{{168}{40}{Connection}{equation.9.168}{}}
\newlabel{eq:extn_definition2}{{170}{40}{Connection}{equation.9.170}{}}
\@writefile{toc}{\contentsline {section}{\numberline {10}Markov variable maps and variables form a Markov category}{40}{section.10}\protected@file@percent }
\newlabel{sec:app_mcat}{{10}{40}{Markov variable maps and variables form a Markov category}{section.10}{}}
\newlabel{eq:ccom_1}{{175}{41}{}{equation.10.175}{}}
\newlabel{eq:nat}{{180}{41}{}{equation.10.180}{}}
\citation{fong_causal_2013,cho_disintegration_2019,fritz_synthetic_2020}
\bibstyle{plainnat}
\bibdata{references}
\bibcite{chiribella_quantum_2008}{{1}{2008}{{Chiribella et~al.}}{{Chiribella, D'Ariano, and Perinotti}}}
\@writefile{tdo}{\contentsline {todo}{I'm not sure how to formally argue that it is monoidal and symmetric as the relevant texts I've checked all gloss over the functors with respect to which the relevant isomorphisms should be natural, but labels with products were intentionally made to act just like sets with cartesian products which are symmetric monoidal}{42}{section*.9}\protected@file@percent }
\pgfsyspdfmark {pgfid134}{20088093}{13881244}
\bibcite{cho_disintegration_2019}{{2}{2019}{{Cho and Jacobs}}{{}}}
\bibcite{constantinou_extended_2017}{{3}{2017}{{Constantinou and Dawid}}{{}}}
\bibcite{dawid_decision-theoretic_2020}{{4}{2020}{{Dawid}}{{}}}
\bibcite{feynman_feynman_1979}{{5}{1979}{{Feynman}}{{}}}
\bibcite{fong_causal_2013}{{6}{2013}{{Fong}}{{}}}
\bibcite{fritz_synthetic_2020}{{7}{2020}{{Fritz}}{{}}}
\bibcite{heckerman_decision-theoretic_1995}{{8}{1995}{{Heckerman and Shachter}}{{}}}
\bibcite{hernan_does_2008}{{9}{2008}{{Hernán and Taubman}}{{}}}
\bibcite{hajek_what_2003}{{10}{2003}{{Hájek}}{{}}}
\bibcite{jacobs_causal_2019}{{11}{2019}{{Jacobs et~al.}}{{Jacobs, Kissinger, and Zanasi}}}
\bibcite{korzybski_science_1933}{{12}{1933}{{Korzybski}}{{}}}
\bibcite{lattimore_causal_2019}{{13}{2019{a}}{{Lattimore and Rohde}}{{}}}
\bibcite{lattimore_replacing_2019}{{14}{2019{b}}{{Lattimore and Rohde}}{{}}}
\bibcite{lewis_causal_1981}{{15}{1981}{{Lewis}}{{}}}
\bibcite{menger_random_2003}{{16}{2003}{{Menger}}{{}}}
\bibcite{mueller_causes_2021}{{17}{2021}{{Mueller et~al.}}{{Mueller, Li, and Pearl}}}
\bibcite{paul_f_christiano_edt_2018}{{18}{2018}{{Paul F. Christiano}}{{}}}
\bibcite{pearl_causality:_2009}{{19}{2009}{{Pearl}}{{}}}
\bibcite{pearl_does_2018}{{20}{2018}{{Pearl}}{{}}}
\bibcite{rubin_causal_2005}{{21}{2005}{{Rubin}}{{}}}
\bibcite{selinger_survey_2010}{{22}{2010}{{Selinger}}{{}}}
\bibcite{shahar_association_2009}{{23}{2009}{{Shahar}}{{}}}
\bibcite{tian_probabilities_2000}{{24}{2000}{{Tian and Pearl}}{{}}}
\bibcite{tian2002general}{{25}{2002}{{Tian and Pearl}}{{}}}
\bibcite{von_neumann_theory_1944}{{26}{1944}{{Von~Neumann and Morgenstern}}{{}}}
\bibcite{wald_statistical_1950}{{27}{1950}{{Wald}}{{}}}
\bibcite{weirich_causal_2016}{{28}{2016}{{Weirich}}{{}}}
