\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand*\HyPL@Entry[1]{}
\citation{pearl_book_2018}
\HyPL@Entry{0<</S/D>>}
\@writefile{tdo}{\contentsline {todo}{or something like that; it could also be functions of a distribution like a maximum likelihood estimate or a p-value}{1}{section*.1}\protected@file@percent }
\pgfsyspdfmark {pgfid1}{20442753}{25777404}
\pgfsyspdfmark {pgfid4}{34243869}{25791511}
\pgfsyspdfmark {pgfid5}{36357404}{25565781}
\citation{selinger_survey_2010}
\@writefile{toc}{\contentsline {section}{\numberline {1}Technical prerequesites}{2}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Markov kernels}{2}{subsection.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Cartesian and tensor products}{3}{subsection.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Delta measures, erase maps, copy maps}{3}{subsection.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}Products}{4}{subsection.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5}Labeled Markov kernels, conditional probabilities}{4}{subsection.1.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6}Modelling context, extension}{4}{subsection.1.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.7}Sequences}{5}{subsection.1.7}\protected@file@percent }
\@writefile{tdo}{\contentsline {todo}{We can associate a random variable with any state variable by copying it from the input to the output, but I think with low confidence that not doing this is going to be simpler for this paper}{5}{section*.3}\protected@file@percent }
\pgfsyspdfmark {pgfid31}{20088093}{12308380}
\citation{cinlar_probability_2011}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.8}Disintegration}{6}{subsection.1.8}\protected@file@percent }
\newlabel{ssec:disintegration}{{1.8}{6}{Disintegration}{subsection.1.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.9}Conditional independence}{6}{subsection.1.9}\protected@file@percent }
\newlabel{ssec:cond_indep}{{1.9}{6}{Conditional independence}{subsection.1.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}See-do models}{6}{section.2}\protected@file@percent }
\newlabel{sec:seedo_models}{{2}{6}{See-do models}{section.2}{}}
\newlabel{def:statistical model}{{2.1}{6}{Statistical model}{theorem.2.1}{}}
\citation{dawid_influence_2002,dawid_decision-theoretic_2012,dawid_decision-theoretic_2020}
\citation{lattimore_replacing_2019}
\citation{savage_foundations_1954}
\citation{heckerman_decision-theoretic_1995}
\newlabel{eq:see_do_independence_requirement}{{15}{7}{}{equation.2.15}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}See-do models for data-driven decision problems}{8}{subsection.2.1}\protected@file@percent }
\newlabel{ssec:data_driven_decision}{{2.1}{8}{See-do models for data-driven decision problems}{subsection.2.1}{}}
\newlabel{th:see_do_rep}{{2.3}{8}{See-do model representation}{theorem.2.3}{}}
\@writefile{tdo}{\contentsline {todo}{This will eventually move to an appendix}{8}{section*.4}\protected@file@percent }
\pgfsyspdfmark {pgfid32}{20088093}{11330844}
\newlabel{eq:disint}{{16}{9}{See-do models for data-driven decision problems}{equation.2.16}{}}
\newlabel{eq:comb_disint}{{17}{9}{See-do models for data-driven decision problems}{equation.2.17}{}}
\newlabel{eq:t_is_comb_disint_start}{{22}{9}{See-do models for data-driven decision problems}{equation.2.22}{}}
\newlabel{eq:t_is_comb_disint_end}{{27}{9}{See-do models for data-driven decision problems}{equation.2.27}{}}
\newlabel{eq:comb_disint_nonuniq}{{30}{9}{See-do models for data-driven decision problems}{equation.2.30}{}}
\citation{rubin_causal_2005}
\@writefile{toc}{\contentsline {section}{\numberline {3}Potential outcomes and counterfactuals}{10}{section.3}\protected@file@percent }
\citation{dawid_causal_2000}
\citation{robins_causal_2000}
\@writefile{tdo}{\contentsline {todo}{I have a vague intuition here that you always need some kind of assumption like ``my model is faithful to the real thing'', but if you are stating fairly specific conditions in English you should also be able to state them mathematically. Among other reasons, this is useful because it's easier for other people to know what you mean when you state them.}{11}{section*.5}\protected@file@percent }
\pgfsyspdfmark {pgfid33}{20088093}{38577412}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Potential outcomes in see-do models}{11}{subsection.3.1}\protected@file@percent }
\newlabel{def:potential_outcomes}{{3.1}{11}{Potential outcomes}{theorem.3.1}{}}
\newlabel{def:pa_pot_outcomes}{{3.2}{12}{Parallel potential outcomes}{theorem.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Parallel potential outcomes representation theorem}{12}{subsection.3.2}\protected@file@percent }
\@writefile{tdo}{\contentsline {todo}{An interesting question is whether there is a similar representation theorem for potential outcomes without the assumption of deterministic reproducibility. I'm reasonably confident that this is a straightforward corollary of the representation theorem proved in my thesis. However, this requires maths not introduced in this draft of the paper.}{13}{section*.6}\protected@file@percent }
\pgfsyspdfmark {pgfid34}{20088093}{25994500}
\newlabel{def:exchangeable}{{3.3}{13}{Exchangeability}{theorem.3.3}{}}
\@writefile{tdo}{\contentsline {todo}{I think this is a very standard thing to do -- propose some $\ensuremath  {\mathsf  {X}}$ and $\ensuremath  {\mathbb  {P}}(\ensuremath  {\mathsf  {X}})$ then introduce some random variable $\ensuremath  {\mathsf  {Y}}$ and $\ensuremath  {\mathbb  {P}}(\ensuremath  {\mathsf  {X}}\ensuremath  {\mathsf  {Y}})$ as if the sample space contained both $\ensuremath  {\mathsf  {X}}$ and $\ensuremath  {\mathsf  {Y}}$ all along.}{13}{section*.7}\protected@file@percent }
\pgfsyspdfmark {pgfid37}{20088093}{42551436}
\newlabel{def:ext_exchangeable}{{3.5}{14}{Extendably exchangeable}{theorem.3.5}{}}
\newlabel{th:cfac_rep}{{3.7}{14}{Potential outcomes representation}{theorem.3.7}{}}
\bibstyle{plainnat}
\bibdata{references}
\bibcite{cinlar_probability_2011}{{1}{2011}{{\c {C}inlar}}{{}}}
\bibcite{dawid_causal_2000}{{2}{2000}{{Dawid}}{{}}}
\bibcite{dawid_influence_2002}{{3}{2002}{{Dawid}}{{}}}
\bibcite{dawid_decision-theoretic_2020}{{4}{2020}{{Dawid}}{{}}}
\bibcite{dawid_decision-theoretic_2012}{{5}{2012}{{Dawid}}{{}}}
\bibcite{heckerman_decision-theoretic_1995}{{6}{1995}{{Heckerman and Shachter}}{{}}}
\bibcite{lattimore_replacing_2019}{{7}{2019}{{Lattimore and Rohde}}{{}}}
\bibcite{pearl_book_2018}{{8}{2018}{{Pearl and Mackenzie}}{{}}}
\bibcite{robins_causal_2000}{{9}{2000}{{Robins and Greenland}}{{}}}
\bibcite{rubin_causal_2005}{{10}{2005}{{Rubin}}{{}}}
\bibcite{savage_foundations_1954}{{11}{1954}{{Savage}}{{}}}
\bibcite{selinger_survey_2010}{{12}{2010}{{Selinger}}{{}}}
