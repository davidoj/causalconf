\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand*\HyPL@Entry[1]{}
\HyPL@Entry{0<</S/D>>}
\citation{pearl_causality:_2009}
\citation{rubin_causal_2005}
\citation{korzybski_science_1933}
\citation{hernan_does_2008}
\citation{pearl_does_2018}
\citation{shahar_association_2009}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Variables and Probability Models}{3}{section.2}\protected@file@percent }
\newlabel{sec:vague_variables}{{2}{3}{Variables and Probability Models}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Section outline}{3}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1}Brief outline of probability gap models}{4}{subsubsection.2.1.1}\protected@file@percent }
\citation{selinger_survey_2010}
\citation{fritz_synthetic_2020,cho_disintegration_2019}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Probability distributions, Markov kernels and string diagrams}{5}{subsection.2.2}\protected@file@percent }
\citation{fritz_synthetic_2020,cho_disintegration_2019,fong_causal_2013}
\citation{selinger_survey_2010}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}Examples}{7}{subsubsection.2.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2}Example: comb insertion}{8}{subsubsection.2.2.2}\protected@file@percent }
\newlabel{eq:2comb_M}{{20}{8}{Example: comb insertion}{equation.2.20}{}}
\newlabel{eq:2comb_winsert}{{28}{9}{Example: comb insertion}{equation.2.28}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Semantics of observed and unobserved variables}{9}{subsection.2.3}\protected@file@percent }
\newlabel{sec:variables}{{2.3}{9}{Semantics of observed and unobserved variables}{subsection.2.3}{}}
\citation{feynman_feynman_1979}
\citation{menger_random_2003}
\newlabel{def:observable}{{2.6}{11}{Consistency with observation}{theorem.2.6}{}}
\citation{pearl_causality:_2009}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Events}{12}{subsection.2.4}\protected@file@percent }
\citation{hajek_what_2003}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Standard probability theory}{13}{subsection.2.5}\protected@file@percent }
\newlabel{def:pushforward}{{2.8}{13}{Marginal distribution with respect to a probability space}{theorem.2.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6}Probabilistic models for causal inference}{13}{subsection.2.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7}Probability sets}{14}{subsection.2.7}\protected@file@percent }
\newlabel{lem:prod_pushf}{{2.13}{15}{Equivalence of pushforward definitions}{theorem.2.13}{}}
\newlabel{th:recurs_pushf}{{2.14}{15}{Recursive pushforward}{theorem.2.14}{}}
\citation{ershov_extension_1975}
\citation{ershov_extension_1975}
\citation{hajek_what_2003}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.8}Probability sets defined by marginal and conditional probabilities}{16}{subsection.2.8}\protected@file@percent }
\newlabel{def:valid_dist}{{2.16}{16}{Valid canditate distribution}{theorem.2.16}{}}
\newlabel{th:completion}{{2.17}{16}{Validity}{theorem.2.17}{}}
\newlabel{def:valid_conditional_prob}{{2.18}{16}{Valid candidate conditional}{theorem.2.18}{}}
\newlabel{def:copyproduct}{{2.19}{17}{Copy-product}{theorem.2.19}{}}
\newlabel{th:intersection}{{2.20}{17}{Copy-product is an intersection of probability sets}{theorem.2.20}{}}
\@writefile{tdo}{\contentsline {todo}{Make sure this works for a.s. equality}{17}{section*.2}\protected@file@percent }
\pgfsyspdfmark {pgfid23}{20088093}{17107849}
\newlabel{th:valid_agree}{{2.21}{17}{Equivalence of validity definitions}{theorem.2.21}{}}
\newlabel{lem:valid_extendability}{{2.22}{18}{Copy-product of valid candidate conditionals is valid}{theorem.2.22}{}}
\newlabel{corr:valid_extend_order1}{{2.23}{18}{Valid candidate conditional is validly extendable to a valid candidate distribution}{theorem.2.23}{}}
\newlabel{th:valid_conditional_probability}{{2.24}{18}{Validity of conditional probabilities}{theorem.2.24}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.9}Probability gap models}{19}{subsection.2.9}\protected@file@percent }
\@writefile{tdo}{\contentsline {todo}{I need a statement like: probability gap model of type $X|Y$ with insert $Z|X$}{19}{section*.3}\protected@file@percent }
\pgfsyspdfmark {pgfid24}{20088093}{32099219}
\newlabel{def:order1_bycond}{{2.25}{19}{Marginal probability gap model}{theorem.2.25}{}}
\citation{chiribella_quantum_2008,jacobs_causal_2019}
\newlabel{eq:insert_op}{{77}{20}{Probability gap models}{equation.2.77}{}}
\citation{hutter_universal_2004}
\newlabel{eq:intersperse_pgap}{{80}{21}{Probability gap models}{equation.2.80}{}}
\newlabel{eq:n_comb_def}{{81}{21}{Probability gap models}{equation.2.81}{}}
\citation{cho_disintegration_2019}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.9.1}Disintegrations}{22}{subsubsection.2.9.1}\protected@file@percent }
\newlabel{lem:disint_exist}{{2.27}{22}{Disintegration existence in standard measurable Markov kernels}{theorem.2.27}{}}
\newlabel{th:valid_disint}{{2.28}{22}{Existence of conditional probabilities}{theorem.2.28}{}}
\newlabel{eq:k_disint}{{83}{22}{Disintegrations}{equation.2.83}{}}
\citation{constantinou_extended_2017}
\citation{cho_disintegration_2019}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.9.2}Conditional independence}{23}{subsubsection.2.9.2}\protected@file@percent }
\newlabel{ssec:cond_indep}{{2.9.2}{23}{Conditional independence}{subsubsection.2.9.2}{}}
\citation{cho_disintegration_2019}
\newlabel{th:cho_ci_equiv}{{2.30}{24}{}{theorem.2.30}{}}
\newlabel{eq:higherorder_ci_erase}{{95}{24}{Unresponsiveness}{equation.2.95}{}}
\newlabel{th:cons_ci}{{2.34}{24}{Conditional independence from kernel unresponsiveness}{theorem.2.34}{}}
\citation{ramsey_truth_2016}
\citation{savage_foundations_1954}
\citation{bolker_functions_1966,jeffrey_logic_1990}
\citation{joyce_foundations_1999}
\@writefile{tdo}{\contentsline {todo}{start again here}{25}{section*.4}\protected@file@percent }
\pgfsyspdfmark {pgfid41}{20088093}{32481287}
\@writefile{toc}{\contentsline {section}{\numberline {3}Decision theoretic causal inference}{25}{section.3}\protected@file@percent }
\newlabel{sec:seedo_models}{{3}{25}{Decision theoretic causal inference}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Decision problems}{26}{subsection.3.1}\protected@file@percent }
\@writefile{tdo}{\contentsline {todo}{I think adding hypotheses at this point might make things unnecessarily confusing; on the other hand, they are useful for the connection to classical statistical decision theory. The "repeatable experiments" section shows how see-do models with certain assumptions induce an easier to understand class of hypotheses, and I could just save the idea of a hypothesis until I get there}{26}{section*.5}\protected@file@percent }
\pgfsyspdfmark {pgfid42}{20088093}{20691625}
\@writefile{tdo}{\contentsline {todo}{I need to update the proof for this claim}{27}{section*.6}\protected@file@percent }
\pgfsyspdfmark {pgfid43}{13754458}{26720458}
\pgfsyspdfmark {pgfid46}{34243869}{26734565}
\pgfsyspdfmark {pgfid47}{36357404}{26508835}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Decisions as measurement procedures}{27}{subsection.3.2}\protected@file@percent }
\@writefile{tdo}{\contentsline {todo}{I don't know how important this observation is, but the fact that $\ensuremath  {\EuScript  {D}}$ is an output of a formal decision making system makes it different from other things we might call decisions, and I wonder if I should call it something else in order to avoid ambiguity. The vague reason I think this matters is: whatever you might want to measure, you won't learn more about $\ensuremath  {\EuScript  {D}}$ from it than you already know once you have the model, the utility and the decision rule, this is not a property that other things we call ``decisons'' share and this distinction might be important regarding judgements of causal contractibilty.}{27}{section*.7}\protected@file@percent }
\citation{lattimore_causal_2019}
\citation{lattimore_replacing_2019}
\citation{dawid_decision-theoretic_2020}
\citation{heckerman_decision-theoretic_1995}
\pgfsyspdfmark {pgfid48}{20088093}{40207800}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Causal models similar to see-do models}{28}{subsection.3.3}\protected@file@percent }
\citation{jacobs_causal_2019}
\citation{tian2002general}
\citation{von_neumann_theory_1944}
\citation{wald_statistical_1950}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}See-do models and classical statistics}{29}{subsection.3.4}\protected@file@percent }
\citation{dawid_decision-theoretic_2020}
\citation{greenland_identifiability_1986}
\@writefile{toc}{\contentsline {section}{\numberline {4}Repeatable experiments}{30}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Assumptions of repeatability applicable to models of decisions and consequences}{32}{subsection.4.1}\protected@file@percent }
\newlabel{def:domodel}{{4.1}{32}{Do model}{theorem.4.1}{}}
\newlabel{def:caus_exch}{{4.3}{34}{Commutativity of exchange}{theorem.4.3}{}}
\newlabel{def:caus_cont}{{4.4}{34}{Causal contractibility}{theorem.4.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Representations of contractible probability models}{35}{subsection.4.2}\protected@file@percent }
\@writefile{tdo}{\contentsline {todo}{matrix of variables?}{35}{section*.8}\protected@file@percent }
\pgfsyspdfmark {pgfid59}{31409438}{31303657}
\pgfsyspdfmark {pgfid62}{34243869}{31317764}
\pgfsyspdfmark {pgfid63}{36357404}{31092034}
\@writefile{tdo}{\contentsline {todo}{The following can be deduced from the theorems after it, but I thought it might be helpful to have the explanation.}{35}{section*.9}\protected@file@percent }
\pgfsyspdfmark {pgfid64}{20088093}{14245651}
\newlabel{eq:table_lookup_example}{{117}{36}{Representations of contractible probability models}{equation.4.117}{}}
\newlabel{eq:table_lookup_cons}{{118}{36}{Representations of contractible probability models}{equation.4.118}{}}
\newlabel{th:table_rep}{{4.6}{36}{Table representation of causally contractible do models}{theorem.4.6}{}}
\citation{rubin_causal_2005}
\citation{kallenberg_basic_2005}
\newlabel{th:contractibility_commutativity}{{4.7}{39}{}{theorem.4.7}{}}
\newlabel{th:iid_rep}{{4.8}{40}{}{theorem.4.8}{}}
\newlabel{eq:pci_1}{{158}{41}{Representations of contractible probability models}{equation.4.158}{}}
\newlabel{eq:pci_2}{{159}{41}{Representations of contractible probability models}{equation.4.159}{}}
\newlabel{eq:pci_4}{{161}{41}{Representations of contractible probability models}{equation.4.161}{}}
\newlabel{eq:pci_5}{{162}{41}{Representations of contractible probability models}{equation.4.162}{}}
\newlabel{eq:pci_6}{{163}{41}{Representations of contractible probability models}{equation.4.163}{}}
\newlabel{eq:pci_7}{{164}{41}{Representations of contractible probability models}{equation.4.164}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Potential outcomes}{41}{subsection.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Potential outcomes}{41}{section.5}\protected@file@percent }
\newlabel{sec:potential_outcomes}{{5}{41}{Potential outcomes}{section.5}{}}
\newlabel{def:potential_outcomes}{{5.2}{42}{Potential outcomes: formal requirement}{theorem.5.2}{}}
\newlabel{lem:po_triviality}{{5.3}{42}{Trivial formal potential outcomes}{theorem.5.3}{}}
\newlabel{def:po_whatmodel}{{5.4}{42}{What potential outcomes model: counterfactual extension}{theorem.5.4}{}}
\citation{rubin_causal_2005}
\newlabel{eq:po_probmodel}{{170}{43}{Potential outcomes}{equation.5.170}{}}
\newlabel{lem:selector}{{5.5}{43}{Selector kernel}{theorem.5.5}{}}
\newlabel{eq:multiplex2}{{171}{43}{Selector kernel}{equation.5.171}{}}
\newlabel{eq:multiplex1}{{173}{43}{Selector kernel}{equation.5.173}{}}
\citation{hernan_does_2008}
\newlabel{def:po_dte}{{5.6}{44}{}{theorem.5.6}{}}
\newlabel{eq:different_ys}{{174}{44}{}{equation.5.174}{}}
\newlabel{eq:po_exist_con1}{{177}{44}{}{equation.5.177}{}}
\newlabel{eq:po_exist_con2}{{181}{45}{}{equation.5.181}{}}
\@writefile{tdo}{\contentsline {todo}{I think I asked the wrong question here -- should've asked when I can extend a see-do model with additonal pre-choice variables. I think it's possible to always choose some deterministic potential outcomes.}{45}{section*.10}\protected@file@percent }
\pgfsyspdfmark {pgfid87}{20088093}{29241510}
\newlabel{eq:po_proxycontrol}{{182}{45}{}{equation.5.182}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Appendix:see-do model representation}{45}{section.6}\protected@file@percent }
\newlabel{sec:see-do-rep}{{6}{45}{Appendix:see-do model representation}{section.6}{}}
\@writefile{tdo}{\contentsline {todo}{Update notation}{45}{section*.11}\protected@file@percent }
\pgfsyspdfmark {pgfid88}{20088093}{8376220}
\newlabel{th:see_do_rep}{{6.1}{46}{See-do model representation}{theorem.6.1}{}}
\newlabel{eq:disint}{{183}{46}{Appendix:see-do model representation}{equation.6.183}{}}
\newlabel{eq:comb_disint}{{184}{46}{Appendix:see-do model representation}{equation.6.184}{}}
\newlabel{eq:t_is_comb_disint_start}{{189}{47}{Appendix:see-do model representation}{equation.6.189}{}}
\newlabel{eq:t_is_comb_disint_end}{{194}{47}{Appendix:see-do model representation}{equation.6.194}{}}
\newlabel{eq:comb_disint_nonuniq}{{197}{47}{Appendix:see-do model representation}{equation.6.197}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Appendix: Counterfactual representation}{47}{section.7}\protected@file@percent }
\newlabel{sec:pot_rep}{{7}{47}{Appendix: Counterfactual representation}{section.7}{}}
\newlabel{def:pa_pot_outcomes}{{7.1}{47}{Parallel potential outcomes}{theorem.7.1}{}}
\@writefile{tdo}{\contentsline {todo}{How this will change: a parallel potential outcomes model is a comb $\ensuremath  {\mathbb  {K}}[\ensuremath  {\mathsf  {Y}}(W)|\ensuremath  {\mathsf  {H}}]\rightrightarrows \ensuremath  {\mathbb  {K}}[\ensuremath  {\mathsf  {Y}}_i|\ensuremath  {\mathsf  {W}}_i\ensuremath  {\mathsf  {Y}}(W)]$.}{48}{section*.12}\protected@file@percent }
\pgfsyspdfmark {pgfid89}{20088093}{42946477}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}Parallel potential outcomes representation theorem}{48}{subsection.7.1}\protected@file@percent }
\@writefile{tdo}{\contentsline {todo}{An interesting question is whether there is a similar representation theorem for potential outcomes without the assumption of deterministic reproducibility. I'm reasonably confident that this is a straightforward corollary of the representation theorem proved in my thesis. However, this requires maths not introduced in this draft of the paper.}{49}{section*.13}\protected@file@percent }
\pgfsyspdfmark {pgfid90}{20088093}{37790980}
\newlabel{def:exchangeable}{{7.2}{49}{Exchangeability}{theorem.7.2}{}}
\@writefile{tdo}{\contentsline {todo}{I think this is a very standard thing to do -- propose some $\ensuremath  {\mathsf  {X}}$ and $\ensuremath  {\mathbb  {P}}(\ensuremath  {\mathsf  {X}})$ then introduce some random variable $\ensuremath  {\mathsf  {Y}}$ and $\ensuremath  {\mathbb  {P}}(\ensuremath  {\mathsf  {X}}\ensuremath  {\mathsf  {Y}})$ as if the sample space contained both $\ensuremath  {\mathsf  {X}}$ and $\ensuremath  {\mathsf  {Y}}$ all along.}{49}{section*.14}\protected@file@percent }
\pgfsyspdfmark {pgfid93}{20088093}{18472437}
\newlabel{def:ext_exchangeable}{{7.4}{49}{Extendably exchangeable}{theorem.7.4}{}}
\newlabel{th:cfac_rep}{{7.6}{50}{Potential outcomes representation}{theorem.7.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8}Appendix: Connection is associative}{51}{section.8}\protected@file@percent }
\newlabel{sec:connect_associative}{{8}{51}{Appendix: Connection is associative}{section.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {9}Appendix: String Diagram Examples}{52}{section.9}\protected@file@percent }
\citation{fritz_synthetic_2020}
\newlabel{eq:extn_definition1}{{226}{53}{Connection}{equation.9.226}{}}
\newlabel{eq:extn_definition2}{{228}{53}{Connection}{equation.9.228}{}}
\@writefile{toc}{\contentsline {section}{\numberline {10}Markov variable maps and variables form a Markov category}{53}{section.10}\protected@file@percent }
\newlabel{sec:app_mcat}{{10}{53}{Markov variable maps and variables form a Markov category}{section.10}{}}
\newlabel{eq:ccom_1}{{233}{54}{}{equation.10.233}{}}
\newlabel{eq:nat}{{238}{54}{}{equation.10.238}{}}
\citation{fong_causal_2013,cho_disintegration_2019,fritz_synthetic_2020}
\bibstyle{plainnat}
\bibdata{references}
\bibcite{kallenberg_basic_2005}{{1}{2005}{{kal}}{{}}}
\@writefile{tdo}{\contentsline {todo}{I'm not sure how to formally argue that it is monoidal and symmetric as the relevant texts I've checked all gloss over the functors with respect to which the relevant isomorphisms should be natural, but labels with products were intentionally made to act just like sets with cartesian products which are symmetric monoidal}{55}{section*.15}\protected@file@percent }
\pgfsyspdfmark {pgfid142}{20088093}{13881244}
\bibcite{bolker_functions_1966}{{2}{1966}{{Bolker}}{{}}}
\bibcite{chiribella_quantum_2008}{{3}{2008}{{Chiribella et~al.}}{{Chiribella, D'Ariano, and Perinotti}}}
\bibcite{cho_disintegration_2019}{{4}{2019}{{Cho and Jacobs}}{{}}}
\bibcite{constantinou_extended_2017}{{5}{2017}{{Constantinou and Dawid}}{{}}}
\bibcite{dawid_decision-theoretic_2020}{{6}{2020}{{Dawid}}{{}}}
\bibcite{ershov_extension_1975}{{7}{1975}{{Ershov}}{{}}}
\bibcite{feynman_feynman_1979}{{8}{1979}{{Feynman}}{{}}}
\bibcite{fong_causal_2013}{{9}{2013}{{Fong}}{{}}}
\bibcite{fritz_synthetic_2020}{{10}{2020}{{Fritz}}{{}}}
\bibcite{greenland_identifiability_1986}{{11}{1986}{{GREENLAND and ROBINS}}{{}}}
\bibcite{heckerman_decision-theoretic_1995}{{12}{1995}{{Heckerman and Shachter}}{{}}}
\bibcite{hernan_does_2008}{{13}{2008}{{Hernán and Taubman}}{{}}}
\bibcite{hutter_universal_2004}{{14}{2004}{{Hutter}}{{}}}
\bibcite{hajek_what_2003}{{15}{2003}{{Hájek}}{{}}}
\bibcite{jacobs_causal_2019}{{16}{2019}{{Jacobs et~al.}}{{Jacobs, Kissinger, and Zanasi}}}
\bibcite{jeffrey_logic_1990}{{17}{1965}{{Jeffrey}}{{}}}
\bibcite{joyce_foundations_1999}{{18}{1999}{{Joyce}}{{}}}
\bibcite{korzybski_science_1933}{{19}{1933}{{Korzybski}}{{}}}
\bibcite{lattimore_causal_2019}{{20}{2019{a}}{{Lattimore and Rohde}}{{}}}
\bibcite{lattimore_replacing_2019}{{21}{2019{b}}{{Lattimore and Rohde}}{{}}}
\bibcite{menger_random_2003}{{22}{2003}{{Menger}}{{}}}
\bibcite{pearl_causality:_2009}{{23}{2009}{{Pearl}}{{}}}
\bibcite{pearl_does_2018}{{24}{2018}{{Pearl}}{{}}}
\bibcite{ramsey_truth_2016}{{25}{2016}{{Ramsey}}{{}}}
\bibcite{rubin_causal_2005}{{26}{2005}{{Rubin}}{{}}}
\bibcite{savage_foundations_1954}{{27}{1954}{{Savage}}{{}}}
\bibcite{selinger_survey_2010}{{28}{2010}{{Selinger}}{{}}}
\bibcite{shahar_association_2009}{{29}{2009}{{Shahar}}{{}}}
\bibcite{tian2002general}{{30}{2002}{{Tian and Pearl}}{{}}}
\bibcite{von_neumann_theory_1944}{{31}{1944}{{Von~Neumann and Morgenstern}}{{}}}
\bibcite{wald_statistical_1950}{{32}{1950}{{Wald}}{{}}}
