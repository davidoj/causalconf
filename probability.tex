%!TEX root = main.tex


\section{Probability prerequisites}\label{sec:vague_variables}

\todo[inline]{Notation table, including iverson bracket}

\subsection{The roles of variables and probabilistic models}

The sample space $(\Omega,\sigalg{F})$ along with the measurement procedure(s) $\proc{S}$ and the associated model variable $\RV{S}$ is a ``model skeleton''. The criterion of \emph{compatibility with observation} establishes a relation between the results of measurements and elements of $\sigalg{F}$.

The basic kind of problem we want to consider is one in which we wish to decide upon an action that we expect will yield good consequences. We suppose that whether a consequence is good or not can somehow be deduced from the result of $\proc{S}$. However, we do not know the result of $\proc{S}$, so we need to say something about the result we expect to see for each action we could choose.

It is common to to represent uncertain knowledge about the outcomes of not-yet-performed measurements using probabilistic models, and we follow this well-trodden path. However, we do need to generalise common practice somewhat, because we need a model that tells us that different consequences may arise from deciding on different actions.

We use probability sets and probability gap models to represent decision problems. A probability set is a set of probability measures on a common sample space $(\Omega,\sigalg{F})$, and a probability gap model is a probability set along with a collection of subsets (the terminology comes from \citet{hajek_what_2003}). A decision problem presents us with a set of choices, and we assume that each choice is associated with a probability set representing uncertain knowledge (or best guesses) about the outcome of this choice. A probability gap model is the collection of all probability sets associated with a choice, along with the union of all of these sets. The union of all of the individual choice sets represents what we know about the outcome regardless of which choice is decided on.

Our use of probability sets to represent uncertain knowledge about the outcome of each choice is not the result of a strong opinion that probability sets are the best way to do this. We've already had to introduce probability sets to handle different choices in the first place and we don't see any harm in continuing to use them for this additional purpose. A model in which a unique probability distribution is associated with each choice is simply a special case of this setup, where the probability set associated with each choice is of size 1.

A great deal of standard probability theory is applicable to reasoning with probability sets, and readers may be quite familiar with much of this. In particular, our notions of uniform conditional probability and uniform conditional independence are similar in many ways to the familiar notions of conditional probability and conditional independence, with the different being that -- even in finite sets -- the former do not always exist. We also make use of a diagrammatic notation for Markov kernels (or stochastic functions) taken from the categorical study of probability theory, which may be less familiar.

\subsection{Standard probability theory}

\begin{definition}[Measurable space]
A measurable space $(X,\sigalg{X})$ is a set $X$ along with a $\sigma$-algebra of subsets $\sigalg{X}$.
\end{definition}

We use a number of shorthands for measurable spaces:
\begin{itemize}
	\item Where the choice of $\sigma$-algebra is unambiguous, we will just use the set name $X$ to refer to $X$ along with a $\sigma$-algebra $\sigalg{X}$
	\item For a discrete set $X$, the sigma-algebra $\sigalg{X}$ referred to with the same letter is the discrete sigma-algebra
	\item For a continuous set $X$, the sigma-algebra $\sigalg{X}$ referred to with the same letter is the Borel sigma-algebra
\end{itemize}

\begin{definition}[Probability measure]
Given a measurable space $(X,\sigalg{X})$, a probability measure is a $\sigma$-additive function $\mu:\sigalg{X}\to [0,1]$ such that $\mu(\emptyset)=0$ and $\mu(X)=1$. We write $\Delta(X)$ for the set of all probability measures on $(X,\sigalg{X})$.
\end{definition}

\begin{definition}[Markov kernel]\label{def:mkern}
Given measurable spaces $(X,\sigalg{X})$ and $(Y,\sigalg{Y})$, a Markov kernel $\prob{Q}:X\kto Y$ is a map $Y\times \sigalg{X}\to [0,1]$ such that
\begin{enumerate}
	\item $y\mapsto \prob{Q}(A|y)$ is $\sigalg{Y}$-measurable for all $A\in \sigalg{X}$
	\item $A\mapsto \prob{Q}(A|y)$ is a probability measure on $(X,\sigalg{X})$ for all $y\in Y$
\end{enumerate}
\end{definition}

\begin{definition}[Delta measure]
Given a measureable space $(X,\sigalg{X})$ and $x\in X$, $\delta_x\in \Delta(X)$ is the measure defined by $\delta_x(A):=\llbracket x\in A \rrbracket$ for all $A\in \sigalg{X}$
\end{definition}

\begin{definition}[Probability space]
A probability space is a triple $(\mu,\Omega,\sigalg{F})$, where $\mu$ is a base measure on $\sigalg{F}$ and $(\Omega,\sigalg{F})$ is a measurable space.
\end{definition}

\begin{definition}[Variable]
Given a measureable space $(\Omega,\sigalg{F})$ and a measurable space of values $(X,\sigalg{X})$, an \emph{$X$-valued variable} is a measurable function $\RV{X}:(\Omega,\sigalg{F})\to (X,\sigalg{X})$.
\end{definition}

\begin{definition}[Sequence of variables]
Given a measureable space $(\Omega,\sigalg{F})$ and two variables $\RV{X}:(\Omega,\sigalg{F})\to (X,\sigalg{X})$, $\RV{Y}:(\Omega,\sigalg{F})\to (Y,\sigalg{Y})$, $(\RV{X},\RV{Y}):\Omega\to X\times Y$ is the variable $\omega\mapsto (\RV{X}(\omega),\RV{Y}(\omega))$.
\end{definition}

\begin{definition}[Marginal distribution with respect to a probability space]\label{def:pushforward}
Given a probability space $(\mu,\Omega,\sigalg{F})$ and a variable $\RV{X}:\Omega\to (X,\sigalg{X})$, we can define the \emph{marginal distribution} of $\RV{X}$ with respect to $\mu$, $\mu^{\RV{X}}:\sigalg{X}\to [0,1]$ by $\mu^{\RV{X}}(A):=\mu(\RV{X}^{-1}(A))$ for any $A\in \sigalg{X}$.
\end{definition}

\begin{definition}[Distribution-kernel products]
Given $(X,\sigalg{X})$, $(Y,\sigalg{Y})$ a probability distribution $\mu\in \Delta(X)$ and a Markov kernel $\kernel{K}:X\kto Y$, $\mu\kernel{K}$ is a probability distribution on $(Y,\sigalg{Y})$ defined by
\begin{align}
	\mu\kernel{K}(A):= \int_{X} \kernel{K}(A|x)\mu(\mathrm{d}x)
\end{align}
for all $A\in \sigalg{Y}$.
\end{definition}

\begin{definition}[Kernel-kernel products]
Given $(X,\sigalg{X})$, $(Y,\sigalg{Y})$, $(Z,\sigalg{Z})$ and Markov kernels $\kernel{K}:X\kto Y$ and $\kernel{L}:Y\kto Z$, $\kernel{K}\kernel{L}$ is a Markov kernel $X\kto Z$ defined by
\begin{align}
	\kernel{K}\kernel{L}(A|x):= \int_{Y} \kernel{L}(A|y)\kernel{K}(\mathrm{d}y|x)
\end{align}
for all $A\in \sigalg{Z}$.
\end{definition}

\begin{lemma}[Marginal distribution as a kernel product]\label{lem:pushf_kprod}
Given a probability space $(\mu,\Omega,\sigalg{F})$ and a variable $\RV{X}:\Omega\to (X,\sigalg{X})$, define $\kernel{F}_{\RV{X}}:\Omega\kto X$ by $\kernel{F}_{\RV{X}}(A|\omega)=\delta_{\RV{X}(\omega)}(A)$, then
\begin{align}
	\mu^{\RV{X}} = \mu\kernel{F}_{\RV{X}}
\end{align}
\end{lemma}

\begin{proof}
Consider any $A\in \sigalg{X}$.
\begin{align}
	\mu \kernel{F}_{\RV{X}}(A) &= \int_\Omega \delta_{\RV{X}(\omega)}(A) \mathrm{d}\mu(\omega)\\
	&= \int_{\RV{X}^{-1}(\omega)} \mathrm{d}\mu(\omega)\\
	&= \mu^{\RV{X}}(A)
\end{align}
\end{proof}

\subsection{Not quite standard probability theory}

Instead of having probability distributions and Markov kernels as two different kinds of thing, we can identify probability distributions with Markov kernels whose domain is a one element set $\{*\}$. This will prove useful in further developments, as it means that we can treat probability distributions and Markov kernels as different varieties of the same kind of thing.

\begin{definition}[Probability measures as Markov kernels]
Given a measurable space $(X,\sigalg{X})$ and $\mu\in \Delta(X)$, the Markov kernel $\kernel{K}:\{*\}\kto X$ associated with $\mu$ is given by $\kernel{K}(A|*)=\mu(A)$ for all $A\in \sigalg{X}$.
\end{definition}

We will use probability measures and their associated Markov kernels interchangeably, as it is transparent how to get from one to another.

Conditional probability distributions are ``Markov kernel annotated with variables''.

\begin{definition}[Conditional distribution]\label{def:disint}
Given a probability space $(\mu,\Omega,\sigalg{F})$ and variables $\RV{X}:\Omega\to X$, $\RV{Y}:\Omega\to Y$, the probability of $\RV{Y}$ given $\RV{X}$ is any Markov kernel $\mu^{\RV{Y}|\RV{X}}:X\kto Y$ such that
\begin{align}
	\mu^{\RV{XY}}(A\times B)&=\int_{A} \mu^{\RV{Y}|\RV{X}}(B|x) \mathrm{d}\mu^{\RV{X}}(x) &\forall A\in \sigalg{X}, B\in \sigalg{Y}\\
	&\iff\\
	\mu^{\RV{XY}}&= \tikzfig{disint_def}\label{eq:conditional} 
\end{align}
\end{definition}

We define higher order conditionals as ``conditionals of conditionals''.

\begin{definition}[Higher order conditionals]
Given a probability space $(\mu,\Omega,\sigalg{F})$ and variables $\RV{X}:\Omega\to X$, $\RV{Y}:\Omega\to Y$ and $\RV{Z}:\Omega\to Z$, a higher order conditional $\mu^{\RV{Z}|(\RV{Y}|\RV{X})}:X\times Y\to Z$ is any Markov kernel such that, for some $\mu^{\RV{Y}|\RV{X}}$, 
\begin{align}
	\mu^{\RV{ZY}|\RV{X}}(B\times C|x) &=\int_B \mu^{\RV{Z}|(\RV{Y}|\RV{X})}(C|x,y)\mu^{\RV{Y}|\RV{X}}(dy|x)\\ 
	&\iff\\
	\mu^{\RV{ZY}|\RV{X}} &= \tikzfig{disintegration_existence}\label{eq:disint_def}
\end{align}
\end{definition}

Higher order conditionals are useful because $\mu^{\RV{Z}|(\RV{Y}|\RV{X})}$ is a version of $\mu^{\RV{Z}|\RV{YX}}$, so if we're given $\mu^{\RV{ZY}|\RV{X}}$ but not $\mu$ itself, we use the higher order conditional $\mu^{\RV{Z}|(\RV{Y}|\RV{X})}$ as a version of $\mu^{\RV{X}|\RV{YX}}$. This also hold for conditional with respect to probability sets, which we will introduce later (Theorem \ref{th:higher_order_conditionals}).

Furthermore, given $\mu^{\RV{XY}|\RV{Z}}$ and $\RV{X}$, $\RV{Y}$ standard measurable, it has recently been proven that a higher order conditional $\mu^{\RV{Z}|(\RV{Y}|\RV{X})}$ exists \citet{bogachev_kantorovich_2020}, Theorem 3.5. See also Theorem \ref{th:ho_cond_psets} for the extension of this theorem to probability sets.

