%!TEX root = main.tex


\section{Variables and Probability Models}\label{sec:vague_variables}

\subsection{Section outline}

This section introduces the mathematical foundations used throughout the rest of the paper. The first subsection briefly introduces probability theory, which is likely to be familiar to many readers, as well as how string diagrams can be used to represent probabilistic functions (or \emph{Markov kernels}), which may be less familiar. We use string diagrams for probabilstic reasoning in a number of places, and this section is intended to help interpret mathematical statements in this form.

The second subsection discusses the interpretation of probabilistic variables. Our formalisation of probabilistic variables is standard -- we define them as measurable functions on a fundamental probability set $\Omega$. We discusses how this formalisation can be connected to statements about the real world via \emph{measurement processes}, and distinguishes observed variables (which are associated with measurement processes) from unobserved variables (which are not associated with measurement processes). This section is not part of the mathematical theory of probability gap models, but it is relevant when one wants to apply this theory to real problems or to understand how the theory of probability gap models relates to other theories of causal inference.

Finally, we introduce \emph{probability gap models}. Probability gap models are a generalisation of probability models, and to understand the rest of this paper a reader needs to understand what a probability gap model is, how we define the common kinds of probability gap models used in this paper and what conditional probabilities and conditional independence statements mean for probability gap models.

\subsubsection{Brief outline of probability gap models}

We consider a probability model to be a probability space $(\Omega,\sigalg{F},\mu)$ along with a collection of random variables. However, if I want to use probabilistic models to support decision making, then I need function from options to probability models. For example, suppose I have two options $A=\{0,1\}$, and I want to compare these options based on what I expect to happen if I choose them. If I choose option $0$, then I can (perhaps) represent my expectations about the consequences with a probability model, and if I choose option $1$ I can represent my expectations about the consequences with a different probability model. I can compare the two consequences, then decide which option seems to be better. To make this comparison, I have used a function from elements of $A$ to probability models. A function that takes elements of some set as inputs (which may or may not be decisions) and returns probability models is a \emph{probability gap model}, and the set of inputs it accepts is a \emph{probability gap}.

We are particularly interested in probability gap models where the consequences of all inputs share some marginal or conditional probabilities. The simplest example of a model like this can be represented by a probability distribution $\prob{P}^{\RV{X}}$ for some variable $\RV{X}:\Omega\to X$. Such a probability distribution is consistent with many base measures on the fundamental probability set $\Omega$, and so we can consider the choice of base measure to be a probability gap. Not every probability distribution over $X$ can define a probability gap model in this way. In particular, we need $\prob{P}^{\RV{X}}$ to assign probability 0 to outcomes that are mathematically impossible according to the definition of $\RV{X}$ to ensure that there is some base measure that features $\prob{P}^{\RV{X}}$ as a marginal. We call probability gap models represented by probability distributions \emph{order 0 probability gap models}.

Higher order probability gap models can be represented by conditional probabilities $\prob{P}^{\RV{Y}|\RV{X}}$ or pairs of conditional probabilities $\{\prob{P}^{\RV{X}|\RV{W}},\prob{P}^{\RV{Z}|\RV{WXY}}\}$, which we call \emph{order 1} and \emph{order 2} models respectively. Decision functions in data-driven decision problems correspond to probability gaps in order 2 models, as we discuss in Section \ref{sec:seedo_models}, which makes this type of model particularly interesting for our purposes. We also require these to be valid, and we define conditions for validity and prove that they are sufficient to ensure that models represented by conditional probabilities can in fact be mapped to base measures on the fundamental probability set.

A conditional independence statement in a probability gap model means that the corresponding conditional independence statement holds for all base measures in the range of the function defined by the model. It is possible to deduce conditional independences from ``independences'' in the conditional probabilities that we use to represent these models, and conditional independences can imply the existence of conditional probabilities with certain independence properties.

We can consider causal Bayesian networks to represent order 2 probability gap models. That is, a causal Bayesian network represents a function $\prob{P}$ that take inserts from some set $A$ of conditional probabilities and returns a probability model, and it does so in such a way that there are a pair of conditional probabilities $\{\prob{P}^{\RV{X}|\RV{W}},\prob{P}^{\RV{Z}|\RV{WXY}}\}$ shared by all models in the codomain of $\prob{P}$. The observational distribution is the value of $\prob{P}(\text{obs})$ for some \emph{observational insert} $\text{obs}\in A$, and other choices of inserts yield interventional distributions. Defining causal Bayesian networks in this manner resolves two areas of difficulty with causal Bayesian networks. First, under the standard definition of causal Bayesian networks interventional probabilities may fail to exist; with our perspective we can see that this arises due to misunderstanding the domain of $\prob{P}$. Secondly, there may be multiple distributions that differ in important ways that all satisfy the standard definition of ``interventional distributions''. The one-to-many relationship between observations and interventions is a basic challenge of causal inference, the problem arises when this relationship is obscured by calling multiple different things ``the interventional distribution''. If we consider causal Bayesian networks to represent order 2 probability gap models, we avoid doing this. 


\subsection{Semantics of observed and unobserved variables}\label{sec:variables}

We are interested in constructing \emph{probabilistic models} which explain some part of the world. In a model, variables play the role of ``pointing to the parts of the world the model is explaining''. Both observed an unobserved variables play important roles in causal modelling and we think it is worth clarifying what variables of either type refer to. Our approach is a standard one: a probabilistic model is associated with an experiment or measurement procedure that yields values in a well-defined set. Observable variables are obtained by applying well-defined functions to the result of this total measurement. We use a richer fundamental probability set that includes ``unobserved variables'' that are formally treated the same way as observed variables, but aren't associated with any real-world counterparts.

Consider Newton's second law in the form $\proc{F}=\proc{MA}$ as a simple example of a model that relates ``variables'' $\proc{F}$, $\proc{M}$ and $\proc{A}$. As \citet{feynman_feynman_1979} noted, this law is incomplete -- in order to understand it, we must bring some pre-existing understanding of force, mass and acceleration as independent things. Furthermore, the nature of this knowledge is somewhat perculiar. Acknowledging that physicists happen to know a great deal about forces on an object, it remains true that in order to actually say what the net force on a real object is, even a highly knowledgeable physicist will still have to go and do some measurements, and the result of such measurements will be a vector representing the net forces on that object.

This suggests that we can think about ``force'' $\proc{F}$ (or mass or acceleration) as a kind of procedure that we apply to a particular real world object and which returns a mathematical object (in this case, a vector). We will call $\proc{F}$ a \emph{procedure}. Our view of $\proc{F}$ is akin to \citet{menger_random_2003}'s notion of variables as ``consistent classes of quantities'' that consist of pairing between real-world objects and quantities of some type. Force $\proc{F}$ itself is not a well-defined mathematical thing, as measurement procedures are not mathematically well-defined. At the same time, the set of values it may yield \emph{are} well-defined mathematical things. No actual procedure can be guaranteed to return elements of a mathematical set known in advance -- anything can fail -- but we assume that we can study procedures reliable enough that we don't lose much by making this assumption.

\begin{definition}[Measurement procedure]
A \emph{measurement procedure} is a procedure that involves interacting with the real world somehow and delivering an element of a mathematical set as a result. The set of possible values is known prior to the measurement taking place, but the value that it will yield is not known. A procedure is given the font $\proc{B}$, we say it takes values in $X$ and $\proc{B}\yields x$ is the proposition that the the procedure $\proc{B}$ will yield the value $x\in X$. $\proc{B}\yields A$ for $A\subset X$ is the proposition $\lor_{x\in A} \proc{B}\yields x$. Two procedures $\proc{B}$ and $\proc{C}$ are the same if $\proc{B}\yields x\iff \proc{C}\yields x$ for all $x\in B$ (note that $\proc{B}$ and $\proc{C}$ could involve different actions in the real world).
\end{definition}

Measurement procedures are like functions without well-defined domains. We can compose measurement procedures with functions to produce new measurement procedures.

\begin{definition}[Composition of functions with procedures]
Given a procedure $\proc{B}$ that takes values in some set $B$, and a function $f:B\to C$, define the ``composition'' $f\circ \proc{B}$ to be any procedure $\proc{C}$ that yields $f(x)$ whenever $\proc{B}$ yields $x$. We can construct such a procedure by describing the steps: first, do $\proc{B}$ and secondly, apply $f$ to the value yielded by $\proc{B}$.
\end{definition}

For example, $\proc{MA}$ is the composition of $h:(x,y)\mapsto xy$ with the procedure $(\proc{M},\proc{A})$ that yields the mass and acceleration of the same object. Measurement procedure composition is associative:

\begin{align}
	(g\circ f)\circ\proc{B}\text{ yields } x &\iff B\text{ yields } (g\circ f)^{-1}(x) \\
	&\iff B\text{ yields } f^{-1}(g^{-1}(x))\\
	&\iff f\circ B \text{ yields } g^{-1}(x)\\
	&\iff g\circ(f\circ B)\text{ yields } x
\end{align}


One might whether there is also some kind of ``append'' operation that takes a standalone $\proc{M}$ and a standalone $\proc{A}$ and returns a procedure $(\proc{M},\proc{A})$. Unlike function composition, this would be an operation that acts on two procedures rather than a procedure and a function. Unlike composition, we can't easily reason about such an operation mathematically, because of the fact that measurment procedures have a foot in the real world. Our approach here is to suppose that there is some master measurement procedure $\proc{S}$ which takes values in $\Psi$ that handles all of the ``real world'' interaction relevant to our problem. Specifically, we assume that any measurement procedure of interest to our problem can be written as the composition $f\circ \proc{S}$ for some $f$.

For the model $\proc{F}=\proc{MA}$, for example, we could assume $\proc{F}=f\circ \proc{S}$ for some $f$ and $(\proc{M},\proc{A})=g\circ \proc{S}$ for some $g$. In this case, we can get $\proc{MA}=h\circ(\proc{M},\proc{A})=(h\circ g)\circ\proc{S}$. Note that each procedure is associated with a unique function with domain $\Psi$.

Given that measurement processes are in practice finite precision and with finite range, $\Psi$ will generally be a finite set. We can therefore equip $\Psi$ with the collection of measurable sets given by the power set $\sigalg{E}:=\mathscr{P}(\Psi)$, and $(\Psi,\sigalg{E})$ is a standard measurable space. $\sigalg{E}$ stands for a complete collection of logical propositions we can generate that depend on the results yielded by the measurement procedure $\proc{S}$.

$(\Psi,\sigalg{E})$ defines is a ``sample space'' limited to observable variables. That is, $(\Psi,\sigalg{E})$ is associated with a measurement procedure. Unobserved variables need not be associated with measurement procedures, and to accommodate these we use instead of $(\Psi,\sigalg{E})$ a richer sample space $(\Omega,\sigalg{F})$ which represents both observed and unobserved variables.

\begin{definition}[Sample space]
The sample space $(\Omega,\sigalg{F})$ is a set $\Omega$ along with with a $\sigma$-algebra $\sigalg{F}$ of subsets of $\Omega$.
\end{definition}

Observables are represented by a function $\RV{S}:\Omega\to \Psi$, and values of $\omega$ are related to propositions about measurement procedures via the criterion of \emph{consistency with observation}.

\begin{definition}[Consistency with observation]\label{def:observable}
An element $\omega\in \Omega$ is is \emph{consistent with observation} if the result yielded by $\proc{S}\yields \RV{S}(\omega)$
\end{definition}

Thus the procedure $\proc{S}$ restricts the observationally consistent elements of $\Omega$. If $\proc{S}$ yield the result $s$, then the consistent values of $\Omega$ will be $\RV{S}^{-1}(s)$. While two different sets of measurement outcomes $\Psi$ and $\Psi'$ entail a different mesurement procedures $\proc{S}$ and $\proc{S}'$, but different fundamental probability sets $\Omega$ and $\Omega'$ may be used to model a single procedure $\proc{S}$.

As far as we know, distinguishing variables from procedures is somewhat nonstandard, but we feel it is useful to distinguish the formal elements of our theory (variables) from the semi-formal elements (measurement procedures). Both variables and procedures are often discussed in statistical texts. For example, \citet{pearl_causality:_2009} offers the following two, purportedly equivalent, definitions of variables:
\begin{quote}
By a \emph{variable} we will mean an attribute, measurement or inquiry that may take on one of several possible outcomes, or values, from a specified domain. If we have beliefs (i.e., probabilities) attached to the possible values that a variable may attain, we will call that variable a random variable.
\end{quote}

\begin{quote}
This is a minor generalization of the textbook definition, according to which a random variable is a mapping from the fundamental probability set (e.g., the set of elementary events) to the real line. In our definition, the mapping is from the fundamental probability set to any set of objects called ``values,'' which may or may not be ordered.
\end{quote}

Our view is that the first definition is a definition of a procedure, while the second is a definition of a variable. Variables model procedures, but they are not the same thing. We can establish this by noting that, under our definition, every procedure of interest -- that is, all procedures that can be written $f\circ \proc{S}$ for some $f$ -- is modeled by a variable, but there may be variables defined on $\Omega$ that do not factorise through $\proc{S}$, and these variables do not model procedures.

\subsection{Events}

To recap, we have a procedure $\proc{S}$ yielding values in $\Psi$ that measures everything we are interested in, a fundamental probability set $\Omega$ and a function $\RV{S}$ that models $\proc{S}$ in the sense of Definition \ref{def:observable}. We assume also that $\Psi$ has a $\sigma$-algebra $\sigalg{E}$ (this may be the power set of $\Psi$, as measurement procedures are typically limited to finite precision). $\Omega$ is equipped with a $\sigma$-algebra $\sigalg{F}$ such that $\sigma(\RV{S})\subset \sigalg{F}$. If a procedure $\proc{X}=f\circ \RV{S}$ then we define $\RV{X}:\Omega\to X$ by $\RV{X}:=f\circ\RV{S}$.

If a particular procedure $\proc{X}=f\circ \proc{S}$ eventually yields a value $x$, then the values of $\Omega$ consistent with observation must be a subset of $\RV{X}^{-1}(x)$. We define an \emph{event} $\RV{X}\yields x:\equiv \RV{X}^{-1}(x)$, which we read ``the event that $\RV{X}$ yields x''. An event $\RV{X}\yields x$ occurs if the consistent values of $\Omega$ are a subset of $\RV{X}\yields x$, thus ``the event that $\RV{X}$ yields x occurs$\equiv \proc{X}$ yields $x$''. The definition of events applies to all types of variables, not just observables, but we only provide an interpretation of events ``occurring'' when the variable $\RV{X}$ is associated with some $\proc{X}$.

For measurable $A\in \sigalg{X}$, $\RV{X}\yields A=\bigcup_{x\in A} \RV{X}\yields x$. 

Given $\RV{Y}:\Omega\to X$, we can define a sequence of variables: $(\RV{X},\RV{Y}):=\omega\mapsto (\RV{X}(\omega),\RV{Y}(\omega))$. $(\RV{X},\RV{Y})$ has the property that $(\RV{X},\RV{Y})\yields (x,y)= \RV{X}\yields x\cap \RV{Y}\yields y$, which supports the interpretation of $(\RV{X},\RV{Y})$ as the values yielded by $\RV{X}$ and $\RV{Y}$ together.

It is common to use the symbol $=$ instead of $\bowtie$, but we want to avoid this because $\RV{Y}=y$ already has a meaning, namely that $\RV{Y}$ is a constant function everywhere equal to $y$. 

\subsection{Standard probability theory}

\begin{definition}[Probability measure]
Given a measure space $(X,\sigalg{X})$, a probability measure is a $\sigma$-additive function $\mu:\sigalg{X}\to [0,1]$ such that $\mu(\emptyset)=0$ and $\mu(X)=1$. We write $\Delta(X)$ for the set of all probability measures on $(X,\sigalg{X})$.
\end{definition}

\begin{definition}[Markov kernel]
Given measure spaces $(X,\sigalg{X})$, $(Y,\sigalg{Y})$ $\RV{Y}:\Omega\to Y$, a Markov kernel $\prob{Q}:X\kto Y$ is a map $Y\times \sigalg{X}\to [0,1]$ such that
\begin{enumerate}
	\item $y\mapsto \prob{Q}(A|y)$ is $\sigalg{B}$-measurable for all $A\in \sigalg{X}$
	\item $A\mapsto \prob{Q}(A|y)$ is a probability measure on $(X,\sigalg{X})$ for all $y\in Y$
\end{enumerate}
\end{definition}

\begin{definition}[Probability measures as Markov kernel]
Given $(X,\sigalg{X})$ and $\mu\in \Delta(X)$, the Markov kernel $\kernel{K}:\{*\}\kto X$ given by $\kernel{K}(A|*)=\mu(A)$ for all $A\in \sigalg{X}$ is the Markov kernel associated with the probability measure $\mu$. We will use probability measures and their associated Markov kernels interchangeably, as it is transparent how to get from one to another.
\end{definition}

\begin{definition}[Delta measure]
Given a measure space $(X,\sigalg{X})$ and $x\in X$, $\delta_x\in \Delta(X)$ is the measure defined by $\delta_x(A)=\llbracket x\in A \rrbracket$.
\end{definition}

\begin{definition}[Probability space]
A probability space is a triple $(\mu,\Omega,\sigalg{F})$, where $\mu$ is a base measure on $\sigalg{F}$.
\end{definition}

\begin{definition}[Marginal distribution with respect to a probability space]\label{def:pushforward}
Given a probability space $(\mu,\Omega,\sigalg{F})$ and a random variable $\RV{X}:\Omega\to (X,\sigalg{X})$, we can define the \emph{marginal distribution} of $\RV{X}$ with respect to $\mu$, $\mu^{\RV{X}}:\sigalg{X}\to [0,1]$ by $\mu^{\RV{X}}(A):=\mu(\RV{X}\yields A)$ for any $A\in \sigalg{X}$.
\end{definition}

\begin{definition}[Disintegration]
Given a Markov kernel $\kernel{K}:W\kto X\times Y$, with $W$, $X$ and $Y$ standard measurable, any kernel $\kernel{L}:W\times X\kto Y$ satisfying
\begin{align}
	\kernel{K} = \tikzfig{disintegration_existence}\label{eq:disint_def}
\end{align}
is a $W\times X\kto Y$ \emph{disintegration} of $\kernel{K}$.
\end{definition}

\begin{definition}[Conditional probability with respect to a probability space]\label{def:disint}
Given a probability space $(\mu,\Omega)$ and random variables $\RV{X}:\Omega\to (X,\sigalg{X})$, $\RV{Y}:\Omega\to (Y,\sigalg{Y})$, the probability of $\RV{Y}$ given $\RV{X}$ is any $X\kto Y$ disintegration of $\mu^{\RV{XY}}$. That is,
\begin{align}
	\int_{A} \mu^{\RV{Y}|\RV{X}}(B|x) \mathrm{d}\mu^{\RV{X}}(x)&= \mu^{\RV{XY}}(x,y) &\forall A\in \sigalg{X}, B\in \sigalg{Y}\\
	&\iff\\
	\tikzfig{disint_def} &= \mu^{\RV{XY}}
\end{align}
\end{definition}

\begin{lemma}[Marginal distribution as a kernel product]
Given a probability space $(\mu,\Omega,\sigalg{F})$ and a random variable $\RV{X}:\Omega\to (X,\sigalg{X})$, define $\kernel{F}_{\RV{X}}:\Omega\kto X$ by $\kernel{F}_{\RV{X}}(A|\omega)=\delta_{\RV{X}(\omega)}(A)$, then
\begin{align}
	\mu^{\RV{X}} = \mu\kernel{F}_{\RV{X}}
\end{align}
\end{lemma}

\begin{proof}
Consier any $A\in \sigalg{X}$.
\begin{align}
	\mu \kernel{F}_{\RV{X}}(A) &= \int_\Omega \delta_{\RV{X}(\omega)}(A) \mathrm{d}\mu(\omega)\\
	&= \int_{\RV{X}^{-1}(\omega)} \mathrm{d}\mu(\omega)\\
	&= \mu^{\RV{X}}(A)
\end{align}
\end{proof}

Disintegration of arbitrary Markov kernels is possible in standard measurable spaces. We will assume that all spaces are standard measurable, such that whenever we have a Markov kernel we are able to disintegrate it.

\begin{lemma}[Disintegration existence in standard measurable Markov kernels]\label{lem:disint_exist}
For any Markov kernel $\kernel{K}:X\kto W\times Y$ and $X$, $W$, $Y$ are standard measurable, there exists $\kernel{L}:W\times X\kto Y$ such that
\begin{align}
	\kernel{K} = \tikzfig{disintegration_existence}
\end{align}
$\kernel{L}$ is a \emph{disintegration} of $\kernel{K}$.
\end{lemma}

\begin{proof}
\citet{cho_disintegration_2019} Theorem 3.11
\end{proof}

\subsection{Probabilistic models for causal inference}

The sample space $(\Omega,\sigalg{F})$ along with our collection of variables is a ``model skeleton'' -- it tells us what kind of data we might see. The process $\proc{S}$ which tells us which part of the world we're interested in is related to the model $\Omega$ and the observable variables by the criterion of \emph{consistency with observation}. The kind of problem we are mainly interested in here is one where we make use of data to help make decisions under uncertainty. Probabilistic models have a long history of being used for this purpose, and our interest here is in constructing probabilistic models that can be attached to our variable ``skeleton''. 

Given a model skeleton, a common approach to attaching a probabilistic model involves defining a base measure $\mu$ on $(\Omega,\sigalg{F})$ which yields a probability space $(\Omega,\sigalg{F},\mu)$. For causal inference, we need a to generalise this approach, because we need to handle \emph{choices}. If I have different options I can choose, and I want to use a model to compare the options according to some criteria, then I need a model that can accept a choice and output the expected result of that choice. According to this model, anything that we consider a ``consequence of a choice'' doesn't have a definite probability, because it depends on the choice we make.

In general, we might have arbitrary sets of choices that map to probability models in an arbitrary way. However, we are here interested in a simpler case: we suppose that there are a number of points at which we can act, and prior to acting we can observe some variables, and we are able to choose probabilistic maps from observations to acts. We also assume that, given the same observation and the same act, the same consequence is expected. That is, the consequences do not depend directly way on the choice of map from observations to acts.

These assumptions together imply that our model should contain a number of fixed conditional probabilities -- the probabilities of consequences given observations and acts -- and a number of ``choosable'' conditional probabilities -- the probabilities of acts given observations. The fixed conditional probabilities form a probability model with \emph{gaps}, and those gaps correspond to choices we can make. When we combine the fixed conditional probabilities and a choice of a conditional probability for each gap, we get a regular probability model. The terminology of ``probability gaps'' comes from \citet{hajek_what_2003}. 

To restate our general approach: we model decision problems with a collection of fixed conditional probabilities and a collection of choosable conditional probabilities, and combine the fixed conditionals with particular choices to get a probability measure. Two issues present themselves here: firstly, what \emph{is} a collection of conditional probabilities without a fixed underlying probability measure? Secondly, we need to ensure that our chosen collection of conditional probabilities actually does induce a probability model. We address these questions with \emph{probability sets}. A probability set is a collection of probability measures on $(\Omega,\sigalg{F})$, and we identify a collection of conditional probabilities with the set of probability measures that induce those conditional probabilities. We then define an operation $\odot$ for combining conditional probabilities, and a criterion of \emph{validity} such that a collection of valid conditional probabilities recursively combined using $\odot$ is guaranteed to corresponds to a non-empty probability set.

\subsection{Probability sets}

A probability set is a set of probability measures. This section establishes a number of useful properties of conditional probability with respect to probability sets. Unlike conditional probability with repsect to a probability space, conditional probabilities don't always exist for probability sets. Where they do, however, they are almost surely unique and we can marginalise and disintegrate them to obtain other conditional probabilities with respect to the same probability set.

\begin{definition}[Probability set]
A probability set $\prob{P}_{\{\}}$ on $(\Omega,\sigalg{F})$ is a collection of probability measures on $(\Omega,\sigalg{F})$. In other words it is a subset of $\mathscr{P}(\Delta(\Omega))$, where $\mathscr{P}$ indicates the power set.
\end{definition}

Given a probability set $\prob{P}_{\{\}}$, we define marginal and conditional probabilities as probability measures and Markov kernels that satisfy Definitions \ref{def:pushforward} and \ref{def:disint} respectively for \emph{all} base measures in $\prob{P}_{\{\}}$. There are generally multiple Markov kernels that satisfy the properties of a conditional probability with respect to a probability set, and this definition ensures that marginal and conditional probabilities are ``almost surely'' unique (Definition \ref{def:asequal}) with respect to probability sets.

\begin{definition}[Marginal probability with respect to a probability set]
Given a sample space $(\Omega,\sigalg{F})$, a variable $\RV{X}:\Omega\to X$ and a probability set $\prob{P}_{\{\}}$, the marginal distribution $\prob{P}_{\{\}}^{\RV{X}}=\prob{P}_\alpha^{\RV{X}}$ for any $\prob{P}_\alpha\in\prob{P}_{\{\}}$ if a distribution satisfying this condition exists. Otherwise, it is undefined.
\end{definition}

\begin{definition}[Conditional probability with respect to a probability set]\label{def:cprob_pset}
Given a fundamental probability set $\Omega$ variables $\RV{X}:\Omega\to X$ and $\RV{Y}:\Omega\to Y$ and a probability set $\prob{P}_{\{\}}$, \emph{a version of} $\prob{P}_{\{\}}^{\RV{Y}|\RV{X}}$ is any Markov kernel $X\kto Y$ such that $\prob{P}_{\{\}}^{\RV{Y}|\RV{X}}$ is an $X\kto Y$ disintegration of $\prob{P}_\alpha^{\RV{XY}}$ for all $\prob{P}_\alpha\in \prob{P}_{\{\}}$. If no such Markov kernel exists, $\prob{P}_{\{\}}^{\RV{Y}|\RV{X}}$ is undefined.
\end{definition}

Given a conditional probability with respect to a probability set, we can find other conditional probabilities by ``pushing it forward''.

% \begin{lemma}[Equivalence of pushforward definitions]\label{lem:prod_pushf}
% Given a probability space $\kernel{M}:W\to \Omega$ and $\RV{X}:\Omega\to X$, define $\kernel{K}^{\RV{X}|\RV{W}}:W\kto X$ by $\kernel{K}^{\RV{X}|\RV{W}}(x|w):=\kernel{M}(\RV{X}\yields x|w)$ for any $x\in X$m $w\in W$ and $\kernel{L}^{\RV{X}}:W\kto X$ by
% \begin{align}
% 	\kernel{L}^{\RV{X}|\RV{W}} = \kernel{M}\kernel{F}_{\RV{X}}
% \end{align}
% Then
% \begin{align}
% \kernel{L}^{\RV{X}|\RV{W}} =\kernel{K}^{\RV{X}|\RV{W}}
% \end{align}
% \end{lemma}

% \begin{proof}
% For any $x\in X$, $w\in W$
% \begin{align}
% 	\kernel{L}^{\RV{X}|\RV{W}}(x|w) &= \sum_{\omega\in \Omega} \llbracket x=\RV{X}(\omega)\rrbracket \kernel{M}(\omega|w)\\
% 									&= \sum_{\omega\in \RV{X}^{-1}(x)} \kernel{M}(\omega|w)\\
% 									&= \kernel{M}(\RV{X}\yields x|w)\\
% 									&= \kernel{K}^{\RV{X}|\RV{W}}(x|w)
% \end{align}
% \end{proof}

\begin{theorem}[Recursive pushforward]\label{th:recurs_pushf}
Suppose we have a sample space $\Omega$ variables $\RV{X}:\Omega\to X$ and $\RV{Y}:\Omega\to Y$, $\RV{Z}:\Omega\to Z$ and a probability set $\prob{P}_{\{\}}$ such that $\prob{P}_{\{\}}^{\RV{X}|\RV{Y}}$ is a $\RV{Y}|\RV{X}$ conditional probability of $\prob{P}_{\{\}}$ and $\RV{Z}=f\circ \RV{Y}$ for some $f:Y\to Z$. Then there exists a $\RV{Z}|\RV{X}$ conditional probability of $\prob{P}_{\{\}}$ given by $\prob{P}_{\{\}}^{\RV{Z}|\RV{X}}=\prob{P}_{\{\}}^{\RV{Y}|\RV{X}}\kernel{F}_{f}$.
\end{theorem}

\begin{proof}
For any $\prob{P}_\alpha\in \prob{P}_{\{\}}$, $x,z$
\begin{align}
\prob{P}_\alpha^{\RV{X}}(x)\prob{P}_\alpha^{\RV{Z}|\RV{X}}(z|x) &= \prob{P}_\alpha(\RV{X}^{-1}(x)\cap\RV{Z}^{-1}(z))\\
					   &= \prob{P}_\alpha(\RV{X}^{-1}(x)\cap\RV{Y}^{-1}(f^{-1}(z)))\\
					   &= \prob{P}_\alpha^{\RV{X},\RV{Y}}(\{x\}\times f^{-1}(z))\\
					   &= \prob{P}_\alpha^{\RV{X}}(x)\prob{P}_\alpha^{\RV{Y}|\RV{X}}(f^{-1}(z)|x)
\end{align}
\end{proof}

We define the copy-product $\odot$ as a shorthand for the operation in Equation \ref{eq:disint} that combines a marginal with a disintegration to get the original Markov kernel back.

\begin{definition}[Semidirect product]\label{def:copyproduct}
Given $\prob{K}:X\kto Y$ and $\prob{L}:Y\times X\kto Z$, define the copy-product $\prob{K}\odot\prob{L}:X\to Y\times Z$ as
\begin{align}
	\prob{K}\odot\prob{L}:&= \text{copy}_X(\prob{K}\otimes \text{id}_X)(\text{copy}_Y\otimes\text{id}_X )(\text{id}_Y \otimes \prob{L})\\
							&= \tikzfig{copy_product}\\
							&\iff\\
	(\prob{K}\odot\prob{L})(A\times B|x) &= \int_A \prob{L}(B|y,x)\prob{K}(dy|x)&A\in \sigalg{Y},B\in\sigalg{Z}
\end{align}
\end{definition}

\begin{lemma}[Semidirect product is associative]
Given $\prob{K}:X\kto Y$, $\prob{L}:Y\times X\kto Z$ and $\prob{M}:Z\times Y\times X\kto W$
\begin{align}
	(\prob{K}\odot \prob{L})\odot \prob{Z} &= \prob{K}\odot(\prob{L}\odot\prob{Z})\\
\end{align}
\end{lemma}

\begin{proof}
\begin{align}
	(\prob{K}\odot \prob{L})\odot \prob{M} &= \tikzfig{odot_assoc_1}\\
											&=  \tikzfig{odot_assoc_2}\\
											&= \prob{K}\odot (\prob{L}\odot \prob{M})
\end{align}
\end{proof}

Two Markov kernels are almost surely equal with respect to a probability set $\prob{P}_{\{\}}$ if the semidirect product $\odot$ of all marginal probabilities of $\RV{X}$ with each Markov kernel is identical.

\begin{definition}[Almost sure equality]\label{def:asequal}
Two Markov kernels $\kernel{K}:X\kto Y$ and $\kernel{L}:X\kto Y$ are almost surely equal $\overset{\prob{P}_{\{\}}}{\cong}$ with respect to a probability set $\prob{P}_{\{\}}$ and variable $\RV{X}:\Omega\to X$ if for all $\prob{P}_\alpha \in \prob{P}_{\{\}}$,
\begin{align}
	\prob{P}^{\RV{X}}_\alpha\odot \kernel{K}=\prob{P}^{\RV{X}}_\alpha\odot \kernel{L}
\end{align}
\end{definition}

\begin{lemma}[Conditional probabilities are almost surely equal]
If $\kernel{K}:X\kto Y$ and $\kernel{L}:X\kto Y$ are both versions of $\prob{P}_{\{\}}^{\RV{Y}|\RV{X}}$, $\kernel{K}\overset{\prob{P}_{\{\}}}{\cong}\kernel{L}$
\end{lemma}

\begin{proof}
For all $\prob{P}_\alpha \in \prob{P}_{\{\}}$
\begin{align}
	\prob{P}^{\RV{X}}_\alpha\odot \kernel{K} &= \prob{P}^{\RV{XY}}_\alpha\\
	&= \prob{P}^{\RV{X}}_\alpha\odot \kernel{L}
\end{align}
\end{proof}

\begin{lemma}[Substitution of almost surely equal Markov kernels]
Given $\prob{P}_{\{\}}$, if $\kernel{K}:X\times Y \kto Z$ and $\kernel{L}:X\times Y \kto Z$ are almost surely equal $\kernel{K}\overset{\prob{P}_{\{\}}}{\cong}\kernel{L}$, then for any $\prob{P}_\alpha\in \prob{P}_{\{\}}$
\begin{align}
	\prob{P}_\alpha^{\RV{Y}|\RV{X}}\odot \kernel{K} &\overset{a.s.}{\cong} \prob{P}_\alpha^{\RV{Y}|\RV{X}}\odot \kernel{L}
\end{align}
\end{lemma}

\begin{proof}
For any $\prob{P}_\alpha\in\prob{P}_{\{\}}$
\begin{align}
	\prob{P}_\alpha^{\RV{XY}}\odot \kernel{K} &= (\prob{P}_\alpha^{\RV{X}}\odot \prob{P}_{\{\}}^{\RV{Y}|\RV{X}})\odot \kernel{K}\\
											  &= \prob{P}_\alpha^{\RV{X}}\odot (\prob{P}_{\{\}}^{\RV{Y}|\RV{X}}\odot \kernel{K})\\
											  &= \prob{P}_\alpha^{\RV{X}}\odot (\prob{P}_{\{\}}^{\RV{Y}|\RV{X}}\odot \kernel{L})
\end{align}
\end{proof}

\begin{lemma}[Semidirect product of conditionals is a joint conditional]\label{lem:joint_conditional}
Given a probability set $\prob{P}_{\{\}}$ on $(\Omega,\sigalg{F})$ along with conditional probabilities $\prob{P}_{\{\}}^{\RV{Y}|\RV{X}}$ and $\prob{P}_{\{\}}^{\RV{Z}|\RV{XY}}$, $\prob{P}_{\{\}}^{\RV{YZ}|\RV{X}}$ exists and is equal to
\begin{align}
	\prob{P}_{\{\}}^{\RV{YZ}|\RV{X}} &= \prob{P}_{\{\}}^{\RV{Y}|\RV{X}}\odot \prob{P}_{\{\}}^{\RV{Z}|\RV{XY}}\\
\end{align}
\end{lemma}

\begin{proof}
By definition, for any $\prob{P}_\alpha\in \prob{P}_{\{\}}$
\begin{align}
	\prob{P}_\alpha^{\RV{XYZ}} &= \prob{P}_\alpha^{\RV{X}}\odot \prob{P}_\alpha^{\RV{YZ}|\RV{X}}\\
							   &= \prob{P}_\alpha^{\RV{X}}\odot(\prob{P}_\alpha^{\RV{Y}|\RV{X}}\odot \prob{P}_\alpha^{\RV{Z}|\RV{YX}})\\
							   &= \prob{P}_\alpha^{\RV{X}}\odot(\prob{P}_{\{\}}^{\RV{Y}|\RV{X}}\odot \prob{P}_{\{\}}^{\RV{Z}|\RV{YX}})
\end{align}
\end{proof}



% \begin{theorem}[Disintegrations are conditional probabilities]
% Suppose we have a fundamental probability set $\Omega$ variables $\RV{W}:\Omega\to W$, $\RV{X}:\Omega\to X$, $\RV{Y}:\Omega\to Y$ and $\RV{Z}:\Omega\to Z$ and a probability set $\prob{P}_{\{\}}$ such that $\prob{P}_{\{\}}^{\RV{X}|\RV{Y}}$ is a $\RV{Y}|\RV{X}$ conditional probability and there is some $\kernel{K}^{\RV{$
% \end{theorem}

% Given a conditional probability with respect to a probability gap model, we can also find additional conditional probabilities by disintegrating the original conditional probability.

% \begin{lemma}[Recursive disintegration]
% Suppose we have a fundamental probability set $\Omega$, variables $\RV{W}:\Omega\to W$, $\RV{X}:\Omega\to X$ and $\RV{Y}:\Omega\to Y$, $\RV{Z}:\Omega\to Z$ and a probability set $\prob{P}_{\{\}}$ such that $\prob{P}_{\{\}}^{\RV{X}|\RV{Y}}$ is a $\RV{Y}|\RV{X}$ conditional probability. Define $\prob{Q}_{\{\}}$ as the largest probability set such that $\prob{Q}_{\{\}}^{\RV{Y}|\RV{X}}=\prob{P}_{\{\}}^{\RV{Y}|\RV{X}}$. Then if $\prob{Q}_{\{\}}^{\RV{Z}|\RV{W}}$ is a $\RV{Z}|\RV{W}$ conditional probability of $\prob{Q}_{\{\}}$, it is also a $\RV{Z}|\RV{W}$ conditional probability of $\prob{P}_{\{\}}$.
% \end{lemma}

% \begin{proof}
% $\prob{Q}_{\{\}}\supset \prob{P}_{\{\}}$, so any conditional probability of $\prob{Q}_{\{\}}$ is also a conditional probability of $\prob{P}_{\{\}}$.
% \end{proof}

\subsection{Probability sets defined by marginal and conditional probabilities}

In the previous section we defined marginal and conditional probabilities for probability sets. Here we will go in the other direction: define probability sets by specifying key marginal or conditional probabilities. There is an issue to be careful of here: not all probability measures $\prob{Q}^{\RV{X}}$ on $X$ define nonempty sets of probability measures on $\Omega$ with respect to the variable $\RV{X}$. Consider, for example, $\RV{X}=(\RV{Y},\RV{Y})$ for some $\RV{Y}:\Omega\to \{0,1\}$ and any measure $\prob{Q}^{\RV{YY}}$ that assigns nonzero probability to the event $(\RV{Y},\RV{Y})\yields (1,0)$. There is no base measure that pushes forward to such a $\prob{P}^{\RV{YY}}$, because two copies of the same variable must always be deterministically equal. A \emph{valid distribution} is a distribution associated with a particular variable that defines a nonempty set of base measures on $\Omega$ (Theorem \ref{th:completion}).

\begin{definition}[Valid distribution]\label{def:valid_dist}
A valid $\RV{X}$ probability distribution $\prob{P}^{\RV{X}}$ is any probability mesure on $\Delta(X)$ such that $\RV{X}^{-1}(A)=\emptyset\implies \prob{P}^{\RV{X}}(A) = 0$ for all $A\in\sigalg{X}$.
\end{definition}

\emph{Valid conditionals} not only define a nonempty set of base measures on $\Omega$, but the $\odot$ product of two appropriately typed valid conditionals itself defines a nonempty set of base measures on $\Omega$ (Lemma \ref{lem:valid_extendability}). 

\begin{definition}[Valid conditional]\label{def:valid_conditional_prob}
Given $(\Omega,\sigalg{F})$, $\RV{X}:\Omega\to X$, $\RV{Y}:\Omega\to Y$ a \emph{valid $\RV{Y}|\RV{X}$ conditional probability} $\prob{P}^{\RV{Y}|\RV{X}}$ is a Markov kernel $X\kto Y$ such that it assigns probability 0 to contradictions:
\begin{align}
    \forall B\in \sigalg{Y}, x\in \sigalg{X}: (\RV{X},\RV{Y})\yields \{x\}\times B = \emptyset \implies \left(\prob{P}^{\RV{Y}|\RV{X}}(A|x) = 0\right) \lor \left(\RV{X}\yields \{x\} = \emptyset\right)
\end{align}
\end{definition}

Thus, given a collection of valid conditional probabilities $\{\prob{P}_i^{\RV{X}_i|\RV{X}_{[i-1]}}|i\in [n]\}$ such that each adjacent pair can be combined with the $\odot$ product, the sequential product of each conditional probability is a valid conditional probability and there is a non-empty set of probability measures on the sample space that with that conditional probability. Collections of recursive conditional probabilities often arise in causal modelling -- in particular, they are the foundation of the structural equation modelling approach \citet{richardson2013single,pearl_causality:_2009}. Lemma \ref{lem:valid_extendability} establishes that recursive collections of conditional probabilities define non-empty probability sets as long as all the conditional probabilities in the collection are valid. 

\begin{definition}[Probability set defined by a valid conditional]
If $\prob{P}_{\{\}}$ is a probability set such that there is a valid conditional probability $\prob{P}_{\{\}}^{\RV{Y}|\RV{X}}:X\kto Y$ and for every $\mu\in \Delta(\Omega)$ such that $\mu^{\RV{Y}|\RV{X}}\overset{\mu}{\cong}\prob{P}_{\{\}}^{\RV{Y}|\RV{X}}$, we say $\prob{P}_{\{\}}^{\overline{\RV{Y}|\RV{X}}}:=\prob{P}_{\{\}}$ is the probability set defined by $\prob{P}_{\{\}}^{\RV{Y}|\RV{X}}$.
\end{definition}

\subsection{Probability gap models}

A probability set gives us a collection of different probability models that ``could'' model the consequences of an action. In addition to possibilities, we also want \emph{choices} and a model that tells us, given a particular choice, we realise a particular possibility. This is what we call a \emph{probability gap model}. The general form of a probability gap models is

\begin{itemize}
	\item A fixed probability set $\prob{P}_{\{\}}\subset \Delta(\Omega)$ which we call the \emph{model}
	\item A collection of probability sets $A\subset \Delta(\Omega)$ that we call \emph{choices}
	\item A map $\prob{P}_\square: A\to \mathscr{P}(\Delta(\Omega))$ defined by $\prob{P}_\alpha:=\prob{P}_\square(\alpha)=\prob{P}_{\{\}}\cap\alpha$
\end{itemize}

We require that the choices are compatible with the model in the sense that $\prob{P}_{\{\}}\cap\alpha\neq \emptyset$ for all $\alpha\in A$. Here, we will limit our attention to a a particular type of probability gap model, where we define the probability set $\prob{P}_{\{\}}$ is defined by a conditional probability and each choice is defined by a marginal probability relative to the same variable.

\begin{definition}[Conditional probability model]
A \emph{conditional probability model} $\prob{P}_\square$ is a probability gap model $(\prob{P}_{\{\}}^{\overline{\RV{Y}|\RV{X}}},A)$ such that each $\alpha\in A$ is some probability set  defined by an $\RV{X}$-valid marginal probability $\alpha^{\overline{\RV{X}}}$.
\end{definition}

We will compute the intersection $\prob{P}_\alpha$ between the model $\prob{P}_{\{\}}$ and a choice $\alpha\in A$ as the probability set $\prob{P}_{\alpha}^{\overline{\RV{XY}}}$ such that:
\begin{align}
	\prob{P}_\alpha^{\RV{XY}} &= \alpha^{\RV{X}}\odot \prob{P}_{\{\}}^{\RV{Y}|\RV{X}}\label{eq:semidirect_intersection}
\end{align}

This is justified by Lemma \ref{th:intersection}, which says that the probability set defined by Equation \ref{eq:semidirect_intersection} is equivalent to the intersection of $\alpha$ and $\prob{P}_{\{\}}$.

If the conditional probability $\prob{P}_{\{\}}^{\RV{Y}|\RV{X}}$ and all the marginal probabilities $\alpha^{\overline{\RV{X}}}$ are valid, then by Lemma \ref{lem:valid_extendability} $\prob{P}_{\{\}}\cap\alpha \neq \emptyset$ for all $\alpha\in A$. Thus validity of all the individual parts is enough to ensure compatibility.

We can define more complex probability gap models with a similar approach where, for example, the model is specified by an incomplete collection of conditional probabilities and the choices are each a complementary collection of conditional probabilities; we call such models \emph{probability comb models} after \citet{chiribella_quantum_2008,jacobs_causal_2019}, but we will not address them in this paper.

\subsection{Example: invalidity}

Body mass index is defined as a person's weight divided by the square of their height. Suppose we have a measurment process $\proc{S}=(\proc{W},\proc{H})$ and $\proc{B}=\frac{\proc{W}}{\proc{H}^2}$ - i.e. we figure out someone's body mass index first by measuring both their height and weight, and then passing the result through a function that divides the second by the square of the first. Thus, given the random variables $\RV{W},\RV{H}$ modelling $\proc{W},\proc{H}$, $\proc{B}$ is the function given by $\RV{B}=\frac{\RV{W}}{\RV{H}^2}$. Given $x\in \mathbb{R}$, consider the conditional probability
\begin{align}
	\nu^{\RV{B}|\RV{WH}} &= \tikzfig{invalid_BMI_model} \label{eq:bmi_example}
\end{align}
Then pick some $w,h\in\mathbb{R}$ such that $\frac{w}{h^2}\neq x$ and $(\RV{W},\RV{H})\yields (w,h)\neq \emptyset$ (our measurement procedure could possibly yield $(w,h)$ for a person's height and weight). We have $\nu^{\RV{B}|\RV{WH}}(x|w,h)=1$, but 
\begin{align}
	(\RV{B},\RV{W},\RV{H})\yields \{(x,w,h)\} &= \{\omega|(\RV{W},\RV{H})(\omega) = (w,h),\RV{B}(\omega) = \frac{w}{h^2}\}\\
	&=\emptyset
\end{align}
so $\nu^{\RV{B}|\RV{WH}}$ is invalid, and there is some valid $\mu^{\RV{X}}$ such that the probability set $\prob{P}_{\{\}}$ with $\prob{P}_{\{\}}^{\RV{XY}} = \mu^{\RV{X}}\odot \nu^{\RV{Y}|\RV{X}}$ is empty.

Validity rules out conditional probabilities like \ref{eq:bmi_example}. We guess that in many cases this condition may either be trivial or unconsiously taken into account when constructing conditional probabilities. However, if we are not cognizant of the conditional our model depends on, we may inadvertently propose a model that depends on invalid conditional probabilities. For example, the conditional probability \ref{eq:bmi_example} would be used to evaluate the causal effect of body mass index in the causal diagram found in \citet{shahar_association_2009}, presuming the author used the term ``causal effect'' to depend somehow on the function $x\mapsto P(\cdot|do(\RV{B}=x))$ as is the usual convention when discussing causal Bayesian networks.

\subsubsection{Conditional independence}\label{ssec:cond_indep}

Conditional independence has a familiar definition in probability models. We define conditional independence with respect to a probability gap model to be equivalent to conditional independence with resepect to every base measure in the range of the model. This definition is closely related to the idea of \emph{extended conditional independence} proposed by \citet{constantinou_extended_2017}, see Appendix \ref{ap:eci}.

\begin{definition}[Conditional independence with respect to a probability set]
For a \emph{probability set} $\model{P}_{\{\}}$ and variables $\RV{A},\RV{B},\RV{Z}$, we say $\RV{B}$ is conditionally independent of $\RV{A}$ given $\RV{C}$, written $\RV{B}\CI_{\model{P}_{\{\}}}\RV{A}|\RV{C}$, if
\begin{align}
	\kernel{P}_{\{\}}^{\RV{ABC}} &= \tikzfig{cond_indep1}
\end{align}
\end{definition}

\citet{cho_disintegration_2019} have shown that this definition coincides with the standard notion of conditional independence for a particular probability model. In particular, it satisfies the \emph{semi-graphoid axioms}.

\begin{enumerate}
	\item Symmetry: $\RV{A}\CI_{\prob{P}} \RV{B}|\RV{C}$ iff $\RV{B}\CI_{\prob{P}} \RV{A}|\RV{C}$
	\item Decomposition: $\RV{A}\CI_{\prob{P}} (\RV{B},\RV{C})|\RV{W}$ implies $\RV{A}\CI_{\prob{P}}\RV{B}|\RV{W}$ and $\RV{A}\CI_{\prob{P}_\square}\RV{C}|\RV{W}$
	\item Weak union: $\RV{A}\CI_{\prob{P}}(\RV{B},\RV{C})|\RV{W}$ implies $\RV{A}\CI_{\prob{P}}\RV{B}|(\RV{C},\RV{W})$
	\item Contraction: $\RV{A}\CI_{\prob{P}}\RV{C}|\RV{W}$ and $\RV{A}\CI_{\prob{P}}\RV{B}|(\RV{C},\RV{W})$ implies $\RV{A}\CI_{\prob{P}_\square}(\RV{B},\RV{C})|\RV{W}$
\end{enumerate}

\begin{theorem}\label{th:cho_ci_equiv}
Given standard measurable $\Omega$, a probability model $\prob{P}$ and variables $\RV{W}:\Omega\to W$, $\RV{X}:\Omega\to X$, $\RV{Y}:\Omega\to Y$, $\RV{Y}\CI_{\prob{P}}\RV{X}|\RV{W}$ if and only if there exists some version of $\prob{P}^{\RV{Y}|\RV{WX}}$ and $\prob{P}^{\RV{Y}|\RV{W}}$ such that
\begin{align}
	\prob{P}^{\RV{Y}|\RV{WX}} &= \tikzfig{cond_indep_erase}\\
	\iff
	\prob{P}^{\RV{Y}|\RV{WX}}(y|w,x) &= \prob{P}^{\RV{Y}|\RV{W}}(y|w)
\end{align}
\end{theorem}

\begin{proof}
See \citet{cho_disintegration_2019}.
\end{proof}

The semi-graphoid axioms hold for all probability measures $\prob{P}$, so in particualr they hold for all $\prob{P}_\alpha\in \prob{P}_{\{\}}$. Thus conditional independence with respect to a probability set also satisfies the semi-graphoid axioms.

% \begin{definition}[Conditional independence with respect to a probability comb]
% Conditional independence $\RV{A}\CI_{\prob{P}_\square}\RV{B}|\RV{C}$ holds for an arbitrary probability comb $\model{P}_\square:A\to \mathscr{P}(\Delta(\Omega))$ if $\RV{A}\CI_{\prob{P}_\alpha}\RV{B}|\RV{C}$ holds for all probability models $\prob{P}_\alpha$, $\alpha\in A$.
% \end{definition}

\subsection{Curried Markov kernels}\label{sec:curry}

Given a function $f:X\times Y\to Z$, we can obtain a curried version $\lambda f:Y\to Z^X$. In particular, if $Y=\{*\}$ then $\lambda f:\{*\}\to Y^X$. At least for countable $X$, we can apply this construction to Markov kernels: given a kernel $\kernel{K}:X\kto Y$, define $\lambda \kernel{K}: \{*\}\kto Y^X$ by 
\begin{align}
	\lambda \kernel{K} ((y_i)_{i\in X}) &= \prod_{i\in X} \kernel{K}(y_i|i)
\end{align}

We can then define an evaluation map $\text{ev}:Y^X\times X\to Y$ by $\text{ev}((y_i)_{i\in X},x)=y_x$. Then

\begin{align}
	\kernel{K} = (\lambda \kernel{K}\otimes \text{id}_X) \kernel{F}_{\text{ev}} \label{eq:curry_identity}
\end{align}

Unlike the case of function currying, $\lambda \kernel{K}$ is not the unique Markov kernel for which \ref{eq:curry_identity} holds. In fact, we can substitute any $\kernel{L}$ such that, for any $i\in X$

\begin{align}
	\sum_{y_{\{i\}^C\in Y^{|X|-1}}} \kernel{L}((y_i)_{i\in X}) = \kernel{K}(y_i|i)
\end{align}

Evaulation of a curried Markov kernel $\lambda \kernel{K}$ resembles the definition of \emph{potential outcomes}; for outcomes $\RV{Y}:\Omega\to Y$ and treatments $X:\Omega\to X$, potential outcomes are described by a probability distribution $\prob{P}^{\RV{Y}^X}$ on $Y^X$ and we have the relation

\begin{align}
	\RV{Y} \overset{a.s.}{=} \text{ev}(\RV{Y}^X,\RV{X})
\end{align}

Then
\begin{align}
	(\prob{P}^{\RV{Y}^X}\otimes \text{id}_X)\kernel{F}_{\text{ev}}
\end{align}

is some Markov kernel $\kernel{K}:X\kto Y$, which is equal to $\prob{P}^{\RV{Y}|\RV{X}}$ if $\RV{Y}^X\CI |\RV{X}$. However, potential outcomes models typically do not explain what the kernel $\kernel{K}$ represents, and instead offer a definition of the variable $\RV{Y}^X$. For $x\in X$, the component $\RV{Y}^x$ of $\RV{Y}^X$ is usually said to express ``the outcomes that would have been observed, if $\RV{X}$ was $x$''.

Our original motivating question was ``when are potential outcomes well-defined?''. We're not actually going to try to answer this question, because our aim is not to tell people using potential outcomes how to do it. Furthermore, that question invites controversy we are not particularly interested in joining; \citet{dawid_causal_2000} and \citet{richardson2013single} have both argued that it is better to use equivalence classes of potential outcomes models induced by a criterion of distinguishability by experiment, while \citet{pearl_causality:_2009} advocates for models that can make finer distinctions than this.

However, given a probability gap model $\prob{P}_\square$, we do have a natural notion of the well-definedness of a conditional probability $\prob{P}^{\RV{Y}|\RV{X}}_\square$ -- it is well-defined when $\prob{P}_\alpha^{\RV{Y}|\RV{X}}$ is equal for all $\alpha$ (Definition \ref{def:cprob_pset}). Furthermore, the formal conditions that guarantee the existence of such a conditional probability very closely resemble the \emph{stable unit treatment value assumption} (SUTVA), which is said to be necessary for the existence of potential outcomes \citet{rubin_causal_2005}:

\begin{blockquote}
(SUTVA) comprises two subassumptions. First, it assumes that \emph{there is no interference between units (Cox 1958)}; that is, neither $Y_i(1)$ nor $Y_i(0)$ is affected by what action any other unit received. Second, it assumes that \emph{there are no hidden versions of treatments}; no matter how unit $i$ received treatment $1$, the outcome that would be observed would be $Y_i(1)$ and similarly for treatment $0$.
\end{blockquote}

The added emphasis is ours. In the next section, we offer formal criteria that correspond to these two statements.