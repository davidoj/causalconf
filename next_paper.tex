

\subsubsection{Graphical properties of conditional independence}

It is well-known that directed acyclic graphs are able to represent some conditional independence properties of probability models via the graphical property of \emph{d-separation}. String diagrams are similar to directed acyclic graphs, and string diagrams can be translated into directed acyclic graphs and vise-versa \citep{fong_causal_2013}. Thus we expect that a property analogous to d-separation can be defined for string diagrams.

We can reason from graphical properties of model disintegrations to graphical properties of models as Theorem \ref{th:cons_ci}. A general theory akin to d-separation for string diagrams may facilitate a more general understanding of how conditional independence properties of a model relate to conditional independence properties of its components.


\subsection{Extending contractible do models with observations}

We have stipulated that, given a do model $\prob{P}_\square$ and a decision function $\alpha$, $\prob{P}_\alpha$ is a model of the observations given the choice of $\alpha$. We can obtain a model of observations and decisions by specifying ``half a decision function''. Define an equivalence class $\sim$ on decision functions $A$ such that $\alpha\sim\alpha'$ if $\prob{P}_\alpha^{\RV{D}_1}=\prob{P}_{\alpha'}^{\RV{D}_1}$. By contractibility, $\alpha\sim\alpha'\implies \prob{P}_\alpha^{\RV{D}_1\RV{Y}_1}=\prob{P}_{\alpha'}^{\RV{D}_1\RV{Y}_1}$. Take $B=A/\sim$; then for $b\in B$ we have $\prob{P}_b^{\RV{D}_1\RV{Y}_1}$ exists.

Fixing some $b\in B$ fixes a distribution of observations for $(\RV{D}_1,\RV{Y}_1)$ and leaves the rest of the sequence unspecified.

If we are planning to conduct a series of experiments, we think a causally contractible do model is appropriate for these experiments and we have a decision function chosen, then we can restrict the do model to a model of observations by inserting the decision function. If we fix a decision function for the first $[n]$ experiments and leave the remaining decisions as a probability gap, then we get a see-do model. Combined with assumptions of full support, this see-do model is identificable. Thus the pair of assumptions
\begin{itemize}
    \item We can get the same result from performing the same action
    \item We have a specific plan for which actions to perform
\end{itemize}
imply identifiability, making them similar in consequence to the assumptions of ignorability (potential outcomes are independent of choices) or causal sufficiency (all causes necessary for identification are observed). However, we may have different reasons for considering these different assumptions to be justified -- or not.

There may be many cases where we feel that the first assumption is justifiable but causal identifiability is not licensed. Thus (perhaps surprisingly) the second assumption is quite crucial to the property of identifiability. When we weaken this assumption, we recover familiar rules of causal inference and (non)-identifiability. That is, in this view it is \emph{not} causal contractibility but the existence of a decision function that distinguishes identifiable from non-identifiable models.

Causal identifiability is considered more sound in RCTs, and also people say ``surely we can't define causes by reference to RCTs''. RCTs ensure the existence of a known decision function. We suggest that observational causal inference may proceed from weaker assumptions regarding the decision function.





We can derive the truncated factorisation rule from assuming 1) a causally contractible do model and 2) an unobserved, exchangeable ``observational decision function''. This derivation is attractive because
\begin{itemize}
    \item If we assume there is a unique interventional distribution, it cannot be defined by causal relationships, an observational distribution and the truncated factorisation rule. Why? Because truncated factorisation rule sometimes defines things that don't exist, and sometimes multiple things satisfy the requirements it imposes
    \item On the other hand, by the representation theorem above, a unique ``IID'' interventional distribution $\iff$ a causally contractible do-model
    \item Furthermore, causally contractible do-model + exchangeable observational decision rule $\implies$ truncated factorisation (but not the other way around!)
\end{itemize}

\todo[inline]{todo, below is just copied and pasted for now}

We will consider a motivating example initially posed using the language of causal Bayesian networks. For this example, we will assume that the reader is familiar enough with causal Bayesian networks to follow along. We will offer more careful definitions later.

Suppose we have a causal Bayesian network $(\prob{P}^{\RV{XYZ}},\mathcal{G})$ where $\RV{X}:\Omega\to X$, $\RV{Y}:\Omega\to Y$ and $\RV{Z}:\Omega\to Z$ are variables, $\prob{P}^{\RV{XYZ}}$ is a probability measure on $X\times Y\times Z$, $\mathcal{G}$ is a Directed Acyclic Graph whose vertices we identify with $\RV{X}$, $\RV{Y}$ and $\RV{Z}$ which contains the edges $\RV{X}\longrightarrowRHD \RV{Y}$ and $\RV{X}\longleftarrowRHD \RV{Z} \longrightarrowRHD \RV{Y}$. ``Setting $\RV{X}$ to $x$'' is an operation that takes as inputs $\prob{P}^{\RV{XYZ}}$, $\mathcal{G}$ and some $x\in X$ and returns a new probability measure $\prob{P}_x^{\RV{XYZ}}$ on $X\times Y\times Z$ given by \citep[page ~24]{pearl_causality:_2009}:
\begin{align}
    \prob{P}^{\RV{XYZ}}_{x}(x',y,z)=\disint{P}^{\RV{Y|XZ}}(y|x,z)\prob{P}^{\RV{Z}}(z)\llbracket x=x' \rrbracket\label{eq:truncated_fac}
\end{align}

Equation \ref{eq:truncated_fac} embodies three assumptions about a model of the operation of ``setting $\RV{X}$ to $x$''. First, such a model must assign probability 1 to the proposition that $\RV{X}$ yields $x$. Second, such a model must assign the same marginal probability distribution to $\RV{Z}$ as the input distribution; $\prob{P}^{\RV{Z}}=\prob{P}_{x}^{\RV{Z}}$. Finally, there must be some version of the interventional conditional probability $\RV{Y}|(\RV{X},\RV{Z})$ that is equal to some version of the observational conditional probability $\RV{Y}|(\RV{X},\RV{Z})$; there exists $\disint{P}^{\RV{Y}|\RV{XZ}}$ and $\disint{P}_x^{\RV{Y}|\RV{XZ}}$ such that $\disint{P}^{\RV{Y}|\RV{XZ}}=\disint{P}_x^{\RV{Y}|\RV{XZ}}$. We use the overbars here to indicate that, unlike in familiar cases, the particular choice of $\disint{P}^{\RV{Y}|\RV{XZ}}$ can matter here.

% Notice that the map $x\mapsto \prob{P}^{\RV{XYZ}}_x$ itself ``looks like'' a conditional probability. It maps each $x\in X$ to a probability distribution over $(\RV{X},\RV{Y},\RV{Z})$. In fact, a popular alternative notation for $\prob{P}^{\RV{XYZ}}_x$ this map is $\prob{P}^{\RV{XYZ}|do(\RV{X}=x)}$, which is clearly suggestive of an interpretation as a kind of conditional probability. We will take this interpretation seriously: we will posit some variable $\RV{U}$ (which may or may not be observable) and a probabilisitic model $\prob{Q}^{\RV{XYZ}|\RV{U}}:=x\mapsto \prob{P}^{\RV{XYZ}}_x$.

The operation of ``setting $\RV{X}$ to $x$'' is often referred to as an \emph{intervention}. Interventions are things we can choose to do, or not to do. We can also consider choosing to do or not do an intervention based on the output of some random process. We need some kind of model that can tell us which result we are likely to see for any choice of interventions, random or nonrandom. This means that we need a model with a \emph{probability gap} for the choice of interventions. For a nonrandom choice of intervention $x$, we can consider the map $x\mapsto \prob{P}^{\RV{XYZ}}_{x}$ such a model, and if we include random choices we can consider $\prob{Q}:\prob{P}_\alpha^{\RV{X}}\mapsto \sum_{x} \prob{P}_\alpha^{\RV{X}}(x) \prob{P}^{\RV{XYZ}}_{x}$ to be such a model.

$\prob{Q}$, as we have defined it, is not quite an ideal candidate for a probability gap model. Firstly, theconditional probability $\disint{P}^{\RV{Y}|\RV{XZ}}$ may be chosen arbitrarily on a set of measure zero with regard to $\prob{P}^{\RV{XZ}}$. As a result, depending on the choice of $\disint{P}^{\RV{Y}|\RV{XZ}}$, Equation \ref{eq:truncated_fac} can be satisfied by multiple probability distributions that differ in meaningful ways. For example, suppose $\RV{X}$, $\RV{Y}$ and $\RV{Z}$ are binary and $\prob{P}((\RV{X}, \RV{Z})\yields (1,1))=1$. Then we can consistently choose $\prob{P}^{\RV{Y|XZ}}(1|0,1)=1$ or $\prob{P}^{\RV{Y|XZ}}(1|0,1)=0$ because $\{0,1\}$ is a measure zero event. However, the first choice gives us  $\prob{P}^{\RV{XYZ}}_{0}(0,1,1)=1$ while the second gives us $\prob{P}^{\RV{XYZ}}_{0}(0,1,1)=0$, which are very different opinions regarding ``the result of setting $\RV{X}$ to $1$''.

Secondly, there may be no probability model at all that satisfies Equation \ref{eq:truncated_fac}. For example, suppose $\RV{X}=f\circ\RV{Z}$ for some $f$. Then we must have $\prob{P}^{\RV{X}}_x(x')=\prob{P}^{\RV{Z}}_x(f^{-1}(x'))$ for any $x$. However, we also have $\prob{P}^{\RV{X}}_x(x')=\llbracket x = x' \rrbracket$ for all $x,x'$ and $\prob{P}^{\RV{Z}}_x=\prob{P}^{\RV{Z}}$ for all $x$. Thus if $\RV{X}$ can more than one value, there is at least one choice of $x$ that cannot simultaneously satisfy these requirements.

A more subtle example of this latter problem appears in \citet{shahar_association_2009}. A causal graph in that paper features an arrow $\RV{Z}\longrightarrowRHD \RV{X}$ where $\RV{Z}=(\RV{H},\RV{W})$, representing a person's height and weight, and $\RV{X}$ represents their body mass index. This causal model is used to draw conclusions about the result of intervening on $\RV{X}$. By definition, $\RV{X}=\frac{\RV{W}}{\RV{H}^2}$. While we don't have $\RV{X}$ equal to $\RV{Z}$, it must still be a deterministic function of $\RV{Z}$.  However, any intervention on $\RV{X}$ along the lines of Equation \ref{eq:truncated_fac} will yield $\RV{X}$ independent of $(\RV{H},\RV{W})$, and unless $(\RV{H},\RV{W})$ is determistically equal to a constant and the intervention on $\RV{X}$ is carefully chosen, there is no probability model at all that has this independence.

The theory of probability gap models allows us to model things like interventions and it does not share these problems of non-uniqueness and non-existence with models defined via truncated factorisation.

In our original look at truncated factorisation, we noted a few problems with Equation \ref{eq:truncated_fac} being a \emph{definition} of interventional probability models. In particular:

\begin{itemize}
    \item There may be multiple different probability models that satisfy Equation \ref{eq:truncated_fac} for different versions of the disintegration $\prob{P}^{\RV{Y|XZ}}$
    \item There may be no probability models that satisfy Equation \ref{eq:truncated_fac}
\end{itemize}

We propose a different way to define interventional probability models:

\begin{itemize}
    \item The interventional probability model is some probability 2-comb 
    \begin{align}
        \prob{Q}^{\RV{Z}\square\RV{Y}|\RV{X}}
    \end{align}
    \item For some observational conditional probability $\prob{Q}_{\text{obs}}^{\RV{X}|\RV{Z}}$, observations are distributed according to 
    \begin{align}
    \prob{Q}_{\text{obs}}:=\text{insert}(\prob{Q}_{\text{obs}}^{\RV{X}|\RV{Z}},\prob{Q}^{\RV{Z}\square\RV{Y}|\RV{X}})\label{eq:int_to_obs}
    \end{align}
\end{itemize}

Note that, by definition, $\prob{Q}^{\RV{Y}|\RV{XZ}}$ exists and all versions of it are also versions of $\prob{Q}_{\text{obs}}^{\RV{Y}|\RV{XZ}}$. If in addition, for every $x\in X$, the deterministic insert $\prob{Q}_{x}^{\RV{X}|\RV{Z}}$ defined by $\prob{Q}_{x}^{\RV{X}|\RV{Z}}(x'|z) == \llbracket x=x' \rrbracket$ is a valid conditional probability, then there exists a version of $\prob{Q}_{\text{obs}}^{\RV{Y}|\RV{XZ}}$ such that:

\begin{align}
    \prob{Q}_x^{\RV{XYZ}} = \prob{Q}_{\text{obs}}^{\RV{Y|XZ}}(y|x,z)\prob{Q}_{\text{obs}}^{\RV{Z}}(z)\llbracket x=x' \rrbracket
\end{align}

This is similar to Equation \ref{eq:truncated_fac}. The two key differences are that this is existentially quantified over $\prob{Q}_{\text{obs}}^{\RV{Y|XZ}}$ and we have made explicit the assumptions that hard interventions on $\RV{X}$ are valid inserts.



\section{Causal Bayesian Networks}\label{sec:CBN}

Like some of the causal modelling frameworks discussed in the previous section, including see-do models, Causal Bayesian Networks (CBNs) represent both ``observations'' and ``consequences of interventions''. It seems reasonable to think that the real-world things that the see-do framework and the CBN framework address are sometimes the same. The question we have here is: if we have a decision problem represented by a see-do model, when can we represent the same problem with a CBN?

In order to answer this question, we have to deal with the fact that neither theory is formally contained by the other, so for example there's no precise way in which decisions correspond to interventions. The correspondence exists in the territory, the world that is inhabited by measurement processes, not the mathematical world that is inhabited by random variables. We therefore have to make some choices about what corresponds to what that seem to be reasonable given our understanding of what these models are used for.

To compare CBNs to see-do models, we will argue that CBNs can be understood as describing probabilistic models of observations and consequences, just like see-do models. Furthermore, CBNs feature an order-1 probability gap and so they describe a probability 2-comb over observations, interventions and consequences. If we suppose that there is some variable describing decisions that does not appear within the CBN, then we can posit a see-do model over observations, decisions and consequences. Finally, we ask: when is the see-do model compatible with the CBN 2-comb, or more precisely, when can we identify each \emph{decision rule} with a \emph{intervention rule} such that the probability model obtained by inserting a decision rule into the see-do model is identical to the probability model obtained by inserting an intervention rule into the CBN 2-comb. We show that see-do models that exhibit a particular type of symmetry are compatible with CBN 2-combs.

\subsection{Probability 2-combs represented by causal Bayesian networks}

Consider a simplified kind of CBN where a single variable may be intervened on. Note that the structure of the previous section -- $\RV{X}\longrightarrowRHD \RV{Y}$ and $\RV{X}\longleftarrowRHD \RV{W} \longrightarrowRHD \RV{Y}$ -- is generically appliccable to such a model if we identify $\RV{W}$ with the variable formed by taking a sequence of all of the ancestors of $\RV{X}$ and $\RV{Y}$ with the variable formed by taking a sequence of all non-ancestors of $\RV{X}$. The existence of an edge from $\RV{X}$ to $\RV{Y}$ in such a case does no harm as if $\RV{Y}$ is not ``actually'' a descendent of $\RV{X}$ then it will be independent conditional on $\RV{Z}$ (see \citet{peters_structural_2015} for a detailed treatment of when two graphs may or may not imply the same underlying model).

We will adopt the definiton discussed in Section \ref{sec:truncated_fac_again}: we take the causal Bayesian network in question to express the assumption that the result of intervention is modeled by some probability 2-comb $\prob{P}^{\RV{W}\square \RV{Y}|\RV{X}}$ and the observations are distributed according to $\text{insert}(\model{P}_{\text{obs}}^{\RV{X}|\RV{Z}},\prob{P}^{\RV{W}\square\RV{Y}|\RV{X}})$ for some $\model{P}_{\text{obs}}^{\RV{X}|\RV{Z}}$.

We are uncertain about the particular 2-comb $\prob{P}^{\RV{W}\square \RV{Y}|\RV{X}}$ that we should use to model interventions, as well as the particular $\model{P}_{\text{obs}}^{\RV{X}|\RV{Z}}$ appropriate for observations, so we will represent this uncertainty with an unobserved variable $\RV{H}$. Furthermore, if we are being precise about what is being modeled, we suppose that we have a sequence of ``observation'' variables $\RV{V}_{[n]}:=(\RV{W}_i,\RV{X}_i,\RV{Y}_i)_{i\in [n]}$ and a sequence of ``consequence'' variables modeled by $\RV{V}_{(n,m]}:=(\RV{W}_i,\RV{X}_i,\RV{Y}_i)_{i\in (n,m]}$ both defined on a fundamental probability set $\Omega$ (assume $n<m$). 

With this additional detail, we interpret Equation \ref{eq:int_to_obs} as saying

\begin{align}
    \model{Q}_{\text{obs}}^{\RV{W_iX_iY_i}|\RV{H}} &= \text{insert}(\model{Q}_{\text{obs}}^{\RV{X}_{j}|\RV{HW}_{j}},\prob{Q}^{\RV{W}_{j}|\RV{H}\square\RV{Y}_{j}|\RV{X}_{j}})
\end{align}

for all $i\in[n]$, $j\in (n,m]$.

Because we are now considering sequences of observations and consequences, we also think it is reasonable to understand a CBN model as coming equipped with assumptions of mutual independence:

\begin{align}
    \RV{V}_i\CI_{\prob{Q}} \RV{V}_{[m]\setminus \{i\}}|\RV{H}&\forall i\in [n]\\
    \RV{W}_i\CI_{\prob{Q}} \RV{V}_{[m]\setminus \{i\}}|\RV{H}&\forall i\in [m]\\
    \RV{Y}_i\CI_{\prob{Q}} \RV{V}_{[m]\setminus \{i\}}|(\RV{H},\RV{X}_i,\RV{W}_i)&\forall i\in [m]\\
\end{align}

The first condition says that the observations $\RV{V}_{[n]}$ are mutually independent, the second that if we ignore the $\RV{X}_i$s and $\RV{Y}_i$s, then the $\RV{W}_i$s are mutually independent for all $[m]$ and the third says that the $\RV{Y}_i$s are independent of the other variables in the sequence conditional on $(\RV{H},\RV{X}_i,\RV{W}_i)$. Note that we exclude $\RV{X}_i$ from these conditional independence assumptions. The reason for this is that we interpret $\RV{X}_i$ as a directly controlled variable and as such it may be chosen to be dependent on other variables in the sequence.

\begin{definition}[Order 2 model associated with the CBN in question]\label{def:cbn_o2}
A CBN order 2 model $\prob{Q}^{\RV{V}_{[n]}\RV{W}_{(n,m]}|\RV{H}\square \RV{Y}_{(n,m]}|\RV{X}_{(n,m]}}$ where $\RV{V}_{i} = (\RV{W}_i,\RV{X}_i,\RV{Y}_i)$ for $i\in [m]$, such that the CBN mutual independences hold:
\begin{align}
    \RV{V}_i\CI_{\prob{Q}} \RV{V}_{[m]\setminus \{i\}}|\RV{H}&\forall i\in [n]\label{eq:cbn_ci1}\\
    \RV{W}_i\CI_{\prob{Q}} \RV{V}_{[m]\setminus \{i\}}|\RV{H}&\forall i\in [m]\label{eq:cbn_ci2}\\
    \RV{Y}_i\CI_{\prob{Q}} \RV{V}_{[m]\setminus \{i\}}|(\RV{H,X}_i,\RV{W}_i)&\forall i\in [m]\label{eq:cbn_ci3}\\
\end{align}
Under these assumptions $\prob{Q}^{\RV{W}_i|\RV{H}}$ and $\prob{Q}^{\RV{Y}_i|\RV{H}\RV{X}_i\RV{W}_i}$ exist, and for all $i\in [n],j,k\in [m]$
\begin{align}
    \model{Q}^{\RV{W}_{j}|\RV{H}\square\RV{Y}_{j}|\RV{X}_{j}}&=\model{Q}^{\RV{W}_{k}|\RV{H}\square\RV{Y}_{k}|\RV{X}_{k}}\label{eq:identically_distributed}\\
    \model{Q}^{\RV{W_iX_iY_i}|\RV{H}} &= \text{insert}(\model{Q}_{\text{obs}}^{\RV{X}_{j}|\RV{HW}_{j}},\prob{Q}^{\RV{W}_{j}|\RV{H}\square\RV{Y}_{j}|\RV{X}_{j}})\label{eq:cbn_insert}
\end{align}
for all $i\in[n]$, $j\in (n,m]$.
\end{definition}


\begin{theorem}[Existence of CBN 2-comb]\label{th:cbn_2comb_exist}
Given a CBN order 2 model $\prob{Q}^{\RV{V}_{[n]}\RV{W}_{(n,m]}|\RV{H}\square \RV{Y}_{(n,m]}|\RV{X}_{(n,m]}}$ in accordance with Definition \ref{def:cbn_o2}, there exist conditional probabilities $\prob{Q}^{\RV{W}_i|\RV{H}}$ and $\prob{Q}^{\RV{Y}_i|\RV{H}\RV{X}_i\RV{W}_i}$ for all $i$.
\end{theorem}

\begin{proof}
Order 2 models $\prob{Q}^{\RV{V}_{[n]}\RV{W}_{(n,m]}|\RV{H}\square \RV{Y}_{(n,m]}|\RV{X}_{(n,m]}}$ have the following conditional probabilities:
\begin{align}
   & \prob{Q}^{\RV{V}_{[n]}\RV{W}_{(n,m])}|\RV{H}}&\text{exists}\label{eq:obs_cbn_existence}\\
   & \prob{Q}^{\RV{Y}_{(n,m]}|\RV{X}_{(n,m]}\RV{W}_{(n,m]}\RV{V}_{[n]}\RV{H}}&\text{exists}\label{eq:int_cbn_existence}
\end{align}

Equations \ref{eq:obs_cbn_existence} and \ref{eq:int_cbn_existence} together with conditional independences \ref{eq:cbn_ci1}, \ref{eq:cbn_ci2}, \ref{eq:cbn_ci3} and Theorem \ref{th:cons_ci} imply there exist versions of the following conditional proabilities such that

\begin{align}
    \prob{Q}^{\RV{V}_i|\RV{H}\RV{V}_{[m]\setminus\{i\}}}&=\prob{Q}^{\RV{V}_i|\RV{H}}\otimes \text{erase}_{V^{m-1}} & \forall i\in[n]\\
    \prob{Q}^{\RV{W}_i|\RV{H}\RV{V}_{[m]\setminus\{i\}}}&=\prob{Q}^{\RV{W}_i|\RV{H}}\otimes \text{erase}_{V^{m-1}} & \forall i\in[m]\\
    \prob{Q}^{\RV{Y}_i|\RV{H}\RV{X}_i\RV{W}_i\RV{V}_{[m]\setminus\{i\}}}&= \prob{Q}^{\RV{Y}_i|\RV{H}\RV{X}_i\RV{W}_i}\otimes \text{erase}_{V^{m-1}} & \forall i\in [m]
\end{align}
\end{proof}

Note also that Equation \ref{eq:identically_distributed} implies that the conditional probabilities from Theorem \ref{th:cbn_2comb_exist} can be chosen to be equal for all $i,j\in [m]$:
\begin{align}
    \prob{Q}^{\RV{W}_i|\RV{H}} = \prob{Q}^{\RV{W}_j|\RV{H}}\label{eq:identical_1}\\
    \model{Q}^{\RV{Y}_i|\RV{X}_i\RV{W}_i\RV{H}} &= \model{Q}^{\RV{Y}_j|\RV{X}_j\RV{W}_j\RV{H}}\label{eq:identical_2}
\end{align}

\begin{theorem}[CBN observations are identically distributed]
Given a CBN order 2 model $\prob{Q}^{\RV{V}_{[n]}\RV{W}_{(n,m]}|\RV{H}\square \RV{Y}_{(n,m]}|\RV{X}_{(n,m]}}$ in accordance with Definition \ref{def:cbn_o2}, there exists $\prob{Q}^{\RV{V}_{i}|\RV{H}}$ for $i\in [n]$ and for all $i,j\in [n]$
\begin{align}
    \prob{Q}^{\RV{V}_i|\RV{H}} = \prob{Q}^{\RV{V}_j|\RV{H}}\label{eq:identical_3}
\end{align}
\end{theorem}

\begin{proof}
Existence follows from the fact that $\RV{V}_i=\pi_i\circ\RV{V}_{[n]}$ with $\pi_i$ the function $V^n\to V$ that projects the $i$th index. Theorem \ref{th:recurs_pushf} then implies $\prob{Q}^{\RV{V}_i|\RV{H}}=\prob{Q}^{\RV{V}_{[n]}|\RV{H}}\kernel{F}_{\pi_i}$.

Equality follows from the fact that for all $i,j\in[n]$
\begin{align}
    \model{Q}^{\RV{V}_i|\RV{H}} &= \text{insert}(\model{Q}_{\text{obs}}^{\RV{X}_{j}|\RV{HW}_{j}},\prob{Q}^{\RV{W}_{j}|\RV{H}\square\RV{Y}_{j}|\RV{X}_{j}})\\
    &= \model{Q}^{\RV{V}_j|\RV{H}}
\end{align}
\end{proof}

\subsection{See-do models corresponding to causal Bayesian networks}

We have defined a class of order 2 probability gap models associated with causal Bayesian networks. We next want to ask: when does a see-do model induce an order 2 model associated with a CBN? Specifically, given a see-do model $\prob{T}^{\RV{V}_{[n]}|\RV{H}\square \RV{V}_{(n,m]}|\RV{D}}$ with decision rules of type $\{\prob{T}_\alpha^{\RV{D}|\RV{V}_{[n]}}\}_{\alpha\in A}$, when is there a CBN model $\prob{Q}^{\RV{V}_{[n]}\RV{W}_{(n,m]}|\RV{H}\square \RV{Y}_{(n,m]}|\RV{X}_{(n,m]}}$ with inserts of type $\{\prob{Q}_\alpha^{\RV{X}_{(n,m]}|\RV{V}_{[n]}\RV{W}_{(n,m]}\RV{H}}\}_{\alpha\in A}$ such that marginalising $\prob{T}_\alpha$ over $\RV{D}$ yields $\prob{Q}_\alpha$ for all $\alpha\in A$?

\begin{align}
    \model{T}^{\RV{V}_{[m]}|\RV{H}}_\alpha&=\tikzfig{seedo_equality2}\\
    &=\tikzfig{seedo_equality} \label{eq:consistent}\\
    &= \model{Q}^{\RV{V}_{[m]}|\RV{H}}_\alpha
\end{align}

We need a number of conditions to hold for this to be true of any $\prob{T}$; first, $\prob{T}$ needs to satisfy the CBN versions of ``mutually independent'' (Eqs. \ref{eq:cbn_ci1}-\ref{eq:cbn_ci3}) as well as the CBN version of ``identically distributed'' (Eqs. \ref{eq:identical_1}, \ref{eq:identical_2} and \ref{eq:identical_3}). Finally, we require decisions $\RV{D}$ to act as ``interventions'' on $\RV{X}$ in an appropriate way.

Theorem \ref{th:seedo_rep} shows that this correspondence holds exactly when:
\begin{itemize}
    \item The CBN mutual independences (Definition \ref{def:cbn_o2}) hold for the $\prob{T}$
    \item For all $i\in (n,m]$, $\RV{W}_i\CI_{\prob{T}}\RV{D}|\RV{H}$
    \item For all $i\in [m]$, $\RV{Y}_i\CI_{\prob{T}}\RV{D}|(\RV{W}_i,\RV{X}_i,\RV{H})$; we say that under conditions of perfect information, $(\RV{W}_i,\RV{X}_i)$ control $\RV{Y}_i$ by proxy
\end{itemize}

\begin{theorem}\label{th:seedo_rep}
Given a see-do model $\prob{T}^{\RV{V}_{[n]}|\RV{H}\square \RV{V}_{(n,m]}|\RV{D}}$ there exists a corresponding CBN probability 2-comb $\prob{Q}^{\RV{V}_{[n]}\RV{W}_{(n,m]}|\RV{H}\square \RV{Y}_{(n,m]}|\RV{X}_{(n,m]}}$ if and only if (1) the CBN mutual independences hold
\begin{align}
    \RV{V}_i\CI_{\prob{T}} \RV{V}_{[m]\setminus \{i\}}|\RV{H}&\forall i\in [n]\\
    \RV{W}_i\CI_{\prob{T}} \RV{V}_{[m]\setminus \{i\}}|\RV{H}&\forall i\in [m]\\
    \RV{Y}_i\CI_{\prob{T}} \RV{V}_{[m]\setminus \{i\}}|\RV{HX}_i&\forall i\in [m]\\
\end{align}
(2) for every insert $\alpha$ the following conditional probabilities are identical
\begin{align}
    \prob{T}_\alpha^{\RV{V}_i|\RV{H}} &= \prob{T}_\alpha^{\RV{V}_j|\RV{H}}&\forall i,j\in [n]\\
    \prob{T}_\alpha^{\RV{W}_i|\RV{H}} &= \prob{T}_\alpha^{\RV{W}_j|\RV{H}}&\forall i,j\in [m]\\
    \model{T}_\alpha^{\RV{Y}_i|\RV{X}_i\RV{W}_i\RV{H}} &= \model{T}_\alpha^{\RV{Y}_j|\RV{X}_j\RV{W}_j\RV{H}}&\forall i,j\in [m]
\end{align}
(3) $\RV{W}_i$ is unaffected by $\RV{D}$
\begin{align}
    \RV{W}_i\CI_{\prob{D}}
\end{align}
And (4) under conditions of perfect information, $(\RV{W}_i,\RV{X}_i,\RV{V}_{[n]})$ control $\RV{Y}_i$ by proxy:
\begin{align}
    \RV{Y}_i\CI_{\prob{T}}\RV{D}|(\RV{W}_i,\RV{X}_i,\RV{H},\RV{V}_{[n]})&\forall i\in [m]
\end{align}
\end{theorem}

\begin{proof}
\textbf{If:}
If all assumptions hold, we can write
\begin{align}
    \model{T}^{\RV{V}_{[n]}\RV{V}_j|\RV{HD}} = \tikzfig{t_vs_u}
\end{align}
For each $\model{S}_\alpha^{\RV{D}|\RV{V}_{[n]}}$, define
\begin{align}
    \model{R}_\alpha^{\RV{X}_j|\RV{V}_{[n]}\RV{W}_j\RV{H}}:= \tikzfig{defn_ra}
\end{align}
Then
\begin{align}
    &\tikzfig{seedo_equality2}\\
    &= \tikzfig{seedo_cbn_with_s}\\
    &= \tikzfig{seedo_equality}
\end{align}
\textbf{Only if:}
Suppose the CBN mutual independences do not hold for $\prob{T}$. Then there must be some $\alpha$ such that one of these conditional independences does not hold for $\prob{T}_\alpha$. By construction of CBN order 2 models, these indepenences hold for every probability model in the range of every CBN order 2 model $\prob{Q}$. Thus there is no CBN model corresponding to $\prob{T}$.

Suppose for some $\alpha$ and $i,j\in [n]$ we have $\prob{T}_\alpha^{\RV{V}_i|\RV{H}} \neq \prob{T}_\alpha^{\RV{V}_j|\RV{H}}$. 

Suppose the assumption of proxy control does not hold for $\prob{T}$. Then there is some $d,d'\in D$, $w\in W$, $h\in H$, $v\in V^{n}$, $x\in X$ and $y\in Y$ such that
\begin{align}
    \model{T}^{\RV{Y}_j|\RV{W}_j\RV{V}_{[n]}\RV{HX}_j\RV{D}}(y|w,v,h,x,d) &\neq \model{T}^{\RV{Y}_j|\RV{W}_j\RV{V}_{[n]}\RV{HX}_j\RV{D}}(y|w,v,h,x,d')\label{eq:not_indep}\\
    &\text{and }\model{T}_\alpha^{\RV{X}_j\RV{W}_j\RV{V}_{[n]}|\RV{HD}}(x,w,v|h,d) >0\\
    &\text{and }\model{T}_\alpha^{\RV{X}_j\RV{W}_j\RV{V}_{[n]}|\RV{HD}}(x,w,v|h,d') >0\\
\end{align}

If Equation \ref{eq:not_indep} only held on sets of measure 0 then we could choose versions of the conditional probabilities such that the independence held.

Then
\begin{align}
    \model{T}_d^{\RV{Y}_j|\RV{W}_j\RV{V}_{[n]}\RV{HX}_j}(y|w,v,h,x)&= \model{T}^{\RV{Y}_j|\RV{W}_j\RV{V}_{[n]}\RV{HX}_j\RV{D}}(y|w,v,h,x,d)\\
    &\neq \model{T}^{\RV{Y}_j|\RV{W}_j\RV{V}_{[n]}\RV{HX}_j\RV{D}}(y|w,v,h,x,d')\\
    &= \model{T}_{d'}^{\RV{Y}_j|\RV{W}_j\RV{V}_{[n]}\RV{HX}_j}(y|w,v,h,x)\\
    \implies \model{T}_d^{\RV{Y}_j|\RV{W}_j\RV{V}_{[n]}\RV{HX}_j\RV{D}}(y|w,v,h,x) &\neq \model{Q}_d^{\RV{Y}_j|\RV{W}_j\RV{V}_{[n]}\RV{HX}_j\RV{D}}(y|w,v,h,x)\\
    \text{or } \model{T}_{d'}^{\RV{Y}_j|\RV{W}_j\RV{V}_{[n]}\RV{HX}_j\RV{D}}(y|w,v,h,x) &\neq \model{Q}_{d'}^{\RV{Y}_j|\RV{W}_j\RV{V}_{[n]}\RV{HX}_j\RV{D}}(y|w,v,h,x)
\end{align}
As the conditional probabilities disagree on a positive measure set, $\model{P}\neq\model{Q}$.

Suppose assumption 3 holds but assumption 4 does not. Then for some $h\in H$, some $w\in W$, $v\in V^{|A|}$, $x\in X$ with positive measure and some $y\in Y$
\begin{align}
    \model{P}_d^{\RV{Y}_j|\RV{W}_j\RV{V}_{[n]}\RV{HX}_j\RV{D}}(y|w,v,h,x)&= \model{T}^{\RV{Y}_j|\RV{W}_j\RV{V}_{[n]}\RV{HX}_j}(y|w,v,h,x)\\
    &\neq \model{U}^{\RV{Y}_j|\RV{W}_j\RV{V}_{[n]}\RV{HX}_j}(y|w,v,h,x)\\
    &\neq model{Q}_d^{\RV{Y}_j|\RV{W}_j\RV{V}_{[n]}\RV{HX}_j\RV{D}}(y|w,v,h,x)
\end{align}
\end{proof}

Conditional independences like $(\RV{V}_{[n]},\RV{W}_j)\CI_{\model{T}} \RV{D}|\RV{H}$ and $\RV{Y}_j\CI_{\model{T}}\RV{D}|\RV{W}_j\RV{V}_{[n]}\RV{HX}_j$ bear some resemblance to the condition of ``limited unresponsiveness'' proposed by \citet{heckerman_decision-theoretic_1995}. They are conceptually similar in that they indicate that a particular variable does not ``depend on'' a decision $\RV{D}$ in some sense. As Heckerman points out, however, limited unresponsiveness is not equivalent to conditional independence. We tentatively speculate that there may be a relation between our ``pre-choice variables'' $(\RV{W}_j,\RV{V}_{[n]},\RV{H})$ and the ``state'' in Heckerman's work crucial for defining limited unresponsiveness.

\subsection{Proxy control}

We say that $(\RV{V}_{[n]},\RV{W}_j)\CI_{\model{T}} \RV{D}|\RV{H}$ expresses the notion that $\RV{W}_j$ is a \emph{pre-choice variable} and $(\RV{W}_j,\RV{V}_{[n]},\RV{X}_j)$ are \emph{proxies for }$\RV{D}$ with respect to $\RV{Y}$ under conditions of full information. To justify this terminology, we note that under a strong assumption of identifiability $\RV{Y}_j\CI\RV{H}|\RV{W}_j\RV{V}_{[n]}\RV{X}_j$ (i.e. the observed data allow us to identify $\RV{H}$ for the purposes of determining $\RV{T}^{\RV{Y}_j|\RV{W}_j\RV{V}_{[n]}\RV{X}_j\RV{H}}$), then we can write
\begin{align}
    \model{T}^{\RV{V}_{[n]}\RV{V}_{(n,m]}|\RV{H}\RV{D}} &=\tikzfig{strong_identifiability}\\
                                              &=\tikzfig{strong_identifiability2}
                                              &= \model{T}^{\RV{V}_{[n]}\RV{W}_j\RV{X}_j|\RV{H}\RV{D}}\kernel{M}
\end{align}

That is, under conditions of full information, knowing how to control the proxies $(\RV{W}_j,\RV{V}_{[n]},\RV{X}_j)$ is sufficient to control $\RV{Y}$. This echoes \citet{pearl_does_2018}'s view on causal effects representing ``stable characteristics'':
\begin{quote}
Smoking cannot be stopped by any legal or educational means available to us today; cigarette advertising can. That does not stop researchers from aiming to estimate ``the effect of smoking on cancer,'' and doing so from experiments in which they vary the instrument—cigarette advertisement—not smoking. The reason they would be interested in the atomic intervention $P(\text{cancer}|do(\text{smoking}))$ rather than (or in addition to) $P(\text{cancer}|do(\text{advertising}))$ is that the former represents a stable biological characteristic of the population, uncontaminated by social factors that affect susceptibility to advertisement, thus rendering it transportable across cultures and environments. With the help of this stable characteristic, one can assess the effects of a wide variety of practical policies, each employing a different smoking-reduction instrument.
\end{quote}



\subsection{Results I use that don't really fit into the flow of the text}

\subsubsection{Repeated variables}

Lemmas \ref{lem:nocopy1} and \ref{lem:nocopy2} establish that models of repeated variables must connect the repetitions with a copy map.

\begin{lemma}[Output copies of the same variable are identical]\label{lem:nocopy1}
For any $\Omega$, $\RV{X},\RV{Y},\RV{Z}$ random variables on $\Omega$ and conditional probability $\model{K}^{\RV{YZ}|\RV{X}}$, there is a conditional probability $\kernel{K}^{\RV{YYZ}|\RV{X}}$ unique up to impossible values of $\RV{X}$ such that
\begin{align}
    \tikzfig{kyyz} = \model{K}^{\RV{YZ}|\RV{X}}
\end{align}
and it is given by
\begin{align}
        \kernel{K}^{\RV{YYZ}|\RV{X}} &= \tikzfig{compose_with_copymap}\\
        &\iff \\
        \kernel{K}^{\RV{YYZ}|\RV{X}}(y,y',z|x) &= \llbracket y=y' \rrbracket\kernel{K}^{\RV{YZ}|\RV{X}}(y,z|x)\\
\end{align}
\end{lemma}

\begin{proof}
If we have a valid $\model{K}^{\RV{YYZ}|\RV{X}}$, it must be the pushforward of $(\RV{Y},\RV{Y},\RV{Z})$ under some $\model{K}^{\RV{I}|\RV{X}}$. Furthermore, $\model{K}^{\RV{YZ}|\RV{X}}$ must be the pushforward of $(*,\RV{Y},\RV{Z})\cong (\RV{Y},\RV{Z})$ under the same $\model{K}^{\RV{I}|\RV{X}}$.

For any $x\in \RV{X}(\Omega)$, validity requires $(\RV{X},\RV{Y},\RV{Y},\RV{Z})\yields (x,y,y',z)=\emptyset \implies \model{K}^{\RV{YYZ}|\RV{X}}(y,y',z|x)=0$. Clearly, whenever $y\neq y'$, $\model{K}^{\RV{YYZ}|\RV{X}}(y,y',z|x)=0$. Because $\model{K}^{\RV{YYZ}|\RV{X}}$ is a Markov kernel, there is some $\model{L}:X\kto X\times Z$ such that
\begin{align}
    \model{K}^{\RV{YYZ}|\RV{X}}(y,y',z|x) = \llbracket y=y' \rrbracket \model{L}(y,z|x)\\
\end{align}
But then
\begin{align}
    \model{K}^{\RV{YZ}|\RV{X}}(y,z|x) &= \sum_{y'\in Y} \model{K}^{\RV{YYZ}|\RV{X}}(y,y',z|x)\\
    &= \model{L}(y,z|x)\\
\end{align}
\end{proof}

\begin{lemma}[Copies shared between input and output are identical]\label{lem:nocopy2}

\todo[inline]{This got mixed up at some point and needs ot be unmixed-up}
For any $\kernel{K}:(\RV{X},\RV{Y})\kto (\RV{X},\RV{Z})$, $\kernel{K}$ is a model iff there exists some $\model{L}:(\RV{X},\RV{Y})\kto \RV{Z}$ such that
\begin{align}
     \kernel{K} &= \tikzfig{precompose_with_copymap}\\
     &\iff\\
     \kernel{K}_{x,y}^{\prime x',z} &= \llbracket x=x'\rrbracket \kernel{L}_{\prime x,y}^{z}
\end{align}

For any $\Omega$, $\RV{X},\RV{Y},\RV{Z}$ random variables on $\Omega$ and conditional probability $\model{K}^{\RV{Z}|\RV{XY}}$, there is a conditional probability $\kernel{K}^{\RV{XZ}|\RV{XY}}$ unique up to impossible values of $(\RV{X},\RV{Y})$ such that
\begin{align}
    \tikzfig{kxyxz} = \kernel{K}^{\RV{XZ}|\RV{XY}}
\end{align}
and it is given by
\begin{align}
        \kernel{K}^{\RV{XZ}|\RV{XY}} &= \tikzfig{compose_with_copymap}\\
        &\iff \\
        \kernel{K}^{\RV{XZ}|\RV{XY}}(x,z|x',y) &= \llbracket x=x' \rrbracket\kernel{K}^{\RV{Z}|\RV{XY}}(z|x',y)\\
\end{align}

\end{lemma}

\begin{proof}
If we have a valid $\model{K}^{\RV{XZ}|\RV{XY}}$, it must be the pushforward of $(\RV{X},\RV{Z})$ under some $\model{K}^{\RV{I}|\RV{XY}}$. Furthermore, $\model{K}^{\RV{Z}|\RV{XY}}$ must be the pushforward of $(*,\RV{Z})\cong (\RV{Z})$ under the same $\model{K}^{\RV{I}|\RV{X}}$.

For any $(x,y)\in (\RV{X},\RV{Y})(\Omega)$, validity requires $(\RV{X},\RV{Y},\RV{X},\RV{Z})\yields (x,y,x',z)=\emptyset \implies \model{K}^{\RV{XZ}|\RV{XY}}(x',z|x,y)=0$. Clearly, whenever $x\neq x'$, $\model{K}^{\RV{XZ}|\RV{XY}}(x',z|x,y)=0$. Because $\model{K}^{\RV{XZ}|\RV{XY}}$ is a Markov kernel, there is some $\model{L}:X\times Y\kto Z$ such that
\begin{align}
    \model{K}^{\RV{XZ}|\RV{XY}}(x',z|x,y)=0 = \llbracket x=x' \rrbracket \model{L}(z|x,y)\\
\end{align}
But then
\begin{align}
    \model{K}^{\RV{Z}|\RV{XY}}(y,z|x) &= \sum_{x'\in X} \model{K}^{\RV{XZ}|\RV{XY}}(x',z|x,y)\\
    &= \model{L}(z|x,y)\\
\end{align}
\end{proof}




\subsection{Combs}\label{ap:combs}

\begin{reptheorem}{th:comb_equiv}[Equivalence of comb representations]
Given sample space $(\Omega,\sigalg{F})$, a finite collection of variables $\RV{X}_{i}:\Omega\to (X_i,\sigalg{X}_i)$ for $i\in [n]$, $X_i$ discrete, and a disassembled probability comb $\{\prob{P}_\square^{\RV{X}_{i}|\RV{X}_{[i-1]}}|i\in \RV{X}_{[n]_{\text{odd}}}\}$, for any $l\in [n]_{\mathrm{odd}}$ and any $\kernel{K}:X_{[l-1]}\kto X_l$
\begin{align}
    (\bigcombprod_{j\in [l-1]_{\mathrm{odd}}} \prob{P}_\square^{\RV{X}_{j}|\RV{X}_{[j-1]}}) \combprod \kernel{K} &\overset{\prob{P}_\square}{\cong} (\bigcombprod_{j\in [l]_{\mathrm{odd}}} \prob{P}_\square^{\RV{X}_{j}|\RV{X}_{[j-1]}})\\
    \implies \kernel{K} &\overset{\prob{P}_\square}{\cong} \prob{P}_\square^{\RV{X}_{l}|\RV{X}_{[l-1]}}
\end{align}
\end{reptheorem}

\begin{proof}
Equality is trivial for $l=1$. 

For a sequence $x_{[l-2]}\in X_{[l-2]}$, let $e_{[l-2]}$ be the even indices of $x_{[l-2]}$ and $o_{[l-2]}$ be the odd indices.

For any $e_{l-1}\in X_{l-1}$, $A\in X_{l}$ let $C^>_{A,e_{l-1}}\in \sigalg{X}_{[l-2]}$ be the set of points $C^>_{A,e_{l-1}}:=\{x_{[l-2]}| \kernel{K}(A|e_{l-1},x_{[l-2]})> \prob{P}_\square^{\RV{X}_{j}|\RV{X}_{[j-1]}}(A|e_{l-1},x_{[l-2]})\}$, and $C^<_{A,e_{l-1}}$ the obvious analog. Then, definining $C_{A,e_{l-1}}=C^>_{A,e_{l-1}}\cup C^<_{A,e_{l-1}}$,
\begin{align}
    (\bigcombprod_{j\in [l-1]_{\mathrm{odd}}} \prob{P}_\square^{\RV{X}_{j}|\RV{X}_{[j-1]}})(A\times C^>_{A,e_{l-1}}|e_{[l-3]},e_{l-1}) &= 0\\
    (\bigcombprod_{j\in [l-1]_{\mathrm{odd}}} \prob{P}_\square^{\RV{X}_{j}|\RV{X}_{[j-1]}})(A\times C^<_{A,e_{l-1}}|e_{[l-3]},e_{l-1}) &= 0\\
    \implies (\bigcombprod_{j\in [l-1]_{\mathrm{odd}}} \prob{P}_\square^{\RV{X}_{j}|\RV{X}_{[j-1]}})(A\times C_{A,e_{l-1}}|e_{[l-3]},e_{l-1}) &= 0\\
    &= \sum_{o_{[l-2]}\in C_{A,e_{l-1},\text{odd}}} \kernel{K}(A|e_{l-1},x_{[l-2]}) \prod_{j\in [l-2]_{\text{odd}}} \prob{P}_\square^{\RV{X}_{j}|\RV{X}_{[j-1]}}(o_{j}|o_{[j-2]},e_{[j-1]})
\end{align}
for all $e_{[l-3]}\in X_{[l-3]_{\text{even}}}$.

Consider arbitrary $\prob{P}_\alpha\in \prob{P}_\square$, $A\subset X_l$, $C\subset X_{[l-1]}$:
\begin{align}
    \prob{P}_\alpha^{\RV{X}_{[l]}} (A\times C) =&  \sum_{x_{[l-2]}\in C} \kernel{P}_\square(A|e_{l-1},x_{[l-2]}) \prod_{j\in [l-2]_{\text{odd}}} \prob{P}_\square^{\RV{X}_{j}|\RV{X}_{[j-1]}}(x_{j}|o_{[j-2]},e_{[j-1]}) \prob{P}_\alpha^{\RV{X}_{j+1}|\RV{X}_{[j]}}(x_{j+1}|o_{[j]},e_{[j-1]})\\
    =& \sum_{x_{[l-2]}\in C_{A,e_{l-1}}} \kernel{P}_\square(A|e_{l-1},x_{[l-2]}) \prod_{j\in [l-2]_{\text{odd}}} \prob{P}_\square^{\RV{X}_{j}|\RV{X}_{[j-1]}}(x_{j}|o_{[j-2]},e_{[j-1]}) \prob{P}_\alpha^{\RV{X}_{j+1}|\RV{X}_{[j]}}(x_{j+1}|o_{[j]},e_{[j-1]}) \\
    &+ \sum_{x_{[l-2]}\in C^C_{A,e_{l-1}}} \kernel{P}_\square(A|e_{l-1},x_{[l-2]}) \prod_{j\in [l-2]_{\text{odd}}} \prob{P}_\square^{\RV{X}_{j}|\RV{X}_{[j-1]}}(x_{j}|o_{[j-2]},e_{[j-1]}) \prob{P}_\alpha^{\RV{X}_{j+1}|\RV{X}_{[j]}}(x_{j+1}|o_{[j]},e_{[j-1]})\\
    =& 0 + \sum_{x_{[l-2]}\in C^C_{A,e_{l-1}}} \kernel{K}(A|e_{l-1},x_{[l-2]}) \prod_{j\in [l-2]_{\text{odd}}} \prob{P}_\square^{\RV{X}_{j}|\RV{X}_{[j-1]}}(x_{j}|o_{[j-2]},e_{[j-1]}) \prob{P}_\alpha^{\RV{X}_{j+1}|\RV{X}_{[j]}}(x_{j+1}|o_{[j]},e_{[j-1]})\\
    =& \prob{P}_\alpha^{\RV{X}_{[l-1]}}\odot \kernel{K} (A\times C)\\
    \implies \kernel{K} \overset{\prob{P}_\square}{\cong}& \prob{P}_\square^{\RV{X}_{l}|\RV{X}_{[l-1]}}
\end{align}
\end{proof}

\subsection{Comb conditional correspondence}\label{ap:ccc}

\begin{reptheorem}{th:comb_conditional_correspondence}[Comb-conditional correspondence]
Given a probability comb $\{\prob{P}_\square^{\RV{X}_{i}|\RV{X}_{[i-1]}}|i\in \RV{X}_{D_{\text{odd}}}\}$ and a blind choice $\alpha$
\begin{align}
\prob{P}_\square^{\RV{X}_{D_{\text{odd}}}\combbreak \RV{X}_{D_{\text{even}}}}\cong{\prob{P}_\alpha}{=}\prob{P}_\alpha^{\RV{X}_{D_{\text{odd}}}| \RV{X}_{D_{\text{even}}}}
\end{align}
\end{reptheorem}

\begin{proof}
Consider $n\in D$. The correspondence is immediate for $n=1$:
\begin{align}
    \prob{P}_\square^{\RV{X}_{1}\combbreak \RV{X}_{0}}\overset{\prob{P}_\alpha}{\cong}\prob{P}_\alpha^{\RV{X}_{1}| \RV{X}_{0}}
\end{align}

Suppose for induction the correspondence holds for odd $n-2$. For any blind $\alpha$
\begin{align}
    \prob{P}_\alpha^{\RV{X}_{[n]}|\RV{X}_0} &= \tikzfig{comb_condi_corresp_1}\\
    &= \tikzfig{comb_condi_corresp_2}\\
    &= \tikzfig{comb_condi_corresp_3}\\
    &= \tikzfig{comb_condi_corresp_4}\\
    &= \tikzfig{comb_condi_corresp_5}
\end{align}

and we also have
\begin{align}
    (\prob{P}_\alpha^{\RV{X}_{[n]_{\text{even}}}|\RV{X}_0} \odot\prob{P}_\square^{\RV{X}_{D_{\text{odd}}}\combbreak \RV{X}_{D_{\text{even}}}})(A\times B|x) &= 
\end{align}
\end{proof}

\subsection{Representation of conditional probability models}

\begin{reptheorem}{th:iid_rep}
Suppose we have a fundamental probability set $\Omega$ and a do model $(\prob{P},\RV{D},\RV{Y},R)$ such that $\RV{D}:=(\RV{D}_i)_{i\in \mathbb{N}}$ and $\RV{Y}:=(\RV{Y}_i)_{i\in\mathbb{N}}$. $\prob{P}$ is causally contractible if and only if there exists some $\RV{H}:\Omega\to H$ such that $\prob{P}^{\RV{Y}_i|\RV{H}\RV{D}_i}$ exists for all $i\in \mathbb{N}$ and
\begin{align}
    \prob{P}^{\RV{Y}|\RV{H}\RV{D}} &= \tikzfig{do_model_representation}\\
    &\iff\\
    \RV{Y}_i&\CI_{\prob{P}} \RV{Y}_{\mathbb{N}\setminus i},\RV{D}_{\mathbb{N}\setminus i}|\RV{H}\RV{D}_i&\forall i\in \mathbb{N}\\
    \land \prob{P}^{\RV{Y}_i|\RV{H}\RV{D}_i} &= \prob{P}^{\RV{Y}_0|\RV{H}\RV{D}_0} & \forall i\in \mathbb{N}
\end{align}
\end{reptheorem}

\begin{proof}
If:
By the assumptions of independence and identical conditionals, for any deterministic decision functions $d,d'\in D$ with equal subsequences $d_S=d'_T$
\begin{align}
    \prob{P}_d^{\RV{Y}_S|\RV{H}\RV{D}}(y|d) &= \int_H\prod_{i\in S}\prob{P}^{\RV{Y}_0|\RV{H}\RV{D}_0}(y_i|h,d_i)d\prob{P}^{\RV{H}}(h)\\
                                          &= \int_{H}\prod_{i\in T}\prob{P}^{\RV{Y}_0|\RV{H}\RV{D}_0}(y_i|h,d'_i)d\prob{P}^{\RV{H}}(h) & \text{by equality of subsequences}\\
                                          &= \prob{P}_{d'}^{\RV{Y}_T|\RV{H}\RV{D}}(y|d)
\end{align}

Only if:
We have
\begin{align}
    \prob{P}^{\RV{Y}|\RV{D}} &= \tikzfig{lookup_representation}
\end{align}

Also, by contractibility of $\prob{P}^{\RV{Y}^D}$ and De Finetti's theorem, there is some $\RV{H}$ such that

\begin{align}
    \prob{P}^{\RV{Y}^\RV{D}} &= \tikzfig{de_finetti_potential_outcomes}
\end{align}

In particular, let $\RV{Y}^D_{\cdot i}:=(\RV{Y}^D_{ji})_{j\in D}$ and $\RV{Y}^D_{\cdot \{i\}^C} = (\RV{Y}^D_{jk})_{j\in D, k\in \mathbb{N}\setminus \{i\}}$, and

\begin{align}
    &\RV{Y}^D_{\cdot i} \CI_{\prob{P}} \RV{Y}^D_{\cdot \{i\}^C} |\RV{H} & \text{ representation theorem}\label{eq:pci_1}\\
    &\RV{Y}^D\RV{H} \CI_{\prob{P}} \RV{D} &\text{ by Theorem \ref{th:cons_ci} and existence of }\prob{P}^{\RV{Y}^D\RV{H}}\label{eq:pci_2}\\
    &\RV{Y}^D_{\cdot i}\CI_{\prob{P}} \RV{D} |\RV{Y}^D_{\cdot \{i\}^C}\RV{H}&\text{ weak union on Eq. }\ref{eq:pci_2}\\
    &\RV{Y}^D_{\cdot i}\CI_{\prob{P}} \RV{D}\RV{Y}^D_{\cdot \{i\}^C} |\RV{H}&\text{ contraction on Eqs. \ref{eq:pci_1} and \ref{eq:pci_2}}\label{eq:pci_4}\\
    &\RV{Y}^D_{\cdot i}\CI_{\prob{P}} \RV{D}_{\{i\}^C}\RV{Y}^D_{\cdot \{i\}^C} |\RV{H}\RV{D}_i&\text{ weak union on Eq. \ref{eq:pci_4}}\label{eq:pci_5}\\
    &\RV{D}_{i} \CI_{\prob{P}}\RV{Y}^D_{\cdot \{i\}^C} \RV{D}_{\{i\}^C} |\RV{H}\RV{D}_i \RV{Y}^D_{\cdot i}&\text{ due to conditioning on }\RV{D}_i\label{eq:pci_6}\\
    &\RV{Y}^D_{i}\RV{D}_i \CI_{\prob{P}} \RV{D}_{\{i\}^C}\RV{Y}^D_{\cdot \{i\}^C} |\RV{H}\RV{D}_i&\text{ contraction on Eqs. \ref{eq:pci_5} and \ref{eq:pci_6}}\label{eq:pci_7}\\
\end{align}

Now, note that $(\RV{Y}_i,\RV{D}_i)$ is a deterministic function of $(\RV{Y}^D_{i},\RV{D}_i)$ and $(\RV{Y}_{\{i\}^C},\RV{D}_{\{i\}^C})$ is a deterministic function of $(\RV{Y}^D_{\{i\}^C},\RV{D}_{\{i\}^C})$. Therefore

\begin{align}
    &\RV{Y}_i \CI_{\prob{P}} \RV{D}_{\{i\}^C}\RV{Y}_{\{i\}^C} |\RV{H}\RV{D}_i&
\end{align}

So, by Theorem \ref{th:cons_ci}, $\prob{P}^{\RV{Y}_i|\RV{HD}}$ exists and by contractibility of $\prob{P}^{\RV{Y}^D}$, for any $i,j\in\mathbb{N}$

\begin{align}
    \prob{P}^{\RV{Y}_i|\RV{HD_i}}(y_i|h,d_i) &= \prob{P}^{\RV{Y}^D_{d_i i}|\RV{H}}(y_i|h) \\
    &= \prob{P}^{\RV{Y}^D_{d_i j}|\RV{H}}(y_i|h)\\
    &= \prob{P}^{\RV{Y}_j|\RV{HD}_j}(y_i|h,d_i)
\end{align}
\end{proof}


\subsection{Decision problems}

Suppose we have an observation process $\proc{X}$, modeled by $\RV{X}$ taking values in $X$ (we are \emph{informed}). Given an observation $x\in X$, we suppose that we can choose a decision from a known set $D$ (the set of decisions is \emph{transparent}), and we suppose that choosing a decision results in some action being taken in the real world. As with processes of observation, we will mostly ignore the details of what ``taking an action'' involves. The process of choosing a decision that yields an element of $D$ is a decision making process $\proc{D}$ modeled by $\RV{D}$. We might be able to introduce randomness to the choice, in which case the relation between $\RV{X}$ and $\RV{D}$ may be stochastic.  We will assume that there is some $\proc{Y}$ modeled by $\RV{Y}$ such that $(\RV{X},\RV{D},\RV{Y})$ tell us everything we want to know for the purposes of deciding which outcomes are better than others.

We want a model that allows us to compare different stochastic \emph{decision functions} $\kernel{Q}^{\RV{D}|\RV{X}}_\alpha:X\kto D$, letting $A$ be the set of all such functions available to be chosen. That is, we need a higher order function $f$ that takes a decision function $\kernel{Q}^{\RV{X}|\RV{D}}_\alpha$ and returns a probabilistic model of the consequences of selecting that decision function $\kernel{P}_\alpha^{\RV{DXY}}$. An order 2 model $(\kernel{P}_\square^{\RV{X},\prob{P}_\square^\RV{Y}|\RV{XD}},A)$ defines such a function, though there are many such functions that are not order 2 models. The key feature of probability gap models is that the map is by intersection of probability sets, so for example the conditional probability of $\RV{X}|\RV{D}$ given a decision function $\kernel{Q}^{\RV{X}|\RV{D}}_\alpha$ must actually be equal to $\kernel{Q}^{\RV{X}|\RV{D}}_\alpha$, and we can say the same for $\kernel{P}_\square^{\RV{X}}$ and $\prob{P}_\square^{\RV{Y}|\RV{XD}}$. If we don't think all of these conditional probabilities are fixed, then we want something other than an order 2 model of the type discussed. We will define \emph{ordinary decision problems} to be those for which the desired model $\prob{P}_\square$ is this type of order 2 probability gap model.

\todo[inline]{I think adding hypotheses at this point might make things unnecessarily confusing; on the other hand, they are useful for the connection to classical statistical decision theory. The "repeatable experiments" section shows how see-do models with certain assumptions induce an easier to understand class of hypotheses, and I could just save the idea of a hypothesis until I get there}

We consider an additional kind of gap in our probability model. The nature of this gap is: we don't know exactly which order 2 model $(\kernel{P}_\square^{\RV{X},\prob{P}_\square^\RV{Y}|\RV{XD}},A)$ we ``ought'' to use. To represent this gap we include an unobserved variable $\RV{H}$, the \emph{hypothesis}. We can interpret $\RV{H}$ as expressing the fac that, if we knew the value of $\RV{H}$ then we would know that our decision problem was represented by a unique order 2 model $(\kernel{P}_{h,\square}^{\RV{X},\prob{P}_{h,\square}^\RV{Y}|\RV{XD}},A)$. However, $\RV{H}$ is not known and in fact we do not know how to determine $\RV{H}$ (this is the nature of an \emph{unoserved} variable -- there is no process available to find the value it yields). Our model is thus given by $$(\kernel{P}_\square^{\RV{X}|\RV{H},\prob{P}_\square^\RV{Y}|\RV{HXD}},A)$$.

\begin{definition}[Ordinary decision problem]
An ordinary decision problem $(\prob{P},\Omega,\RV{H},(\RV{X},\proc{X}),(\RV{D},\proc{D}),(\RV{Y},\proc{Y}))$ consists of a fundamental probability set $\Omega$, hypotheses $\RV{H}:\Omega\to H$, observations $\RV{X}\Omega\to X$, decisions $\RV{D}:\Omega\to D$ and consequences $\RV{Y}:\Omega\to Y$, and the latter three random variables are associated with measurement processes. It is equipped with a probability gap model $\prob{P}:\Delta(D)^X\to \Delta(\Omega)^H$ where $\Delta(D)^X$ is the set of valid $\RV{D}|\RV{X}$ Markov kernels $X\kto D$ and $\Delta(\Omega)^H$ is the set of valid Markov kernels $H\kto \Omega$. We require of $\prob{P}$:
\begin{enumerate}
    \item $\kernel{P}_\alpha^{\RV{D}|\RV{X}}=\model{Q}_\alpha^{\RV{D}|\RV{X}}$ for all decision functions $\model{Q}_\alpha^{\RV{D}|\RV{X}}\in \Delta(D)^X$
    \item $\prob{P}^{\RV{X}|\RV{H}}=\model{P}_\alpha^{\RV{X}|\RV{H}}$ for all $\model{P}_\alpha:=\model{P}(\model{Q}_\alpha^{\RV{D}|\RV{X}})$
    \item $\prob{P}^{\RV{Y}|\RV{XDH}}=\model{P}_\alpha^{\RV{Y}|\RV{XDH}}$ for all $\model{P}_\alpha:=\model{P}(\model{Q}_\alpha^{\RV{D}|\RV{X}})$
\end{enumerate}
\end{definition}

(1) reflects the assumption that the ``probability of $\RV{D}$ given $\RV{X}$'' based on the induced model is equal to the ``probability of $\RV{D}$ given $\RV{X}$'' based on the chosen decision function. (2) reflects the assumption that the observations should be modeled identically no matter which decision function is chosen. (3) reflects the assumption that given hypothesis, the observations and the decision, the model of $\RV{Y}$ does not depend any further on the decision function $\alpha$.

Under these assumptions $\prob{P}_\square$ is an order 2 model $(\kernel{P}_\square^{\RV{X},\prob{P}_\square^\RV{Y}|\RV{XD}},A)$ which we call a ``see-do model''.\todo{I need to update the proof for this claim}

% \subsection{What should a probability model represent? Controversies about decision theories}

% There are some decision problems problems in which one or more of these assumptions may be unreasonable, and the correct way to approach to these problems is controversial. Newcomb's problem, for example, invites us to consider a problem where a second party has predicted our choice of decision function $\alpha$ before we have made it, and we have good reasons to believe this prediction is correct \citep{nozick_newcombs_1969}. \emph{The predictor} may then make choices that affect the consequences we expect to see, and this could mean that it is appropriate to consider models in which consequences to depend on $\alpha$ in addition to $\RV{D}$. The question of which kind of model \emph{should} be adopted in such a situation is controversial \citet{weirich_causal_2016,lewis_causation_1986}, and two prominent views on the correct answer are \emph{causal decision theory} and \emph{evidential decision theory}.

% This work does not propose normative rules for getting from a description of the world to a see-do model, and so the question of ``which decision theory?'' is not addressed here. We could ask if causal and evidential decision theory can be operationalised as different (vague) rules for getting from descriptions of the world to probability 2-combs $\model{P}^{\RV{X}|\RV{H}\square \RV{Y}|\RV{D}}$, but we leave this question open.

\subsection{Decisions as measurement procedures}

We have previously posited that observed variables are variables $\RV{X}$ -- themselves purely mathematical objects -- associated with a measurement process $\proc{X}$ that has ``one foot in the real world''. In the framework we have proposed here, decisions correspond to a special class of measurement procedure.

Suppose that we are only contemplating decision functions that map deterministically to $\RV{D}$. Suppose furthermore that we will $\RV{D}$ according to a model $\prob{P}_\square$, a utility function on $X\times D\times Y\to \mathbb{R}$ and a decision rule which is a function $f$ from models, utility functions and decision rules to decisions. Note that models, utility functions and decision rules are all well-defined mathematical objects. If we are confident that our choice will in the end be an element of a well-defined set of objects of the appropriate type, then we are positing that we have a ``measurement procedure'' $\proc{M}$ that yield models, utilities and decision rules. If so, $f\circ \proc{M}$ -- that is, the function that yields a decision -- is itself a measurement procedure. This is what is unique about decisions: proposing a complete decision problem with models, utilities and decision rules, defines a measurement procedure for decisions. Other quantities of interest do not seem to have this property -- we \emph{require} a measurement process for observations in order to make the whole setup work, but we do not \emph{define} it in the course of setting up a model for our decision problem.

\todo[inline]{I don't know how important this observation is, but the fact that $\proc{D}$ is an output of a formal decision making system makes it different from other things we might call decisions, and I wonder if I should call it something else in order to avoid ambiguity. The vague reason I think this matters is: whatever you might want to measure, you won't learn more about $\proc{D}$ from it than you already know once you have the model, the utility and the decision rule, this is not a property that other things we call ``decisions'' share and this distinction might be important regarding judgements of causal contractibility.}

% \subsection{Unresponsiveness}

% Given a see-do model $\model{P}^{\RV{X}|\RV{H}\square \RV{Y}|\RV{D}}$, $\RV{A}\CI_{\prob{P}}^{2|*} \RV{D}|\RV{H}$ can be interpreted to express the property, given any hypothesis, $\RV{A}$ does not depend on $\RV{D}$ when we choose $\RV{D}$ randomly.

% This might sound like it expresses a property of ``causal independence''. However, it isn't quite satisfactory for this term. Consider $\RV{A}\in\{0,1\}$ representing the outcome of a fair coin toss, $\RV{D}\in\{0,1\}$ representing a bet on the coin toss and $\RV{B}\in\{0,1\}$ representing the outcome of the bet. There is one hypothesis -- ``the coin is fair'' -- and no observations. We construct a see-do model $\prob{P}$ in the obvious way given these assumptions. Then $\RV{A}\CI_{\prob{P}}^{2|*} \RV{D}|\RV{H}$ (``the coin toss is independent of the decision, for any decision rule'') and $\RV{B}\CI_{\prob{P}}^{2|*}\RV{D}|\RV{H}$ (``the outcome of the bet is independent of the decision, for any decision rule''). However, it is not the case that $(\RV{A},\RV{B})\CI_{\prob{P}}^{2|*}\RV{D}|\RV{H}$. Were both $\RV{A}$ and $\RV{B}$ causally independent of $\RV{D}$, then $(\RV{A},\RV{B})$ also ought to be independent of $\RV{D}$. This example is from \citet{heckerman_decision-theoretic_1995}.

% We will borrow the terminology of \emph{unresponsiveness} from \citet{heckerman_decision-theoretic_1995} to refer to independences like $\RV{A}\CI_{\prob{P}}^{2|*} \RV{D}|\RV{H}$; specifically, where a variable is independent of $\RV{D}$ given $\RV{H}$ under the restricted 2-comb $\model{P}^{\RV{X}|\RV{H}\framebox{*} \RV{Y}|\RV{D}}$.

\subsection{See-do models and classical statistical decision theory}

A further property that may hold for some see-do models $\prob{P}^{\RV{X}|\RV{H}\square \RV{Y}|\RV{D}}$ is $\RV{Y}\CI^2_{\prob{P}} \RV{X}|(\RV{H},\RV{D})$. This expresses the view that the consequences are independent of the observations, once the hypothesis and the decision are fixed. Such a situation could hold in our scenario above, where the observations are trial data, the decisions are recommendations to care providers and the consequences are future patient outcomes. In such a situation, we might suppose that the trial data are informative about the consequences only via some parameter such as effect size; if the effect size can be deduced from $\RV{H}$ then our assumption corresponds to the conditional independence above.

Given a see-do model $\prob{P}^{\RV{X}|\RV{H}\square \RV{Y}|\RV{D}}$ along with the principle of expected utility to evaluate strategies, and the assumption $\RV{Y}\CI^2_{\prob{P}} \RV{X}|(\RV{H},\RV{D})$ we obtain a statistical decision problem in the form introduced by \citet{wald_statistical_1950}.

A \emph{statistical model} (or \emph{statistical experiment}) is a collection of probability distributions $\{\prob{P}_\theta\}$ indexed by some set $\Theta$. A statistical decision problem gives us an observation variable $\RV{X}:\Omega\kto X$ and a statistical experiment $\{\prob{P}^{\RV{X}}_\theta\}_\Theta$, a decision set $D$ and a loss $l:\Theta\times D\to \mathbb{R}$. A strategy $\model{S}^{\RV{D}|\RV{X}}_\alpha$ is evaluated according to the risk functional $R(\theta,\alpha):=\sum_{x\in X}\sum_{d\in D} \prob{P}^{\RV{X}}_\theta(x) S^{\RV{D}|\RV{X}}_\alpha (d|x) l(h,d)$. A strategy $\model{S}^{\RV{D}|\RV{X}}_\alpha$ is considered more desirable than $\model{S}^{\RV{D}|\RV{X}}_\beta$ if $R(\theta,\alpha)<R(\theta,\beta)$.

Suppose we have a see-do model $\prob{P}^{\RV{X}|\RV{H}\square \RV{Y}|\RV{D}}$ with $\RV{Y}\CI_{\model{P}} \RV{X}|(\RV{H,D})$, and suppose that the random variable $\RV{Y}$ is a ``negative utility'' function taking values in $\mathbb{R}$ for which \emph{low} values are considered desirable. Define a loss $l:H\times D\to \mathbb{R}$ by $l(h,d) = \sum_{y\in \mathbb{R}} y\model{P}^{\RV{Y}|\RV{H}\RV{D}}(y|h,d)$, we have 

\begin{align}
    \mathbb{E}_{\model{P}_{\alpha}}[\RV{Y}|h] &= \sum_{x\in X} \sum_{d\in D} \sum_{y\in Y} \model{P}^{\RV{X}|\RV{H}}(x|h) \model{Q}_\alpha^{\RV{D}|\RV{X}}(d|x) \model{P}^{\RV{Y}|\RV{HD}}(y|h,d)\\
    &= \sum_{x\in X} \sum_{d\in D} \model{P}^{\RV{X}|\RV{H}}(x|h) \model{Q}_\alpha^{\RV{D}|\RV{X}}(d|x) l(h,d)\\
    &= R(h,\alpha)
\end{align}

If we are given a see-do model where we interpret $\{\model{P}^{\RV{X}|\RV{H}}(\cdot|h)|h\in H\}$ as a statistical experiment and $\RV{Y}$ as a negative utility, the expectation of the utility under the strategy forecast given in equation \ref{eq:see_do_query} is the risk of that strategy under hypothesis $h$.


\citet{jacobs_causal_2019} has used a comb decomposition theorem to prove a sufficient identification condition similar to the identification condition given by \citet{tian2002general}. This theorem depends on the particular inductive hypotheses made by causal Bayesian networks.

\subsection{Dawid}
\begin{quote}
A fundamental feature of the DT approach is its consideration of the relationships between the various probability distributions that govern different regimes of interest. As a very simple example, suppose that we have a binary treatment variable $\RV{T}$, and a response variable $\RV{Y}$. We consider three different regimes [...] the first two regimes may be described as interventional, and the last as observational.
\end{quote}

The difference between the model described here and a see-do model is that a see-do model uses different variables $\RV{X}$ and $\RV{Y}$ to represent observations and consequences, while Dawid's model uses the same variable $(\RV{T},\RV{Y})$ to represent outcomes in interventional and observational regimes. In this work we associate one observed variable with each measurement process, while in Dawid's approach $(\RV{T},\RV{Y})$ seem to be doing double duty, representing measurement processes carried out during observations and after taking action. This can be thought of as the causal analogue of the difference between saying we have a sequence $(\RV{X}_1,\RV{X}_2,\RV{X}_3)$ of observations independent and identically distributed according to $\mu\in \Delta(X)$ and saying that we have some observations distributed according to $\prob{P}^{\RV{X}}\in \Delta(X)$. People usually understand what is meant by the latter, but if one is trying to be careful the former is a more precise statement of the model in question.

\subsection{Representation of decision problems}

Probabilistic models combined with the principle of expected utility have a long history of being used to represent and reason about decision problems. We might ask: while this choice is conventional, how confident are we that it is the right choice for any given problem?

There is a large literature in decision theory that aims to address the question of the ``right way'' to represent decision problems. by posing abstract conditions that we feel decision models ought to respect, and then proving representation theorems showing that all decision problems can be represented as, for example, probability distributions over functions from choices to outcomes. Notable examples are the theorems of \citet{ramsey_truth_2016} and \citet{savage_foundations_1954} (held to be equivalent), and the theorem of \citet{bolker_functions_1966,jeffrey_logic_1990}. Whether probability gap models can be identified with a Savage or Jeffrey style models of a decision problem is a question that we leave open.

There is also a literature that considers the use of probability sets rather than probability distributions as a tool for representing uncertainty under the name of ``imprecise probability'' \citet{bradley_imprecise_2019,walley_statistical_1991}. Probability sets, it is argued, can be used to represent an epistemic state of witholding judgement on certain propositions and it is useful to be able to represent this in addition to the epistemic state of being indifferent to the truth or falsehood of a proposition. We build our theory on probability sets, so we think it is likely (but we do not show) that imprecise probability can naturally be incorporated as a representation of uncertainty. Speculatively, a decision maker may have some reason to withold judgement regarding which choice they are likely to make while deliberating about which choice is the most desirable, and this might be why we find probability sets to be useful tools for modelling decision problems.


\section{Probability gap models}

We can represent such a model $f$ with probability sets. There are many different ways to do this, but the general scheme is as follows:

\begin{itemize}
    \item There is a probability set $\prob{P}_\square$ representing ``properties that hold regardless of which choice is taken''
    \item There is a collection of probability sets $\{\prob{P}_{\tilde{\alpha}}\}_A$ representing ``properties that hold just for the choice $\alpha$''
    \item $\prob{P}_\alpha=\prob{P}_\square \cap \prob{P}_{\tilde{\alpha}}$
\end{itemize}

It is always possible to accomplish this: take $\prob{P}_\square\supset \cup_{\alpha\in C} \prob{P}_\alpha$ and $\prob{P}_{\tilde{\alpha}}=\prob{P}_\alpha$. However, we might be motivated to make different choices for $\prob{P}_\square$ and the $\prob{P}_{\tilde{\alpha}}$s.

The probability set representation allows us to make use of universal conditional probabilities and universal conditional independence to reason about decision problem models. By construction $\prob{P}_\alpha\subset \prob{P}_\square$ for all $\alpha$, any universal conditional independence that holds for $\prob{P}_\square$ holds for all $\prob{P}_\alpha$ as well, and similarly any universal conditional probability with respect to $\prob{P}_\square$ is also a universal conditional probability for all $\prob{P}_\alpha$.

We call this general scheme a \emph{probability gap model}. A probability gap model specifies some universal behaviour via $\prob{P}_\square$, but this specification is incomplete; it has some gaps. We then have a selection of probability sets $\{\prob{P}_{\tilde{\alpha}}\}_A$ that specify choice-specific behaviour; this set represents different ways to fill the gap.

\begin{definition}[Probability gap model]
Given $(\Omega,\sigalg{F})$, a probability gap model is a triple $(\prob{P}_\square,\{\prob{P}_{\tilde{\alpha}}\}_A,f)$ where $\prob{P}_\square$ is a probability set and $\{\prob{P}_{\tilde{\alpha}}\}_A$ is a collection of probability sets and $f:A\to \mathscr{P}(\Delta(\Omega))$ is the map
\begin{align}
    \prob{P}_\alpha&:=f(\alpha)\\
    &= \prob{P}_\square\cap \prob{P}_{\tilde{\alpha}}
\end{align}
\end{definition}



\subsubsection{Conditional independence}\label{ssec:cond_indep}

Conditional independence has a familiar definition in probability models. We define conditional independence with respect to a probability gap model to be equivalent to conditional independence with respect to every base measure in the range of the model. This definition is closely related to the idea of \emph{extended conditional independence} proposed by \citet{constantinou_extended_2017}, see Appendix \ref{ap:eci}.

\begin{definition}[Conditional independence]
For a \emph{probability model} $\model{P}_{\alpha}$ and variables $\RV{A},\RV{B},\RV{Z}$, we say $\RV{B}$ is conditionally independent of $\RV{A}$ given $\RV{C}$, written $\RV{B}\CI_{\model{P}_{\alpha}}\RV{A}|\RV{C}$, if
\begin{align}
    \kernel{P}_{\alpha}^{\RV{ABC}} &= \tikzfig{cond_indep1} \label{eq:cond_indep}
\end{align}
\end{definition}

\citet{cho_disintegration_2019} have shown that this definition coincides with the standard notion of conditional independence for a particular probability model (Theorem \ref{th:cho_ci_equiv}). 

Conditional independence satisfies the \emph{semi-graphoid axioms}. For all standard measurable spaces $(\Omega,\sigalg{F})$ and all probability measures $\prob{P}\in \Delta(\Omega)$:

\begin{enumerate}
    \item Symmetry: $\RV{A}\CI_{\prob{P}} \RV{B}|\RV{C}$ iff $\RV{B}\CI_{\prob{P}} \RV{A}|\RV{C}$
    \item Decomposition: $\RV{A}\CI_{\prob{P}} (\RV{B},\RV{C})|\RV{W}$ implies $\RV{A}\CI_{\prob{P}}\RV{B}|\RV{W}$ and $\RV{A}\CI_{\prob{P}}\RV{C}|\RV{W}$
    \item Weak union: $\RV{A}\CI_{\prob{P}}(\RV{B},\RV{C})|\RV{W}$ implies $\RV{A}\CI_{\prob{P}}\RV{B}|(\RV{C},\RV{W})$
    \item Contraction: $\RV{A}\CI_{\prob{P}}\RV{C}|\RV{W}$ and $\RV{A}\CI_{\prob{P}}\RV{B}|(\RV{C},\RV{W})$ implies $\RV{A}\CI_{\prob{P}}(\RV{B},\RV{C})|\RV{W}$
\end{enumerate}

We define two notions of conditional independence with respect to probability sets. Global conditional independence is just conditional independence universally quantified over all members of the set. \emph{Uniform conditional independence} requires both that conditional independence hold for every element of $\prob{P}_{\{\}}$ and the existence of $\prob{P}_{\{\}}^{\RV{B}|\RV{AC}}$. This latter condition means that uniform conditional independence can be interpreted as $\prob{P}_{\{\}}^{\RV{B}|\RV{AC}}$ ``ignoring one of its inputs'' (Theorem \ref{th:ignore_inputs}). This makes it closely related to \emph{extended conditional independence} \citet{constantinou_extended_2017}. Uniform conditional independence differs from extended conditional independence in that uniform conditional independence only applies to one kind of variable, while extended conditional independence distinguishes stochastic from non-stochastic variables. It is an open question whether stochastic and non-stochastic variables correspond to particular kinds of variables in the theory of probability sets. We speculate that stochastic variables may correspond to variables with uniform marginal distributions and non-stochastic variables to those which lack uniform marginal distributions.

\begin{definition}[Global conditional independence]
For a probability set $\model{P}_{\{\}}$ and variables $\RV{X},\RV{Y},\RV{Z}$, we say $\RV{Y}$ is globally conditionally independent of $\RV{X}$ given $\RV{Z}$, written $\RV{Y}\CI_{\model{P}_{\{\}}}\RV{X}|\RV{Z}$, if for all $\prob{P}_{\alpha}\in \prob{P}_{\{\}}$ $\RV{Y}\CI_{\prob{P}_\alpha} \RV{X}|\RV{Z}$.
\end{definition}

\begin{definition}[Uniform conditional independence]
For a probability set $\model{P}_{\{\}}$ and variables $\RV{X},\RV{Y},\RV{Z}$, we say $\RV{Y}$ is uniformly conditionally independent of $\RV{X}$ given $\RV{Z}$, written $\RV{Y}\CII_{\model{P}_{\{\}}}\RV{X}|\RV{Z}$, if both $\prob{P}_{\{\}}^{\RV{Y}|\RV{XZ}}$ and $\prob{P}_{\{\}}^{\RV{Y}|\RV{Z}}$ exist and
\begin{align}
        \prob{P}_{\{\}}^{\RV{Y}|\RV{XZ}} &\overset{\prob{P}_{\{\}}}{\cong} \tikzfig{universal_conditional_independence}
\end{align}
\end{definition}

Note that, unlike regular conditional independence, $\RV{Y}\CII_{\prob{P}_{\{\}}} *$ (where $*$ is a constant variable) is nontrivial -- it expresses the fact that $\prob{P}__{\{\}}^{\RV{Y}}$ exists.

Global conditional independence is equivalent to uniform conditional independence if there is a dominating choice. However, the global conditional independence does not imply uniform conditional independence in the absence of a dominating choice.

We can show that uniform conditional independence satisfies all the semi-graphoid axioms except symmetry. Symmetry is not satisfied because $\RV{X}\CIII_{\prob{P}_{\{\}}}\RV{Y}|\RV{Z}$ does not imply the existence of $\prob{P}_{\{\}}^{\RV{Y}|\RV{XZ}}$, needed for $\RV{Y}\CIII_{\prob{P}_{\{\}}}\RV{X}|\RV{Z}$

To prove uniform conditional independence satisfies the other semi-graphoid axioms, we make use of Lemma \ref{lem:distribute_quantifier}.

\begin{lemma}\label{lem:distribute_quantifier}
$[\forall x: (f(x)\implies g(x))]\implies[(\forall x: f(x))\implies(\forall x: g(x))]$
\end{lemma}

\begin{proof}
See appendix.
\end{proof}

\begin{theorem}
Given a standard measurable space $(\Omega,\sigalg{F})$ and $\prob{P}_{\{\}}$ on $\Omega$, uniform conditional independence with respect to $\prob{P}_{\{\}}$ satisfies the semi-graphoid axioms with the exception of symmetry. That is:

\begin{enumerate}
    \setcounter{enumi}{1}
    \item Decomposition: $\RV{A}\CII_{\prob{P}_{\{\}}} (\RV{B},\RV{C})|\RV{W}$ implies $\RV{A}\CII_{\prob{P}_{\{\}}}\RV{B}|\RV{W}$ and $\RV{A}\CII_{\prob{P}_{\{\}}}\RV{C}|\RV{W}$
    \item Weak union: $\RV{A}\CII_{\prob{P}_{\{\}}}(\RV{B},\RV{C})|\RV{W}$ implies $\RV{A}\CII_{\prob{P}_{\{\}}}\RV{B}|(\RV{C},\RV{W})$
    \item Contraction: $\RV{A}\CII_{\prob{P}_{\{\}}}\RV{C}|\RV{W}$ and $\RV{A}\CI_{\prob{P}}\RV{B}|(\RV{C},\RV{W})$ implies $\RV{A}\CII_{\prob{P}_{\{\}}}(\RV{B},\RV{C})|\RV{W}$
\end{enumerate}
\end{theorem}

\begin{proof}
First, we will argue that \emph{global conditional independence} satisfies all of semigraphoid axioms.

For a particular probability $\prob{P}_\alpha$, each of the axioms consists of a statement of the form $\forall \prob{P}: f(\prob{P})\implies g(\prob{P})$.

As the axioms hold for conditional independence for any probability model, we have, for arbitrary $\prob{P}_{\{\}}$, $\forall \prob{P}_\alpha\in \prob{P}_{\{\}}: f(\prob{P}_{\alpha}) \implies g(\prob{P}_\alpha)$. 

Then by Lemma \ref{lem:distribute_quantifier}, $(\forall \prob{P}_\alpha\in \prob{P}_{\{\}}: f(\prob{P}_{\alpha}))\implies (\forall \prob{P}_\alpha\in \prob{P}_{\{\}}: g(\prob{P}_{\alpha}))$.

Note that $(\forall \prob{P}_\alpha\in \prob{P}_{\{\}}: f(\prob{P}_{\alpha}))$ is, by definition, a global conditional independence statement with respect to $\prob{P}_{\{\}}$.

Next, we will show that for axioms 2-4 the required uniform conditional probabilities also exist. 

Weak union: By assumption $\RV{A}\CII_{\prob{P}} (\RV{B},\RV{C})|\RV{W}$, we have the existence of $\prob{P}_{\{\}}^{\RV{A}|\RV{BCW}}$, and this is also the uniform conditional probability required by the statement $\RV{A}\CII_{\prob{P}}\RV{B}|(\RV{C},\RV{W})$. Thus weak union is satisfied.

Decomposition: By assumption $\RV{A}\CI_{\prob{P}} (\RV{B},\RV{C})|\RV{W}$. By weak union (which we have established to hold), we also have $\RV{A}\CII_{\prob{P}}\RV{B}|(\RV{C},\RV{W})$. Combining this with Theorem \ref{th:ignore_inputs}, we also have the existence of $\prob{P}_{\{\}}^{\RV{A}|\RV{CW}}$. $\RV{A}\CI_{\prob{P}} (\RV{B},\RV{C})|\RV{W}$ also implies $\RV{A}\CI_{\prob{P}} (\RV{C},\RV{B})|\RV{W}$, and so we can repeat this argument with $\RV{B}$ and $\RV{C}$ interchanged.

Contraction: By assumption $\RV{A}\CI_{\prob{P}}\RV{B}|(\RV{C},\RV{W})$, we have the existence of $\prob{P}_{\{\}}^{\RV{A}|\RV{BCW}}$, and this is also the uniform conditional probability required by the statement $\RV{A}\CI_{\prob{P}}(\RV{B},\RV{C})|\RV{W}$.
\end{proof}






\subsection{Junk}

\begin{proof}
\textbf{Sufficiency:}
We want to show that $\RV{Y}_i\CI_{\prob{P}_\square} \RV{Y}_{\{i\}^C}\RV{X}_{\{i\}^C} |\RV{H}\RV{X}_i$ for all $i\in M$, that $\prob{P}_\square^{\RV{Y}_i|\RV{H}\RV{X}_i}$ exists for all $i\in M$ and that $\prob{P}_\square^{\RV{Y}_i|\RV{H}\RV{X}_i}=\prob{P}_\square^{\RV{Y}_j|\RV{H}\RV{X}_j}$.

From causal contractibility we have
\begin{align}
(\RV{X}_i,\RV{Y}_i)\CI_{\prob{P}_\square} (\RV{X}_{\{i\}^C},\RV{Y}_{\{i\}^C},\RV{D}_{\{i\}^C}) |\RV{H}\RV{D}_i\label{eq:cc}\\
\RV{Y}_i\CI_{\prob{P}_\square} (\RV{Y}_{\{i\}^C},\RV{X}_{\{i\}^C}) |\RV{H}\RV{D}_i\RV{X}_i\label{eq:wu}
\end{align}

Where Eq. \ref{eq:wu} follows from \ref{eq:cc} by weak union.

Thus by contraction, $\RV{Y}_i\CI_{\prob{P}_\square} \RV{Y}_{\{i\}^C}\RV{D}_{M} |\RV{H}\RV{X}_i$.

By Corollary \ref{cor:ci_cp_exist} and the existence of $\prob{P}^{\RV{Y}_i\RV{X}_i|\RV{H}\RV{D}_i}$ for all $i\in M$, $\prob{P}_\square^{\RV{Y}_i|\RV{H}\RV{X}_i}$ exists for all $i$. Furthermore, because $\prob{P}^{\RV{Y}_i\RV{X}_i|\RV{H}\RV{D}_i}=\prob{P}^{\RV{Y}_j\RV{X}_j|\RV{H}\RV{D}_j}$ for all $i,j\in M$, $\prob{P}_\square^{\RV{Y}_i|\RV{H}\RV{X}_i}=\prob{P}_\square^{\RV{Y}_j|\RV{H}\RV{X}_j}$ for all $i,j\in M$.
\textbf{Necessity:}
We will show for all $\alpha\in A$, $B\in \sigalg{Y}$, $(x,d,h)\in X\times D\times H$ that 
\begin{align}
    \prob{P}_\alpha^{\RV{Y}_0|\RV{X}_0\RV{D}_0\RV{H}}(B|x,d,h)=\prob{P}_\alpha^{\RV{Y}_0|\RV{X}_0\RV{H}}(B|x,h)
\end{align}

By assumption, we have the conditionals $\prob{P}_\square^{\RV{Y}_i\RV{X}_i|\RV{D}_i\RV{H}}$ and $\prob{P}_\square^{\RV{X}_i|\RV{H}\RV{D}_i}$ for all $i\in M$. We can conclude that $\prob{P}_\square^{\RV{Y}_i|\RV{X}_i\RV{D}_0i\RV{H}}$ also exists, as it is a higher order conditional with respect to $\prob{P}_\square^{\RV{Y}_i\RV{X}_i|\RV{D}_i\RV{H}}$.

For arbitrary $d\in D$, let $\alpha_d\in A$ be such that $\prob{P}_{\alpha_d}^{\RV{D}_i}=\delta_{d}$. For every version of $\prob{P}_{\alpha_d}^{\RV{Y}_i|\RV{X}_i\RV{D}_i\RV{H}}$ and $\prob{P}_{\alpha_d}^{\RV{Y}_i|\RV{X}_i\RV{D}_i\RV{H}}$
\begin{align}
    \prob{P}_{\alpha_d}^{\RV{Y}_i|\RV{X}_i\RV{H}}(B|x,h) &= \int_{D}\prob{P}_{\alpha_d}^{\RV{Y}_i|\RV{X}_i\RV{D}_i\RV{H}}(B|x,d',h)\delta_d(\mathrm{d}d')\\
                                                         &= \prob{P}_{\alpha_d}^{\RV{Y}_i|\RV{X}_i\RV{D}_i\RV{H}}(B|x,d,h)
\end{align}
For all $x\in X$, $h\in H$ $B\subset\sigalg{Y}$ except on a set of points $C\subset X\times H$ of uniform $\prob{P}_{\alpha_d}$ measure 0.
\todo[inline]{Need to add independence of hypothesis to representation theorem}
However, note that for any $\alpha$
\begin{align}
    \prob{P}_\alpha^{\RV{X}_i\RV{H}\RV{D}_i}(E\times F\times G) &= \sum_{d\in G} \prob{P}_\alpha^{\RV{D}_i}(d)\prob{P}_\square^{\RV{X}_i\RV{H}|\RV{D}_i}(E\times F|d)\\
                                                                  &= \sum_{d\in G} \prob{P}_\alpha^{\RV{D}_i}(d)\sum_{d'\in D}\prob{P}_\square^{\RV{X}_i\RV{H}|\RV{D}_i}(E\times F|d')\prob{P}_{\alpha_d}^{\RV{D}_i}(\{d'\}\\
                                                                  &= \sum_{d\in G} \prob{P}_\alpha^{\RV{D}_i}(d)\prob{P}_{\alpha_d}^{\RV{X}_i\RV{H}\RV{D}_i}(E\times F\times\{d\})
\end{align}

Thus for each $d\in D$ the set $\{d\}\times C\subset D\times X\times H$ is of uniform $\prob{P}_\alpha$ measure 0 for any $\alpha\in A$. Because $\prob{P}_\square=\cup_{\alpha\in A}\prob{P}_\alpha$, it is also of uniform $\prob{P}_\square$ measure 0. Thus 

\begin{align}
    \prob{P}_{\square}^{\RV{Y}_0|\RV{X}_0\RV{H}}(B|x,h) &= \prob{P}_{\square}^{\RV{Y}_0|\RV{X}_0\RV{D}_0\RV{H}}(B|x,d,h)
\end{align}
as desired.

\subsection{Randomness pushbacks}

Given a function $f:X\times Y\to Z$, we can obtain a curried version $\lambda f:Y\to Z^X$. In particular, if $Y=\{*\}$ then $\lambda f:\{*\}\to Z^X$. At least for countable $X$, we can apply this construction to Markov kernels: given a kernel $\kernel{K}:X\kto Y$, define $\kernel{L}: \{*\}\kto Y^X$ by 
\begin{align}
    \kernel{L} ((y_i)_{i\in X}) &= \prod_{i\in X} \kernel{K}(y_i|i)
\end{align}

We can then define an evaluation map $\text{ev}:Y^X\times X\to Y$ by $\text{ev}((y_i)_{i\in X},x)=y_x$. Then

\begin{align}
    \kernel{K} &= \tikzfig{curry_kernel_definition} \label{eq:curry_identity}\\
    &\iff\\
    \kernel{K}(A|x) &= \int_{Y^X} \delta_{\text{ev}(y^X,x)}(A) \kernel{L}(\mathrm{d}y^X|x)
\end{align}

Unlike the case of function currying, $\kernel{L}$ is not the unique Markov kernel for which \ref{eq:curry_identity} holds. In fact, we can substitute any $\kernel{M}$ such that, for any $i\in X$

\begin{align}
    \sum_{y_{\{i\}^C}\in Y^{|X|-1}} \kernel{M}((y_i)_{i\in X}) = \kernel{K}(y_i|i)
\end{align}

This representation of a Markov kernel is called a \emph{randomness pushback} by \citet{fritz_synthetic_2020}. The idea is that the randomness in the orignal Markov kernel $\kernel{L}$ has been ``pushed back'' to $\kernel{L}$, which now passes through the deterministic Markov kernel $\kernel{F}_{\mathrm{ev}}$.

Randomness pushbacks have a few features in common with potential outcomes causal models. For our purposes, we will say a potential outcomes model is a probability set $(\Omega,\sigalg{F},\prob{P}_{\{\}})$ along with variables $\RV{X}$, $\RV{Y}$, $\RV{Y}^X$ such that 

\begin{align}
    \prob{P}_{\{\}}^{\RV{Y}|\RV{XY}^X} &= \kernel{F}_{\text{ev}}
\end{align}

More commonly, this property is expressed as

\begin{align}
    \RV{Y}\overset{a.s.}{=}\text{ev}(\RV{X},\RV{Y}^X)
\end{align}

We consider a potential outcomes model to be a probability set here, but we can formally recover a ``traditional'' potential outcomes model by considering probability sets of size $1$.

If we additionally have the existence of $\prob{P}_{\{\}}^{\RV{Y}^X|\RV{X}}$ and $\RV{Y}^\RV{X}\CI_{\prob{P}_{\{\}}} \RV{X}$ then 

\begin{align}
    \prob{P}_{\{\}}^{\RV{Y}|\RV{X}} &= \tikzfig{curry_kernel_copied}\label{eq:curry_identity_po}
\end{align}

\subsubsection{Example: comb insertion}

The following examples illustrate 2-combs and the insertion operation, both of which we will define later. As an example in translating diagrams, we show how the diagrams for a 2-comb and 2-comb with an inserted Markov kernel can be translated to integral notation.

Consider the Markov kernels $\kernel{K}:W\kto X$, $\kernel{L}:X\times W\times Y\kto Z$ and the 2-comb $\kernel{M}:W\times Y\kto X\times Z$ defined as

\begin{align}
    \kernel{M} = \tikzfig{2_comb}\label{eq:2comb_M}
\end{align}

Following the rules above, we can translate this to ordinary notation by first breaking it down into products and tensor products, and then evaluating these products

\begin{align}
    \kernel{M}(A\times B|w,y) = [&(\text{copy}_W\otimes \text{id}_Y)(\kernel{K}\otimes \text{id}_{W\times Y})\\
    &(\text{copy}_X\otimes \text{id}_{W\times Y})(\text{id}_X\otimes\kernel{L})](A\times B|w,y)\\
                        = [&(\kernel{K}\otimes \text{id}_{W\times Y})(\text{copy}_X\otimes \text{id}_{W\times Y})\\
                        &(\text{id}_X\otimes\kernel{L})](A\times B|w,w,y)\\
                        = \;&\int_{X}  (\text{id}_X\otimes\kernel{L})(A\times B|x',w,y) \kernel{K} (dx'|w)
                        ](y,z|y',x)\\
                        = \;&\int_X \text{id}_X(A|x') \kernel{L}(B|x',w,y)\kernel{K}(dx'|w)\\
                        = \;&\int_A \kernel{L}(B|x',w,y)\kernel{K}(dx'|w)
\end{align}

If we are given additionally $\kernel{J}:X\times W\kto Y$, we can define a new Markov kernel $\kernel{N}:W\kto Z$ given by ``inserting'' $\kernel{J}$ into $\kernel{M}$:

\begin{align}
    \kernel{N} = \tikzfig{2comb_inserted_anon}\label{eq:2comb_winsert}
\end{align}


We can translate Equation \ref{eq:2comb_winsert} to

\begin{align}
    \kernel{N}(A\times B\times C|w) = &[\text{copy}_W(\kernel{K}\text{copy}^3_Y\otimes \text{id}_W)\\
    &(\text{id}_Y\otimes\kernel{J}\otimes \text{id}_Y)(\text{id}_Y \otimes \text{copy}_X\otimes \text{id}_Y)\\
    &(\kernel{L}\otimes \text{id}_X\otimes \text{id}_Y)] (A\times B\times C|w)\\
                    = &[(\kernel{K}\text{copy}^3_Y\otimes \text{id}_W)(\text{id}_Y\otimes\kernel{J}\otimes \text{id}_Y)\\
                    &(\text{id}_Y \otimes \text{copy}_X\otimes \text{id}_Y)\\
                    &(\kernel{L}\otimes \text{id}_X\otimes \text{id}_Y)] (A\times B\times C|w,w)\\
                    = &\int_X\int_Y\kernel{L}(C|x',w,y') \text{id}_X(A|x') \text{id}_Y(B|y') \kernel{J}(dy'|x',w)\kernel{K}(dx'|w)\\
                    = &\int_A\int_B\kernel{L}(C|x',w,y') \kernel{J}(dy'|x',w)\kernel{K}(dx'|w)
\end{align}


\subsection{Global conditional independence}

\begin{reptheorem}{lem:distribute_quantifier}
$[\forall x: (f(x)\implies g(x))]\implies[(\forall x: f(x))\implies(\forall x: g(x))]$
\end{reptheorem}

\begin{proof}
\begin{align}
    \forall x: f(x) \implies g(x)&&\text{premise}\label{eq:premise1}\\
    \forall x: f(x)&& \text{premise}\label{eq:premise2}\\
    f(a) && \text{UI on }\ref{eq:premise2}\text{ sub }a/x\label{eq:ui1}\\
    f(a)\implies g(a) && \text{UI on }\ref{eq:premise1}\text{ sub }a/x\label{eq:ui2}\\
    g(a)&&\text{ MP }\ref{eq:ui1}\text{ and }\ref{eq:ui2}\label{eq:mp1}\\
    \forall x: g(x)&&\text{UG on }\ref{eq:mp1}\label{eq:ug1}\\
    (\forall x: f(x))\implies(\forall x: g(x))&& \text{CP }\ref{eq:premise2}-\ref{eq:ug1}\label{eq:cp1}\\
    [\forall x: (f(x)\implies g(x))]\implies[(\forall x: f(x))\implies(\forall x: g(x))]&& \text{CP }\ref{eq:premise1}\text{--}\ref{eq:cp1}
\end{align}

Where UI: universal instantiation, UG: universal generalisation, MP: modus ponens and CP: conditoinal proof. With thanks to \citet{1377555} for the proof.
\end{proof}

\subsection{Exchange commutativity}\label{sec:exchange_commutativity}

Suppose there's a doctor, Dr Alice, who is examining patients and offering them treatment. We make a number of assumptions about measurement procedures that are \emph{interchangeable}, meaning that the same probability distribution should be used to model them. First, some general assumptions:
\begin{enumerate}
    \item If a procedure $\proc{T}$ is equivalent (Definition \ref{def:equality}) to $f\circ \proc{S}_\alpha$, then $\proc{T}$ is distributed according to $\prob{P}_\alpha\kernel{F}_f$
    \item Two procedures $\proc{S}_\alpha$ and $\proc{S}_{\alpha'}$ are interchangeable if their descriptions are identical up to ``variable'' names
\end{enumerate}

And some assumptions specific to this example:
\begin{enumerate}
    \setcounter{enumi}{2}
    \item Two procedures $\proc{S}_\alpha$ and $\proc{S}_{\alpha'}$ are interchangeable if the description of one can be obtained by permuting patients in the description of the other (\emph{patient indistinguishability})
    \item Two procedures $\proc{S}_\alpha$ and $\proc{S}_{\alpha'}$ are interchangeable if the description of one can be obtained by permuting the order of treatment administration in the description of the other (\emph{order irrelevance})
\end{enumerate}

Consider first the decision procedure $\proc{S}_C$ that, for each $\alpha$, starts with deterministically choosing treatments according to the invertible function $c:A\to D^2$, assigns treatments to patients and returns the results:

\begin{algorithmic}
    \Procedure{$\proc{S}_\alpha$}{}
    \State $(d_1,d_2) \gets c(\alpha)$
    \State $y_1\gets \mathrm{apply}(d_1,\mathrm{patient\;A})$
    \State $y_2\gets \mathrm{apply}(d_2,\mathrm{patient\;B})$
    \State \Return $(d_1,d_2,y_1,y_2)$
    \EndProcedure
\end{algorithmic}

By the assumptions of patient indistinguishability and order irrelevance, this is interchangeable with

\begin{algorithmic}
    \Procedure{$\proc{S}_\alpha$}{}
    \State $(d_1,d_2) \gets c(\alpha)$
    \State $y_2\gets \mathrm{apply}(d_2,\mathrm{patient\;A})$
    \State $y_1\gets \mathrm{apply}(d_1,\mathrm{patient\;B})$
    \State \Return $(d_1,d_2,y_1,y_2)$
    \EndProcedure
\end{algorithmic}

Take some $\alpha'$ such that $\mathrm{choose\_treatments(\alpha)} = \mathrm{swap}\circ \mathrm{choose\_treatments(\alpha')}$.

\begin{algorithmic}
    \Procedure{$\text{swap}_{\RV{DY}}\circ \proc{S}_{\alpha'}$}{}
    \State $(d_1,d_2) \gets c(\alpha')$
    \State $y_1\gets \mathrm{apply}(d_1,\mathrm{patient\;A})$
    \State $y_2\gets \mathrm{apply}(d_2,\mathrm{patient\;B})$
    \State \Return $(d_1,d_2,y_1,y_2)$
    \EndProcedure
\end{algorithmic}

Using interchangeability of equivalent procedures, we express $\proc{S}_{\alpha'}$ as

\begin{algorithmic}
    \Procedure{$\text{swap}_{\RV{DY}}\circ \proc{S}_{\alpha'}$}{}
    \State $(d_2,d_1) \gets c(\alpha)$
    \State $y_1\gets \mathrm{apply}(d_1,\mathrm{patient\;A})$
    \State $y_2\gets \mathrm{apply}(d_2,\mathrm{patient\;B})$
    \State \Return $(d_1,d_2,y_1,y_2)$
    \EndProcedure
\end{algorithmic}

Let $d_1'=d_2$, $d_2'=d_1$, $y_1'=y_2$, $y_2'=y_1$

\begin{algorithmic}
    \Procedure{$\proc{S}_{\alpha'}$}{}
    \State $(d'_1,d'_2) \gets c(\alpha)$
    \State $y'_2\gets \mathrm{apply}(d'_2,\mathrm{patient\;A})$
    \State $y'_1\gets \mathrm{apply}(d'_1,\mathrm{patient\;B})$
    \State \Return $(d'_2,d'_1,y'_2,y'_1)$
    \EndProcedure
\end{algorithmic}

Composing $\text{swap}_{\RV{DY}}$ with $\proc{S}_{\alpha'}$:

\begin{algorithmic}
    \Procedure{$\text{swap}_{\RV{DY}}\circ \proc{S}_{\alpha'}$}{}
    \State $(d'_1,d'_2) \gets c(\alpha)$
    \State $y'_2\gets \mathrm{apply}(d'_2,\mathrm{patient\;A})$
    \State $y'_1\gets \mathrm{apply}(d'_1,\mathrm{patient\;B})$
    \State \Return $(d'_1,d'_2,y'_1,y'_2)$
    \EndProcedure
\end{algorithmic}

This is interchangeable with $\proc{S}_{\alpha}$ by interchangeability of descriptions up to variable names. Thus

\begin{align}
    \prob{P}_\alpha^{\RV{Y}_1\RV{Y}_2} &= \prob{P}_{\alpha'}^{\RV{Y}_1\RV{Y}_2}\text{swap}_{\RV{Y}}
\end{align}

Because $(\RV{D}_1,\RV{D}_2)$ is deterministic for each $\alpha$,
\begin{align}
    \prob{P}_\alpha^{\RV{Y}_1\RV{Y}_2|\RV{D}_1\RV{D}_2}(A\times B|d_1,d_2) &= \begin{cases}
        \prob{P}_\alpha^{\RV{Y}_1\RV{Y}_2}(A\times B) &(d_1,d_2)=c(\alpha)\\ 
        \mathrm{arbitrary}&\mathrm{otherwise}
    \end{cases} \\
    \implies \prob{P}_C^{\RV{Y}_1\RV{Y}_2|\RV{D}_1\RV{D}_2}(A\times B|d_1,d_2) &= \prob{P}_{c^{-1}(d_1,d_2)}(A\times B)
\end{align}

and as we have established by procedure interchangeability (noting that the binary $\text{swap}$ is its own inverse)

\begin{align}
    \text{swap}_{\RV{D}}\prob{P}_C^{\RV{Y}_1\RV{Y}_2|\RV{D}_1\RV{D}_2}\text{swap}_{\RV{Y}}(A\times B|c(\alpha)) &= \prob{P}_{C}^{\RV{Y}_1\RV{Y}_2|\RV{D}_1\RV{D}_2}\text{swap}_{\RV{Y}}(A\times B|c(\alpha'))\\
    &= \prob{P}_{\alpha'}^{\RV{Y}_1\RV{Y}_2}\text{swap}_{\RV{Y}}(A\times B)\\
    &= \prob{P}_\alpha^{\RV{Y}_1\RV{Y}_2}(A\times B)\\
    &= \prob{P}_C^{\RV{Y}_1\RV{Y}_2|\RV{D}_1\RV{D}_2}(A\times B|c(\alpha))
\end{align}

Thus $\prob{P}_C$ is exchange commutative.



In Appendix \ref{sec:exchange_commutativity} we offer a more detailed argument for accepting exchange commutativity for an example decision procedure. Suppose we are in the position of a doctor who will be seeing patients in the future, and resolves to follow a procedure of the form

\begin{algorithmic}
    \Procedure{$\proc{S}_\alpha$}{}
    \State $(d_1,d_2) \gets c(\alpha)$
    \State $y_1\gets \mathrm{apply}(d_1,\mathrm{patient\;A})$
    \State $y_2\gets \mathrm{apply}(d_2,\mathrm{patient\;B})$
    \State \Return $(d_1,d_2,y_1,y_2)$
    \EndProcedure
\end{algorithmic}

for some treatment policy $\alpha\in A$. Define $\RV{D}:=(\RV{D}_1,\RV{D}_2)$ where $\RV{D}:(d_1,d_2,y_1,y_2)\mapsto (d_1,d_2)$ and $\RV{Y}:=(\RV{Y}_1,\RV{Y}_2)$ with $\RV{Y}:(d_1,d_2,y_1,y_2)\mapsto (y_1,y_2)$. $\proc{D}$ is the procedure associated with $\RV{D}$.

We argue that the following conditions are sufficient to support a judgement of $(\RV{D};\RV{Y})$-exchange commutativity:

\begin{enumerate}
    \item Two procedures $\proc{S}_\alpha$ and $\proc{S}_{\alpha'}$ are indistinguishable if the description of one can be obtained by permuting patients in the description of the other (\emph{patient indistinguishability})
    \item Two procedures $\proc{S}_\alpha$ and $\proc{S}_{\alpha'}$ are indistinguishable if the description of one can be obtained by permuting the order of treatment administration in the description of the other (\emph{order indistinguishability})
    \item $c:A\to D^2$ is an invertible function
\end{enumerate}

A typical condition given for the identifiability of treatment effects or interventional conditionals is that the treatment subprocedures $(\proc{D}_1,\proc{D}_2)$) are random \citep{rubin_causal_2005}. Our third condition -- that $\proc{D}$ is a deterministic and invertible function of $\alpha$ -- is somewhat different to this. A rough explanation of this condition is: the model represents the doctor's knowledge at the time they choose $\alpha$, and assumptions 1 and 2 imply that given their knowledge at the time, the two treatment episodes are indistinguishable. A deterministic choice of $\proc{D}$ ensures that the result of $\proc{D}$ cannot reflect anything new that is learned after the fact.

This rough story can be extended to a randomised $\proc{D}$. We could say that $\proc{D}$ is ``randomised'' if it is chosen according to $c'(\alpha,\proc{R})$ for some function $c':A\times R\to D^2$ and some ``random'' $\proc{R}$. Then $\proc{D}$ again cannot reflect anything not known when $\alpha$ was chosen, because we assume that the results of random procedures carry no useful information.

This story depends crucially on the state of knowledge of the decision maker when they make their choice. It seems difficult to state the assumption of deterministic $\proc{D}$ without the decision theoretic approach we adopt here.

\subsection{Semidirect product and almost sure equality}

The operation used in Equation \ref{eq:conditional} that combines $\mu^{\RV{X}}$ and $\mu^{\RV{Y}|\RV{X}}$ is something we will use repeatedly, so we call it the \emph{semidirect product} and give it the symbol $\odot$. We also define a notion of almost sure equality with using $\odot$: $\kernel{K}\overset{\mu^{\RV{X}}}{\cong} \kernel{L}$ if $\mu^{\RV{X}}\odot \kernel{K}=\mu^{\RV{X}}\odot\kernel{L}$ (note that this latter equality is strict; both semidirect products must assign the same measure to the same measurable sets). Thus if two terms are almost surely equal, they are substitutable when they both appear in a semidirect product.

\begin{definition}[Semidirect product]\label{def:copyproduct}
Given $\prob{K}:X\kto Y$ and $\prob{L}:Y\times X\kto Z$, define the copy-product $\prob{K}\odot\prob{L}:X\to Y\times Z$ as
\begin{align}
    \prob{K}\odot\prob{L}:&= \text{copy}_X(\prob{K}\otimes \text{id}_X)(\text{copy}_Y\otimes\text{id}_X )(\text{id}_Y \otimes \prob{L})\\
                            &= \tikzfig{copy_product}\\
                            &\iff\\
    (\prob{K}\odot\prob{L})(A\times B|x) &= \int_A \prob{L}(B|y,x)\prob{K}(dy|x)&A\in \sigalg{Y},B\in\sigalg{Z}
\end{align}
\end{definition}

\begin{lemma}[Semidirect product is associative]
Given $\prob{K}:X\kto Y$, $\prob{L}:Y\times X\kto Z$ and $\prob{M}:Z\times Y\times X\kto W$
\begin{align}
    (\prob{K}\odot \prob{L})\odot \prob{Z} &= \prob{K}\odot(\prob{L}\odot\prob{Z})\\
\end{align}
\end{lemma}

\begin{proof}
\begin{align}
    (\prob{K}\odot \prob{L})\odot \prob{M} &= \tikzfig{odot_assoc_1}\\
                                            &=  \tikzfig{odot_assoc_2}\\
                                            &= \prob{K}\odot (\prob{L}\odot \prob{M})
\end{align}
\end{proof}

Two Markov kernels are almost surely equal with respect to a probability set $\prob{P}_C$ if the semidirect product $\odot$ of all marginal probabilities of $\prob{P}_\alpha^\RV{X}$ with each Markov kernel is identical.

\begin{definition}[Almost sure equality]\label{def:asequal}
Two Markov kernels $\kernel{K}:X\kto Y$ and $\kernel{L}:X\kto Y$ are almost surely equal $\overset{\prob{P}_C}{\cong}$ with respect to a probability set $\prob{P}_C$ and variable $\RV{X}:\Omega\to X$ if for all $\prob{P}_\alpha \in \prob{P}_C$,
\begin{align}
    \prob{P}^{\RV{X}}_\alpha\odot \kernel{K}=\prob{P}^{\RV{X}}_\alpha\odot \kernel{L}
\end{align}
\end{definition}

\begin{lemma}[Uniform conditional distributions are almost surely equal]
If $\kernel{K}:X\kto Y$ and $\kernel{L}:X\kto Y$ are both versions of $\prob{P}_C^{\RV{Y}|\RV{X}}$ then $\kernel{K}\overset{\prob{P}_C}{\cong}\kernel{L}$
\end{lemma}

\begin{proof}
For all $\prob{P}_\alpha \in \prob{P}_C$
\begin{align}
    \prob{P}^{\RV{X}}_\alpha\odot \kernel{K} &= \prob{P}^{\RV{XY}}_\alpha\\
    &= \prob{P}^{\RV{X}}_\alpha\odot \kernel{L}
\end{align}
\end{proof}

\begin{lemma}[Substitution of almost surely equal Markov kernels]\label{lem:sub_asequal}
Given $\prob{P}_C$, if $\kernel{K}:X\times Y \kto Z$ and $\kernel{L}:X\times Y \kto Z$ are almost surely equal $\kernel{K}\overset{\prob{P}_C}{\cong}\kernel{L}$, then for any $\prob{P}_\alpha\in \prob{P}_C$
\begin{align}
    \prob{P}_\alpha^{\RV{Y}|\RV{X}}\odot \kernel{K} &\overset{\prob{P}_C}{\cong} \prob{P}_\alpha^{\RV{Y}|\RV{X}}\odot \kernel{L}
\end{align}
\end{lemma}

\begin{proof}
For any $\prob{P}_\alpha\in\prob{P}_C$
\begin{align}
    \prob{P}_\alpha^{\RV{XY}}\odot \kernel{K} &= (\prob{P}_\alpha^{\RV{X}}\odot \prob{P}_C^{\RV{Y}|\RV{X}})\odot \kernel{K}\\
                                              &= \prob{P}_\alpha^{\RV{X}}\odot (\prob{P}_C^{\RV{Y}|\RV{X}}\odot \kernel{K})\\
                                              &= \prob{P}_\alpha^{\RV{X}}\odot (\prob{P}_C^{\RV{Y}|\RV{X}}\odot \kernel{L})
\end{align}
\end{proof}

\begin{theorem}[Semidirect product of uniform conditional distributions is a joint uniform conditional distribution]\label{lem:joint_conditional}
Given a probability set $\prob{P}_C$ on $(\Omega,\sigalg{F})$, variables $\RV{X}:\Omega\to X$, $\RV{Y}:\Omega\to Y$ and uniform conditional distributions $\prob{P}_C^{\RV{Y}|\RV{X}}$ and $\prob{P}_C^{\RV{Z}|\RV{XY}}$, then $\prob{P}_C^{\RV{YZ}|\RV{X}}$ exists and is equal to
\begin{align}
    \prob{P}_C^{\RV{YZ}|\RV{X}} &= \prob{P}_C^{\RV{Y}|\RV{X}}\odot \prob{P}_C^{\RV{Z}|\RV{XY}}
\end{align}
\end{theorem}

\begin{proof}
By definition, for any $\prob{P}_\alpha\in \prob{P}_C$
\begin{align}
    \prob{P}_\alpha^{\RV{XYZ}} &= \prob{P}_\alpha^{\RV{X}}\odot \prob{P}_\alpha^{\RV{YZ}|\RV{X}}\\
                               &= \prob{P}_\alpha^{\RV{X}}\odot(\prob{P}_\alpha^{\RV{Y}|\RV{X}}\odot \prob{P}_\alpha^{\RV{Z}|\RV{YX}})\\
                               &= \prob{P}_\alpha^{\RV{X}}\odot(\prob{P}_C^{\RV{Y}|\RV{X}}\odot \prob{P}_C^{\RV{Z}|\RV{YX}})
\end{align}
\end{proof}


The latter formulation allows us, in some cases, . We will discuss in Section \ref{sec:dec_probs} how uniform conditional probabilities can be thought of as causal relationships. Thus this means: from a fundamental assumed causal relationship and a conditional independence observed under the right conditions, we can conclude the existence of an additional causal relationship. 

We can't test for extended conditional independence by checking $\prob{P}_\alpha$ for every $\alpha\in C$ cannot be directly observed, because we only ever carry out the measurement procedure associated with one $\alpha\in C$. Theorem \ref{th:cons_ci} shows how, under some circumstances, it is possible to infer an extended conditional independence in a probability set $\prob{P}_C$ from a regular conditional independence that holds in one element of the set $\prob{P}_\alpha$. We ultimately carry out the procedure associated with on only one element of $C$, so usually we cannot test whether some property holds for the whole set $\prob{P}_C$. However, regular conditional independences with respect to a particular element of $\prob{P}_C$ can be tested for (again, subject to some assumptions \citep{shah_hardness_2020}). We can also conclude from a the combination of a uniform conditional probability and a conditional independence statement the existence of a further uniform conditional probability (Corollary \ref{cor:ci_cp_exist})

\begin{theorem}\label{th:cons_ci}
Given standard measurable $(\Omega,\sigalg{F})$, variables $\RV{W}:\Omega\to W$, $\RV{X}:\Omega\to X$, $\RV{Y}:\Omega\to Y$ and a probability set $\prob{P}_{C}$ with uniform conditional probability $\prob{P}_{C}^{\RV{Y}|\RV{WX}}$ and $\alpha\in C$ such that $\prob{P}_\alpha^{\RV{WX}}\gg \{\prob{P}_\beta^{\RV{WX}}|\beta\in C\}$, $\RV{Y}\CI_{\prob{P}_{\alpha}}\RV{X}|\RV{W}$ if and only if there is a version of $\prob{P}_{C}^{\RV{Y}|\RV{WX}}$ and $\kernel{K}:W\kto Y$ such that
\begin{align}
  \prob{P}_C^{\RV{Y}|\RV{WX}} &= \tikzfig{cond_indep_erase} \label{eq:higherorder_ci_erase}
\end{align}
\end{theorem}

\begin{proof}
See Appendix \ref{sec:cond_ind_app}
\end{proof}

\begin{corollary}\label{cor:ci_cp_exist}
Given standard measurable $(\Omega,\sigalg{F})$, variables $\RV{W}:\Omega\to W$, $\RV{X}:\Omega\to X$, $\RV{Y}:\Omega\to Y$ and a probability set $\prob{P}_{C}$ with uniform conditional $\prob{P}_{C}^{\RV{Y}|\RV{WX}}$ and $\alpha\in C$ such that $\prob{P}_\alpha^{\RV{WX}}\gg \{\prob{P}_\beta^{\RV{WX}}|\beta\in C\}$, $\prob{P}_{C}^{\RV{Y}|\RV{W}}$ exists if $\RV{Y}\CI_{\prob{P}_{\alpha}}\RV{X}|\RV{W}$.
\end{corollary}

\begin{proof}
By Theorem \ref{th:cons_ci}, there is $\kernel{K}:W\kto Y$ such that for all $\beta$
\begin{align}
    \prob{P}_{\beta}^{\RV{WY}} &= \tikzfig{conditional_independence_cprob_exist}\\
    &= \tikzfig{conditional_independence_cprob_exist2}\\
    &= \tikzfig{conditional_independence_cprob_exist3}
\end{align}

Thus $\kernel{K}$ is a version of $\prob{P}_{C}^{\RV{Y}|\RV{W}}$.
\end{proof}